{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2024-01-03  184.220001  185.880005  183.429993  184.250000  184.250000   \n",
      "2024-01-04  182.149994  183.089996  180.880005  181.910004  181.910004   \n",
      "2024-01-05  181.990005  182.759995  180.169998  181.179993  181.179993   \n",
      "2024-01-08  182.089996  185.600006  181.500000  185.559998  185.559998   \n",
      "2024-01-09  183.919998  185.149994  182.729996  185.139999  185.139999   \n",
      "2024-01-10  184.350006  186.399994  183.919998  186.190002  186.190002   \n",
      "2024-01-11  186.539993  187.050003  183.619995  185.589996  185.589996   \n",
      "2024-01-12  186.059998  186.740005  185.190002  185.919998  185.919998   \n",
      "2024-01-16  182.160004  184.259995  180.929993  183.630005  183.630005   \n",
      "2024-01-17  181.270004  182.929993  180.300003  182.679993  182.679993   \n",
      "\n",
      "              Volume  \n",
      "Date                  \n",
      "2024-01-03  58414500  \n",
      "2024-01-04  71983600  \n",
      "2024-01-05  62303300  \n",
      "2024-01-08  59144500  \n",
      "2024-01-09  42841800  \n",
      "2024-01-10  46792900  \n",
      "2024-01-11  49128400  \n",
      "2024-01-12  40444700  \n",
      "2024-01-16  65603000  \n",
      "2024-01-17  47317400  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ticker = \"AAPL\"\n",
    "start_date = \"2014-01-01\"\n",
    "end_date = \"2024-01-18\"\n",
    "\n",
    "# Download the historical data\n",
    "data = yf.download(ticker, start=start_date, end=end_date, interval='1d')\n",
    "#print(data.shape)\n",
    "print(data.iloc[-10:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.basic\n",
    "import utils.MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size = 200\n",
    "forecast_size = 2\n",
    "val_percent = 0.05 #percent of data reserved as validation data\n",
    "batch_size = 128\n",
    "\n",
    "model = models.basic.Basic(history_size, forecast_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2227, 8, 202])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor(data.iloc[:, 0:4].values).to(torch.float32)\n",
    "close = tensor[:, 3]\n",
    "\n",
    "MA100 = utils.MA.MA(close, 100)[:, None] # N - days + 1\n",
    "valid_days = MA100.shape[0]\n",
    "MA60 = utils.MA.MA(close, 60)[-valid_days:, None]\n",
    "MA20 = utils.MA.MA(close, 20)[-valid_days:, None]\n",
    "MA5 = utils.MA.MA(close, 5)[-valid_days:, None]\n",
    "tensor = tensor[-valid_days:, :]\n",
    "\n",
    "tensor = torch.cat((tensor, MA5, MA20, MA60, MA100), dim=1)\n",
    "#print(tensor.shape)\n",
    "\n",
    "tensor = tensor.unfold(0, history_size + forecast_size, 1) #(all_data, 8, hist+fore)\n",
    "print(tensor.shape)\n",
    "shape = tensor.shape\n",
    "\n",
    "size = tensor.shape[0]\n",
    "train_size = int(size * (1 - val_percent))\n",
    "shuffle_idx1 = torch.randperm(train_size)\n",
    "shuffle_idx2 = torch.arange(size - train_size) + train_size\n",
    "shuffle_idx = torch.cat((shuffle_idx1, shuffle_idx2))\n",
    "#shuffle = (tensor[shuffle_idx]).flatten(start_dim=-2) #shuffle data (all_data, hist+fore * 8)\n",
    "shuffle = tensor[shuffle_idx] #(N, 8, hist + fore)\n",
    "\n",
    "X_train = shuffle[:train_size, :, :history_size].flatten(start_dim=-2)\n",
    "y_train = shuffle[:train_size, :4, history_size:].flatten(start_dim=-2)\n",
    "\n",
    "X_val = shuffle[train_size:, :, :history_size].flatten(start_dim=-2)\n",
    "y_val = shuffle[train_size:, :4, history_size:].flatten(start_dim=-2)\n",
    "'''\n",
    "X_train = shuffle[:train_size, :history_size*8]\n",
    "y_train = shuffle[:train_size, history_size*8:].reshape((train_size, forecast_size, 8))[:, :, :4]\n",
    "X_val = shuffle[train_size:, :history_size*8]\n",
    "y_val = shuffle[train_size:, history_size*8:].reshape((size - train_size, forecast_size, 8))[:, :, :4]\n",
    "'''\n",
    "\n",
    "data_train = TensorDataset(X_train, y_train)\n",
    "train_data_loader = DataLoader(data_train, batch_size=batch_size)#, shuffle=True) #shuffle is intended before splitting train and val\n",
    "\n",
    "def MAPE_loss_fn(t, p): #truth and prediction\n",
    "    return torch.mean(100 * torch.abs(t-p)/torch.clamp(torch.abs(t), min=1e-8))\n",
    "loss_fn = MAPE_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_data_loader, val_data, epoch = 1):\n",
    "    iteration_per_epoch = len(train_data_loader)\n",
    "    total_iteration = iteration_per_epoch * epoch\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    model.train()\n",
    "    X_val, y_val = val_data\n",
    "    for _e in range(epoch):\n",
    "        for t, (X, y) in enumerate(train_data_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_loss_hist.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iteration = _e*iteration_per_epoch + t\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(X_val)\n",
    "                val_loss = loss_fn(val_pred, y_val)\n",
    "                val_loss_hist.append(val_loss.item())\n",
    "            print(f'training on iteration:{iteration}    \\t/{total_iteration}, val loss:{val_loss.item()}')\n",
    "    model_weight = model.state_dict()\n",
    "    return model_weight, train_loss_hist, val_loss_hist\n",
    "\n",
    "def show_loss_stats(train_loss, val_loss):\n",
    "    x = range(len(train_loss))\n",
    "    plt.plot(x, train_loss, label='train_loss')\n",
    "    plt.plot(x, val_loss, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on iteration:0    \t/3400, val loss:1119.7965087890625\n",
      "training on iteration:1    \t/3400, val loss:367.0437927246094\n",
      "training on iteration:2    \t/3400, val loss:232.27798461914062\n",
      "training on iteration:3    \t/3400, val loss:165.5191192626953\n",
      "training on iteration:4    \t/3400, val loss:127.31549835205078\n",
      "training on iteration:5    \t/3400, val loss:104.63483428955078\n",
      "training on iteration:6    \t/3400, val loss:89.30247497558594\n",
      "training on iteration:7    \t/3400, val loss:78.8119888305664\n",
      "training on iteration:8    \t/3400, val loss:72.8458480834961\n",
      "training on iteration:9    \t/3400, val loss:70.17084503173828\n",
      "training on iteration:10    \t/3400, val loss:69.52230072021484\n",
      "training on iteration:11    \t/3400, val loss:69.29902648925781\n",
      "training on iteration:12    \t/3400, val loss:69.94149780273438\n",
      "training on iteration:13    \t/3400, val loss:71.15660095214844\n",
      "training on iteration:14    \t/3400, val loss:72.16333770751953\n",
      "training on iteration:15    \t/3400, val loss:72.98426818847656\n",
      "training on iteration:16    \t/3400, val loss:73.65562438964844\n",
      "training on iteration:17    \t/3400, val loss:74.20415496826172\n",
      "training on iteration:18    \t/3400, val loss:74.65315246582031\n",
      "training on iteration:19    \t/3400, val loss:75.01825714111328\n",
      "training on iteration:20    \t/3400, val loss:75.31322479248047\n",
      "training on iteration:21    \t/3400, val loss:75.5477523803711\n",
      "training on iteration:22    \t/3400, val loss:75.73011779785156\n",
      "training on iteration:23    \t/3400, val loss:75.8663101196289\n",
      "training on iteration:24    \t/3400, val loss:75.96209716796875\n",
      "training on iteration:25    \t/3400, val loss:76.0215072631836\n",
      "training on iteration:26    \t/3400, val loss:76.04743957519531\n",
      "training on iteration:27    \t/3400, val loss:76.044189453125\n",
      "training on iteration:28    \t/3400, val loss:76.01268768310547\n",
      "training on iteration:29    \t/3400, val loss:75.95509338378906\n",
      "training on iteration:30    \t/3400, val loss:75.8721923828125\n",
      "training on iteration:31    \t/3400, val loss:75.76566314697266\n",
      "training on iteration:32    \t/3400, val loss:75.63593292236328\n",
      "training on iteration:33    \t/3400, val loss:75.48402404785156\n",
      "training on iteration:34    \t/3400, val loss:75.30955505371094\n",
      "training on iteration:35    \t/3400, val loss:75.11302185058594\n",
      "training on iteration:36    \t/3400, val loss:74.89295196533203\n",
      "training on iteration:37    \t/3400, val loss:74.64900207519531\n",
      "training on iteration:38    \t/3400, val loss:74.37964630126953\n",
      "training on iteration:39    \t/3400, val loss:74.08363342285156\n",
      "training on iteration:40    \t/3400, val loss:73.75825500488281\n",
      "training on iteration:41    \t/3400, val loss:73.40184783935547\n",
      "training on iteration:42    \t/3400, val loss:73.01099395751953\n",
      "training on iteration:43    \t/3400, val loss:72.58112335205078\n",
      "training on iteration:44    \t/3400, val loss:72.1100082397461\n",
      "training on iteration:45    \t/3400, val loss:71.59028625488281\n",
      "training on iteration:46    \t/3400, val loss:71.01461029052734\n",
      "training on iteration:47    \t/3400, val loss:70.37236022949219\n",
      "training on iteration:48    \t/3400, val loss:69.65435791015625\n",
      "training on iteration:49    \t/3400, val loss:68.85222625732422\n",
      "training on iteration:50    \t/3400, val loss:67.95335388183594\n",
      "training on iteration:51    \t/3400, val loss:66.9725570678711\n",
      "training on iteration:52    \t/3400, val loss:65.92044067382812\n",
      "training on iteration:53    \t/3400, val loss:65.13341522216797\n",
      "training on iteration:54    \t/3400, val loss:64.90031433105469\n",
      "training on iteration:55    \t/3400, val loss:65.1803970336914\n",
      "training on iteration:56    \t/3400, val loss:65.61096954345703\n",
      "training on iteration:57    \t/3400, val loss:65.89339447021484\n",
      "training on iteration:58    \t/3400, val loss:65.91595458984375\n",
      "training on iteration:59    \t/3400, val loss:65.65540313720703\n",
      "training on iteration:60    \t/3400, val loss:65.16560363769531\n",
      "training on iteration:61    \t/3400, val loss:64.66727447509766\n",
      "training on iteration:62    \t/3400, val loss:64.19405364990234\n",
      "training on iteration:63    \t/3400, val loss:63.96223831176758\n",
      "training on iteration:64    \t/3400, val loss:64.0617446899414\n",
      "training on iteration:65    \t/3400, val loss:64.33132934570312\n",
      "training on iteration:66    \t/3400, val loss:64.6527328491211\n",
      "training on iteration:67    \t/3400, val loss:64.84382629394531\n",
      "training on iteration:68    \t/3400, val loss:64.92884063720703\n",
      "training on iteration:69    \t/3400, val loss:64.88695526123047\n",
      "training on iteration:70    \t/3400, val loss:64.73817443847656\n",
      "training on iteration:71    \t/3400, val loss:64.50349426269531\n",
      "training on iteration:72    \t/3400, val loss:64.17225646972656\n",
      "training on iteration:73    \t/3400, val loss:63.78837203979492\n",
      "training on iteration:74    \t/3400, val loss:63.36630630493164\n",
      "training on iteration:75    \t/3400, val loss:62.91864776611328\n",
      "training on iteration:76    \t/3400, val loss:62.52880096435547\n",
      "training on iteration:77    \t/3400, val loss:62.23686981201172\n",
      "training on iteration:78    \t/3400, val loss:61.98127365112305\n",
      "training on iteration:79    \t/3400, val loss:61.77549362182617\n",
      "training on iteration:80    \t/3400, val loss:61.58629608154297\n",
      "training on iteration:81    \t/3400, val loss:61.38959503173828\n",
      "training on iteration:82    \t/3400, val loss:61.204368591308594\n",
      "training on iteration:83    \t/3400, val loss:61.0477409362793\n",
      "training on iteration:84    \t/3400, val loss:60.91872024536133\n",
      "training on iteration:85    \t/3400, val loss:60.80995559692383\n",
      "training on iteration:86    \t/3400, val loss:60.66331100463867\n",
      "training on iteration:87    \t/3400, val loss:60.541072845458984\n",
      "training on iteration:88    \t/3400, val loss:60.399505615234375\n",
      "training on iteration:89    \t/3400, val loss:60.224151611328125\n",
      "training on iteration:90    \t/3400, val loss:60.008785247802734\n",
      "training on iteration:91    \t/3400, val loss:59.81694793701172\n",
      "training on iteration:92    \t/3400, val loss:59.67633056640625\n",
      "training on iteration:93    \t/3400, val loss:59.58148193359375\n",
      "training on iteration:94    \t/3400, val loss:59.541725158691406\n",
      "training on iteration:95    \t/3400, val loss:59.459739685058594\n",
      "training on iteration:96    \t/3400, val loss:59.38536834716797\n",
      "training on iteration:97    \t/3400, val loss:59.328155517578125\n",
      "training on iteration:98    \t/3400, val loss:59.28535842895508\n",
      "training on iteration:99    \t/3400, val loss:59.23667526245117\n",
      "training on iteration:100    \t/3400, val loss:59.19329071044922\n",
      "training on iteration:101    \t/3400, val loss:59.137908935546875\n",
      "training on iteration:102    \t/3400, val loss:59.09102249145508\n",
      "training on iteration:103    \t/3400, val loss:59.02423095703125\n",
      "training on iteration:104    \t/3400, val loss:58.99449920654297\n",
      "training on iteration:105    \t/3400, val loss:58.983497619628906\n",
      "training on iteration:106    \t/3400, val loss:58.98543930053711\n",
      "training on iteration:107    \t/3400, val loss:58.97609329223633\n",
      "training on iteration:108    \t/3400, val loss:59.01711654663086\n",
      "training on iteration:109    \t/3400, val loss:59.04587936401367\n",
      "training on iteration:110    \t/3400, val loss:59.075225830078125\n",
      "training on iteration:111    \t/3400, val loss:59.15375900268555\n",
      "training on iteration:112    \t/3400, val loss:59.12908172607422\n",
      "training on iteration:113    \t/3400, val loss:59.10958480834961\n",
      "training on iteration:114    \t/3400, val loss:59.10305404663086\n",
      "training on iteration:115    \t/3400, val loss:59.11215591430664\n",
      "training on iteration:116    \t/3400, val loss:59.09318923950195\n",
      "training on iteration:117    \t/3400, val loss:59.06572723388672\n",
      "training on iteration:118    \t/3400, val loss:58.96968460083008\n",
      "training on iteration:119    \t/3400, val loss:58.88253402709961\n",
      "training on iteration:120    \t/3400, val loss:58.76526641845703\n",
      "training on iteration:121    \t/3400, val loss:58.698604583740234\n",
      "training on iteration:122    \t/3400, val loss:58.646728515625\n",
      "training on iteration:123    \t/3400, val loss:58.60031509399414\n",
      "training on iteration:124    \t/3400, val loss:58.54656219482422\n",
      "training on iteration:125    \t/3400, val loss:58.54281997680664\n",
      "training on iteration:126    \t/3400, val loss:58.53868865966797\n",
      "training on iteration:127    \t/3400, val loss:58.54412841796875\n",
      "training on iteration:128    \t/3400, val loss:58.6060791015625\n",
      "training on iteration:129    \t/3400, val loss:58.566978454589844\n",
      "training on iteration:130    \t/3400, val loss:58.52311325073242\n",
      "training on iteration:131    \t/3400, val loss:58.49535369873047\n",
      "training on iteration:132    \t/3400, val loss:58.48138427734375\n",
      "training on iteration:133    \t/3400, val loss:58.44289016723633\n",
      "training on iteration:134    \t/3400, val loss:58.392921447753906\n",
      "training on iteration:135    \t/3400, val loss:58.28424835205078\n",
      "training on iteration:136    \t/3400, val loss:58.19507598876953\n",
      "training on iteration:137    \t/3400, val loss:58.089725494384766\n",
      "training on iteration:138    \t/3400, val loss:58.04441452026367\n",
      "training on iteration:139    \t/3400, val loss:58.02019119262695\n",
      "training on iteration:140    \t/3400, val loss:58.005340576171875\n",
      "training on iteration:141    \t/3400, val loss:57.99228286743164\n",
      "training on iteration:142    \t/3400, val loss:58.03310775756836\n",
      "training on iteration:143    \t/3400, val loss:58.072383880615234\n",
      "training on iteration:144    \t/3400, val loss:58.11932373046875\n",
      "training on iteration:145    \t/3400, val loss:58.213775634765625\n",
      "training on iteration:146    \t/3400, val loss:58.18752670288086\n",
      "training on iteration:147    \t/3400, val loss:58.14594268798828\n",
      "training on iteration:148    \t/3400, val loss:58.10763931274414\n",
      "training on iteration:149    \t/3400, val loss:58.08353042602539\n",
      "training on iteration:150    \t/3400, val loss:58.03034210205078\n",
      "training on iteration:151    \t/3400, val loss:57.96642303466797\n",
      "training on iteration:152    \t/3400, val loss:57.83524703979492\n",
      "training on iteration:153    \t/3400, val loss:57.733577728271484\n",
      "training on iteration:154    \t/3400, val loss:57.6166877746582\n",
      "training on iteration:155    \t/3400, val loss:57.56534194946289\n",
      "training on iteration:156    \t/3400, val loss:57.5374870300293\n",
      "training on iteration:157    \t/3400, val loss:57.525306701660156\n",
      "training on iteration:158    \t/3400, val loss:57.51632308959961\n",
      "training on iteration:159    \t/3400, val loss:57.570068359375\n",
      "training on iteration:160    \t/3400, val loss:57.624366760253906\n",
      "training on iteration:161    \t/3400, val loss:57.67787551879883\n",
      "training on iteration:162    \t/3400, val loss:57.76876449584961\n",
      "training on iteration:163    \t/3400, val loss:57.723758697509766\n",
      "training on iteration:164    \t/3400, val loss:57.66149139404297\n",
      "training on iteration:165    \t/3400, val loss:57.60244369506836\n",
      "training on iteration:166    \t/3400, val loss:57.56092071533203\n",
      "training on iteration:167    \t/3400, val loss:57.49074172973633\n",
      "training on iteration:168    \t/3400, val loss:57.41292190551758\n",
      "training on iteration:169    \t/3400, val loss:57.27484893798828\n",
      "training on iteration:170    \t/3400, val loss:57.17473602294922\n",
      "training on iteration:171    \t/3400, val loss:57.068870544433594\n",
      "training on iteration:172    \t/3400, val loss:57.02383041381836\n",
      "training on iteration:173    \t/3400, val loss:57.00499725341797\n",
      "training on iteration:174    \t/3400, val loss:57.00101089477539\n",
      "training on iteration:175    \t/3400, val loss:57.00808334350586\n",
      "training on iteration:176    \t/3400, val loss:57.08722686767578\n",
      "training on iteration:177    \t/3400, val loss:57.16865921020508\n",
      "training on iteration:178    \t/3400, val loss:57.23369598388672\n",
      "training on iteration:179    \t/3400, val loss:57.331153869628906\n",
      "training on iteration:180    \t/3400, val loss:57.27630615234375\n",
      "training on iteration:181    \t/3400, val loss:57.19904327392578\n",
      "training on iteration:182    \t/3400, val loss:57.11753463745117\n",
      "training on iteration:183    \t/3400, val loss:57.05220413208008\n",
      "training on iteration:184    \t/3400, val loss:56.95899200439453\n",
      "training on iteration:185    \t/3400, val loss:56.867523193359375\n",
      "training on iteration:186    \t/3400, val loss:56.72721481323242\n",
      "training on iteration:187    \t/3400, val loss:56.634986877441406\n",
      "training on iteration:188    \t/3400, val loss:56.534976959228516\n",
      "training on iteration:189    \t/3400, val loss:56.48952102661133\n",
      "training on iteration:190    \t/3400, val loss:56.47149658203125\n",
      "training on iteration:191    \t/3400, val loss:56.47218704223633\n",
      "training on iteration:192    \t/3400, val loss:56.486427307128906\n",
      "training on iteration:193    \t/3400, val loss:56.572547912597656\n",
      "training on iteration:194    \t/3400, val loss:56.64694595336914\n",
      "training on iteration:195    \t/3400, val loss:56.697608947753906\n",
      "training on iteration:196    \t/3400, val loss:56.76393508911133\n",
      "training on iteration:197    \t/3400, val loss:56.676048278808594\n",
      "training on iteration:198    \t/3400, val loss:56.56873321533203\n",
      "training on iteration:199    \t/3400, val loss:56.4748420715332\n",
      "training on iteration:200    \t/3400, val loss:56.40936279296875\n",
      "training on iteration:201    \t/3400, val loss:56.32120895385742\n",
      "training on iteration:202    \t/3400, val loss:56.25075912475586\n",
      "training on iteration:203    \t/3400, val loss:56.15727615356445\n",
      "training on iteration:204    \t/3400, val loss:56.0991325378418\n",
      "training on iteration:205    \t/3400, val loss:56.01300048828125\n",
      "training on iteration:206    \t/3400, val loss:55.98231887817383\n",
      "training on iteration:207    \t/3400, val loss:55.97460174560547\n",
      "training on iteration:208    \t/3400, val loss:55.98196029663086\n",
      "training on iteration:209    \t/3400, val loss:55.979888916015625\n",
      "training on iteration:210    \t/3400, val loss:56.04718017578125\n",
      "training on iteration:211    \t/3400, val loss:56.083770751953125\n",
      "training on iteration:212    \t/3400, val loss:56.09823989868164\n",
      "training on iteration:213    \t/3400, val loss:56.13894271850586\n",
      "training on iteration:214    \t/3400, val loss:56.02533721923828\n",
      "training on iteration:215    \t/3400, val loss:55.91176986694336\n",
      "training on iteration:216    \t/3400, val loss:55.81804656982422\n",
      "training on iteration:217    \t/3400, val loss:55.76000213623047\n",
      "training on iteration:218    \t/3400, val loss:55.692481994628906\n",
      "training on iteration:219    \t/3400, val loss:55.63846969604492\n",
      "training on iteration:220    \t/3400, val loss:55.53718185424805\n",
      "training on iteration:221    \t/3400, val loss:55.47235107421875\n",
      "training on iteration:222    \t/3400, val loss:55.37783432006836\n",
      "training on iteration:223    \t/3400, val loss:55.3427734375\n",
      "training on iteration:224    \t/3400, val loss:55.333831787109375\n",
      "training on iteration:225    \t/3400, val loss:55.34121322631836\n",
      "training on iteration:226    \t/3400, val loss:55.33491897583008\n",
      "training on iteration:227    \t/3400, val loss:55.39745330810547\n",
      "training on iteration:228    \t/3400, val loss:55.42864990234375\n",
      "training on iteration:229    \t/3400, val loss:55.44281005859375\n",
      "training on iteration:230    \t/3400, val loss:55.484886169433594\n",
      "training on iteration:231    \t/3400, val loss:55.362213134765625\n",
      "training on iteration:232    \t/3400, val loss:55.23796463012695\n",
      "training on iteration:233    \t/3400, val loss:55.14320755004883\n",
      "training on iteration:234    \t/3400, val loss:55.08944320678711\n",
      "training on iteration:235    \t/3400, val loss:55.019432067871094\n",
      "training on iteration:236    \t/3400, val loss:54.96000289916992\n",
      "training on iteration:237    \t/3400, val loss:54.86079025268555\n",
      "training on iteration:238    \t/3400, val loss:54.7921142578125\n",
      "training on iteration:239    \t/3400, val loss:54.68916702270508\n",
      "training on iteration:240    \t/3400, val loss:54.64779281616211\n",
      "training on iteration:241    \t/3400, val loss:54.63590621948242\n",
      "training on iteration:242    \t/3400, val loss:54.63650131225586\n",
      "training on iteration:243    \t/3400, val loss:54.6197509765625\n",
      "training on iteration:244    \t/3400, val loss:54.678627014160156\n",
      "training on iteration:245    \t/3400, val loss:54.70267105102539\n",
      "training on iteration:246    \t/3400, val loss:54.69565963745117\n",
      "training on iteration:247    \t/3400, val loss:54.7109375\n",
      "training on iteration:248    \t/3400, val loss:54.55540084838867\n",
      "training on iteration:249    \t/3400, val loss:54.40542221069336\n",
      "training on iteration:250    \t/3400, val loss:54.29268264770508\n",
      "training on iteration:251    \t/3400, val loss:54.23139190673828\n",
      "training on iteration:252    \t/3400, val loss:54.16851806640625\n",
      "training on iteration:253    \t/3400, val loss:54.12593460083008\n",
      "training on iteration:254    \t/3400, val loss:54.05242156982422\n",
      "training on iteration:255    \t/3400, val loss:54.00838851928711\n",
      "training on iteration:256    \t/3400, val loss:53.91629409790039\n",
      "training on iteration:257    \t/3400, val loss:53.888248443603516\n",
      "training on iteration:258    \t/3400, val loss:53.872276306152344\n",
      "training on iteration:259    \t/3400, val loss:53.86650466918945\n",
      "training on iteration:260    \t/3400, val loss:53.8201904296875\n",
      "training on iteration:261    \t/3400, val loss:53.83048629760742\n",
      "training on iteration:262    \t/3400, val loss:53.80478286743164\n",
      "training on iteration:263    \t/3400, val loss:53.75629806518555\n",
      "training on iteration:264    \t/3400, val loss:53.764427185058594\n",
      "training on iteration:265    \t/3400, val loss:53.61028289794922\n",
      "training on iteration:266    \t/3400, val loss:53.47597885131836\n",
      "training on iteration:267    \t/3400, val loss:53.38597869873047\n",
      "training on iteration:268    \t/3400, val loss:53.34507369995117\n",
      "training on iteration:269    \t/3400, val loss:53.297237396240234\n",
      "training on iteration:270    \t/3400, val loss:53.25777816772461\n",
      "training on iteration:271    \t/3400, val loss:53.13520431518555\n",
      "training on iteration:272    \t/3400, val loss:53.04330825805664\n",
      "training on iteration:273    \t/3400, val loss:52.90898513793945\n",
      "training on iteration:274    \t/3400, val loss:52.84906768798828\n",
      "training on iteration:275    \t/3400, val loss:52.8243408203125\n",
      "training on iteration:276    \t/3400, val loss:52.818172454833984\n",
      "training on iteration:277    \t/3400, val loss:52.78414535522461\n",
      "training on iteration:278    \t/3400, val loss:52.82290267944336\n",
      "training on iteration:279    \t/3400, val loss:52.82977294921875\n",
      "training on iteration:280    \t/3400, val loss:52.81036376953125\n",
      "training on iteration:281    \t/3400, val loss:52.831024169921875\n",
      "training on iteration:282    \t/3400, val loss:52.655277252197266\n",
      "training on iteration:283    \t/3400, val loss:52.487022399902344\n",
      "training on iteration:284    \t/3400, val loss:52.370723724365234\n",
      "training on iteration:285    \t/3400, val loss:52.310302734375\n",
      "training on iteration:286    \t/3400, val loss:52.25082778930664\n",
      "training on iteration:287    \t/3400, val loss:52.218955993652344\n",
      "training on iteration:288    \t/3400, val loss:52.138404846191406\n",
      "training on iteration:289    \t/3400, val loss:52.084049224853516\n",
      "training on iteration:290    \t/3400, val loss:51.968929290771484\n",
      "training on iteration:291    \t/3400, val loss:51.93381881713867\n",
      "training on iteration:292    \t/3400, val loss:51.910186767578125\n",
      "training on iteration:293    \t/3400, val loss:51.89676284790039\n",
      "training on iteration:294    \t/3400, val loss:51.83068084716797\n",
      "training on iteration:295    \t/3400, val loss:51.82815170288086\n",
      "training on iteration:296    \t/3400, val loss:51.78659439086914\n",
      "training on iteration:297    \t/3400, val loss:51.72616958618164\n",
      "training on iteration:298    \t/3400, val loss:51.73124313354492\n",
      "training on iteration:299    \t/3400, val loss:51.57335662841797\n",
      "training on iteration:300    \t/3400, val loss:51.45464324951172\n",
      "training on iteration:301    \t/3400, val loss:51.37717819213867\n",
      "training on iteration:302    \t/3400, val loss:51.348812103271484\n",
      "training on iteration:303    \t/3400, val loss:51.3199577331543\n",
      "training on iteration:304    \t/3400, val loss:51.30957794189453\n",
      "training on iteration:305    \t/3400, val loss:51.21706771850586\n",
      "training on iteration:306    \t/3400, val loss:51.15282440185547\n",
      "training on iteration:307    \t/3400, val loss:51.02772903442383\n",
      "training on iteration:308    \t/3400, val loss:50.9961051940918\n",
      "training on iteration:309    \t/3400, val loss:50.98127365112305\n",
      "training on iteration:310    \t/3400, val loss:50.97830581665039\n",
      "training on iteration:311    \t/3400, val loss:50.93870162963867\n",
      "training on iteration:312    \t/3400, val loss:50.97872543334961\n",
      "training on iteration:313    \t/3400, val loss:50.9809455871582\n",
      "training on iteration:314    \t/3400, val loss:50.96044158935547\n",
      "training on iteration:315    \t/3400, val loss:51.003517150878906\n",
      "training on iteration:316    \t/3400, val loss:50.84035110473633\n",
      "training on iteration:317    \t/3400, val loss:50.71099853515625\n",
      "training on iteration:318    \t/3400, val loss:50.63949203491211\n",
      "training on iteration:319    \t/3400, val loss:50.627403259277344\n",
      "training on iteration:320    \t/3400, val loss:50.618003845214844\n",
      "training on iteration:321    \t/3400, val loss:50.63632583618164\n",
      "training on iteration:322    \t/3400, val loss:50.577110290527344\n",
      "training on iteration:323    \t/3400, val loss:50.54151153564453\n",
      "training on iteration:324    \t/3400, val loss:50.43235397338867\n",
      "training on iteration:325    \t/3400, val loss:50.41770553588867\n",
      "training on iteration:326    \t/3400, val loss:50.42438507080078\n",
      "training on iteration:327    \t/3400, val loss:50.44443893432617\n",
      "training on iteration:328    \t/3400, val loss:50.41708755493164\n",
      "training on iteration:329    \t/3400, val loss:50.45623779296875\n",
      "training on iteration:330    \t/3400, val loss:50.45249557495117\n",
      "training on iteration:331    \t/3400, val loss:50.43384552001953\n",
      "training on iteration:332    \t/3400, val loss:50.48761749267578\n",
      "training on iteration:333    \t/3400, val loss:50.3482666015625\n",
      "training on iteration:334    \t/3400, val loss:50.246490478515625\n",
      "training on iteration:335    \t/3400, val loss:50.194156646728516\n",
      "training on iteration:336    \t/3400, val loss:50.200828552246094\n",
      "training on iteration:337    \t/3400, val loss:50.20762252807617\n",
      "training on iteration:338    \t/3400, val loss:50.23195266723633\n",
      "training on iteration:339    \t/3400, val loss:50.16775131225586\n",
      "training on iteration:340    \t/3400, val loss:50.136837005615234\n",
      "training on iteration:341    \t/3400, val loss:50.024845123291016\n",
      "training on iteration:342    \t/3400, val loss:50.01192855834961\n",
      "training on iteration:343    \t/3400, val loss:50.023677825927734\n",
      "training on iteration:344    \t/3400, val loss:50.04867935180664\n",
      "training on iteration:345    \t/3400, val loss:50.023193359375\n",
      "training on iteration:346    \t/3400, val loss:50.06793975830078\n",
      "training on iteration:347    \t/3400, val loss:50.066314697265625\n",
      "training on iteration:348    \t/3400, val loss:50.04730987548828\n",
      "training on iteration:349    \t/3400, val loss:50.098026275634766\n",
      "training on iteration:350    \t/3400, val loss:49.951961517333984\n",
      "training on iteration:351    \t/3400, val loss:49.8488655090332\n",
      "training on iteration:352    \t/3400, val loss:49.80252456665039\n",
      "training on iteration:353    \t/3400, val loss:49.81633758544922\n",
      "training on iteration:354    \t/3400, val loss:49.82776641845703\n",
      "training on iteration:355    \t/3400, val loss:49.85588455200195\n",
      "training on iteration:356    \t/3400, val loss:49.74374771118164\n",
      "training on iteration:357    \t/3400, val loss:49.67504119873047\n",
      "training on iteration:358    \t/3400, val loss:49.548583984375\n",
      "training on iteration:359    \t/3400, val loss:49.5220832824707\n",
      "training on iteration:360    \t/3400, val loss:49.5386962890625\n",
      "training on iteration:361    \t/3400, val loss:49.58046340942383\n",
      "training on iteration:362    \t/3400, val loss:49.57887649536133\n",
      "training on iteration:363    \t/3400, val loss:49.64813995361328\n",
      "training on iteration:364    \t/3400, val loss:49.65747833251953\n",
      "training on iteration:365    \t/3400, val loss:49.63408660888672\n",
      "training on iteration:366    \t/3400, val loss:49.67013168334961\n",
      "training on iteration:367    \t/3400, val loss:49.482154846191406\n",
      "training on iteration:368    \t/3400, val loss:49.35432815551758\n",
      "training on iteration:369    \t/3400, val loss:49.30206298828125\n",
      "training on iteration:370    \t/3400, val loss:49.30478286743164\n",
      "training on iteration:371    \t/3400, val loss:49.30781173706055\n",
      "training on iteration:372    \t/3400, val loss:49.33781051635742\n",
      "training on iteration:373    \t/3400, val loss:49.27003860473633\n",
      "training on iteration:374    \t/3400, val loss:49.24053955078125\n",
      "training on iteration:375    \t/3400, val loss:49.12038040161133\n",
      "training on iteration:376    \t/3400, val loss:49.10025405883789\n",
      "training on iteration:377    \t/3400, val loss:49.10182571411133\n",
      "training on iteration:378    \t/3400, val loss:49.10890579223633\n",
      "training on iteration:379    \t/3400, val loss:49.05078887939453\n",
      "training on iteration:380    \t/3400, val loss:49.0521240234375\n",
      "training on iteration:381    \t/3400, val loss:49.0109748840332\n",
      "training on iteration:382    \t/3400, val loss:48.96576690673828\n",
      "training on iteration:383    \t/3400, val loss:49.02716064453125\n",
      "training on iteration:384    \t/3400, val loss:48.90218734741211\n",
      "training on iteration:385    \t/3400, val loss:48.814117431640625\n",
      "training on iteration:386    \t/3400, val loss:48.78396987915039\n",
      "training on iteration:387    \t/3400, val loss:48.8060188293457\n",
      "training on iteration:388    \t/3400, val loss:48.81179428100586\n",
      "training on iteration:389    \t/3400, val loss:48.82071304321289\n",
      "training on iteration:390    \t/3400, val loss:48.66976547241211\n",
      "training on iteration:391    \t/3400, val loss:48.568904876708984\n",
      "training on iteration:392    \t/3400, val loss:48.40142822265625\n",
      "training on iteration:393    \t/3400, val loss:48.35129928588867\n",
      "training on iteration:394    \t/3400, val loss:48.34949493408203\n",
      "training on iteration:395    \t/3400, val loss:48.37825393676758\n",
      "training on iteration:396    \t/3400, val loss:48.36040496826172\n",
      "training on iteration:397    \t/3400, val loss:48.4140625\n",
      "training on iteration:398    \t/3400, val loss:48.40594482421875\n",
      "training on iteration:399    \t/3400, val loss:48.37413787841797\n",
      "training on iteration:400    \t/3400, val loss:48.41525650024414\n",
      "training on iteration:401    \t/3400, val loss:48.22550582885742\n",
      "training on iteration:402    \t/3400, val loss:48.089359283447266\n",
      "training on iteration:403    \t/3400, val loss:48.03314208984375\n",
      "training on iteration:404    \t/3400, val loss:48.04533004760742\n",
      "training on iteration:405    \t/3400, val loss:48.057369232177734\n",
      "training on iteration:406    \t/3400, val loss:48.0877685546875\n",
      "training on iteration:407    \t/3400, val loss:47.96702194213867\n",
      "training on iteration:408    \t/3400, val loss:47.87727737426758\n",
      "training on iteration:409    \t/3400, val loss:47.689056396484375\n",
      "training on iteration:410    \t/3400, val loss:47.62110137939453\n",
      "training on iteration:411    \t/3400, val loss:47.57609176635742\n",
      "training on iteration:412    \t/3400, val loss:47.55221939086914\n",
      "training on iteration:413    \t/3400, val loss:47.48941421508789\n",
      "training on iteration:414    \t/3400, val loss:47.53364181518555\n",
      "training on iteration:415    \t/3400, val loss:47.53885269165039\n",
      "training on iteration:416    \t/3400, val loss:47.52397537231445\n",
      "training on iteration:417    \t/3400, val loss:47.58797073364258\n",
      "training on iteration:418    \t/3400, val loss:47.413265228271484\n",
      "training on iteration:419    \t/3400, val loss:47.28018569946289\n",
      "training on iteration:420    \t/3400, val loss:47.222129821777344\n",
      "training on iteration:421    \t/3400, val loss:47.21322250366211\n",
      "training on iteration:422    \t/3400, val loss:47.18269729614258\n",
      "training on iteration:423    \t/3400, val loss:47.15705108642578\n",
      "training on iteration:424    \t/3400, val loss:46.9705810546875\n",
      "training on iteration:425    \t/3400, val loss:46.84932327270508\n",
      "training on iteration:426    \t/3400, val loss:46.66071701049805\n",
      "training on iteration:427    \t/3400, val loss:46.591651916503906\n",
      "training on iteration:428    \t/3400, val loss:46.57302474975586\n",
      "training on iteration:429    \t/3400, val loss:46.59024429321289\n",
      "training on iteration:430    \t/3400, val loss:46.54906463623047\n",
      "training on iteration:431    \t/3400, val loss:46.56507110595703\n",
      "training on iteration:432    \t/3400, val loss:46.52011489868164\n",
      "training on iteration:433    \t/3400, val loss:46.45894241333008\n",
      "training on iteration:434    \t/3400, val loss:46.49070358276367\n",
      "training on iteration:435    \t/3400, val loss:46.27825927734375\n",
      "training on iteration:436    \t/3400, val loss:46.12134552001953\n",
      "training on iteration:437    \t/3400, val loss:46.043697357177734\n",
      "training on iteration:438    \t/3400, val loss:46.0327033996582\n",
      "training on iteration:439    \t/3400, val loss:46.03316116333008\n",
      "training on iteration:440    \t/3400, val loss:46.048160552978516\n",
      "training on iteration:441    \t/3400, val loss:45.86281967163086\n",
      "training on iteration:442    \t/3400, val loss:45.73186492919922\n",
      "training on iteration:443    \t/3400, val loss:45.495513916015625\n",
      "training on iteration:444    \t/3400, val loss:45.39809036254883\n",
      "training on iteration:445    \t/3400, val loss:45.3516731262207\n",
      "training on iteration:446    \t/3400, val loss:45.33560562133789\n",
      "training on iteration:447    \t/3400, val loss:45.27300262451172\n",
      "training on iteration:448    \t/3400, val loss:45.29707717895508\n",
      "training on iteration:449    \t/3400, val loss:45.26821517944336\n",
      "training on iteration:450    \t/3400, val loss:45.22907638549805\n",
      "training on iteration:451    \t/3400, val loss:45.28998565673828\n",
      "training on iteration:452    \t/3400, val loss:45.09272003173828\n",
      "training on iteration:453    \t/3400, val loss:44.96067428588867\n",
      "training on iteration:454    \t/3400, val loss:44.91016387939453\n",
      "training on iteration:455    \t/3400, val loss:44.928165435791016\n",
      "training on iteration:456    \t/3400, val loss:44.92134475708008\n",
      "training on iteration:457    \t/3400, val loss:44.91572189331055\n",
      "training on iteration:458    \t/3400, val loss:44.70650863647461\n",
      "training on iteration:459    \t/3400, val loss:44.57476043701172\n",
      "training on iteration:460    \t/3400, val loss:44.37273025512695\n",
      "training on iteration:461    \t/3400, val loss:44.31534957885742\n",
      "training on iteration:462    \t/3400, val loss:44.3420524597168\n",
      "training on iteration:463    \t/3400, val loss:44.41389846801758\n",
      "training on iteration:464    \t/3400, val loss:44.42692184448242\n",
      "training on iteration:465    \t/3400, val loss:44.51760482788086\n",
      "training on iteration:466    \t/3400, val loss:44.53963851928711\n",
      "training on iteration:467    \t/3400, val loss:44.5136833190918\n",
      "training on iteration:468    \t/3400, val loss:44.57149887084961\n",
      "training on iteration:469    \t/3400, val loss:44.380374908447266\n",
      "training on iteration:470    \t/3400, val loss:44.29868698120117\n",
      "training on iteration:471    \t/3400, val loss:44.31151580810547\n",
      "training on iteration:472    \t/3400, val loss:44.404266357421875\n",
      "training on iteration:473    \t/3400, val loss:44.501129150390625\n",
      "training on iteration:474    \t/3400, val loss:44.592315673828125\n",
      "training on iteration:475    \t/3400, val loss:44.43709945678711\n",
      "training on iteration:476    \t/3400, val loss:44.31698226928711\n",
      "training on iteration:477    \t/3400, val loss:44.11260223388672\n",
      "training on iteration:478    \t/3400, val loss:44.070003509521484\n",
      "training on iteration:479    \t/3400, val loss:44.10783767700195\n",
      "training on iteration:480    \t/3400, val loss:44.17522430419922\n",
      "training on iteration:481    \t/3400, val loss:44.19675064086914\n",
      "training on iteration:482    \t/3400, val loss:44.33245086669922\n",
      "training on iteration:483    \t/3400, val loss:44.41786575317383\n",
      "training on iteration:484    \t/3400, val loss:44.475643157958984\n",
      "training on iteration:485    \t/3400, val loss:44.62095260620117\n",
      "training on iteration:486    \t/3400, val loss:44.44652557373047\n",
      "training on iteration:487    \t/3400, val loss:44.334877014160156\n",
      "training on iteration:488    \t/3400, val loss:44.320674896240234\n",
      "training on iteration:489    \t/3400, val loss:44.38525390625\n",
      "training on iteration:490    \t/3400, val loss:44.457942962646484\n",
      "training on iteration:491    \t/3400, val loss:44.5307731628418\n",
      "training on iteration:492    \t/3400, val loss:44.400352478027344\n",
      "training on iteration:493    \t/3400, val loss:44.328617095947266\n",
      "training on iteration:494    \t/3400, val loss:44.144039154052734\n",
      "training on iteration:495    \t/3400, val loss:44.09846878051758\n",
      "training on iteration:496    \t/3400, val loss:44.13131332397461\n",
      "training on iteration:497    \t/3400, val loss:44.21048355102539\n",
      "training on iteration:498    \t/3400, val loss:44.23548126220703\n",
      "training on iteration:499    \t/3400, val loss:44.34622573852539\n",
      "training on iteration:500    \t/3400, val loss:44.39571762084961\n",
      "training on iteration:501    \t/3400, val loss:44.43463897705078\n",
      "training on iteration:502    \t/3400, val loss:44.56230545043945\n",
      "training on iteration:503    \t/3400, val loss:44.39678955078125\n",
      "training on iteration:504    \t/3400, val loss:44.29619598388672\n",
      "training on iteration:505    \t/3400, val loss:44.2841796875\n",
      "training on iteration:506    \t/3400, val loss:44.34919357299805\n",
      "training on iteration:507    \t/3400, val loss:44.39550018310547\n",
      "training on iteration:508    \t/3400, val loss:44.44237518310547\n",
      "training on iteration:509    \t/3400, val loss:44.29740524291992\n",
      "training on iteration:510    \t/3400, val loss:44.22893142700195\n",
      "training on iteration:511    \t/3400, val loss:44.05133819580078\n",
      "training on iteration:512    \t/3400, val loss:44.02169418334961\n",
      "training on iteration:513    \t/3400, val loss:44.076412200927734\n",
      "training on iteration:514    \t/3400, val loss:44.152225494384766\n",
      "training on iteration:515    \t/3400, val loss:44.15654754638672\n",
      "training on iteration:516    \t/3400, val loss:44.22990036010742\n",
      "training on iteration:517    \t/3400, val loss:44.25265121459961\n",
      "training on iteration:518    \t/3400, val loss:44.26816177368164\n",
      "training on iteration:519    \t/3400, val loss:44.38813018798828\n",
      "training on iteration:520    \t/3400, val loss:44.237064361572266\n",
      "training on iteration:521    \t/3400, val loss:44.15079879760742\n",
      "training on iteration:522    \t/3400, val loss:44.157955169677734\n",
      "training on iteration:523    \t/3400, val loss:44.24787521362305\n",
      "training on iteration:524    \t/3400, val loss:44.31674575805664\n",
      "training on iteration:525    \t/3400, val loss:44.36417770385742\n",
      "training on iteration:526    \t/3400, val loss:44.1491813659668\n",
      "training on iteration:527    \t/3400, val loss:44.035545349121094\n",
      "training on iteration:528    \t/3400, val loss:43.864627838134766\n",
      "training on iteration:529    \t/3400, val loss:43.83515548706055\n",
      "training on iteration:530    \t/3400, val loss:43.900367736816406\n",
      "training on iteration:531    \t/3400, val loss:44.014495849609375\n",
      "training on iteration:532    \t/3400, val loss:44.06043243408203\n",
      "training on iteration:533    \t/3400, val loss:44.16493606567383\n",
      "training on iteration:534    \t/3400, val loss:44.18899917602539\n",
      "training on iteration:535    \t/3400, val loss:44.187835693359375\n",
      "training on iteration:536    \t/3400, val loss:44.28596115112305\n",
      "training on iteration:537    \t/3400, val loss:44.12088394165039\n",
      "training on iteration:538    \t/3400, val loss:44.04128646850586\n",
      "training on iteration:539    \t/3400, val loss:44.054630279541016\n",
      "training on iteration:540    \t/3400, val loss:44.15552520751953\n",
      "training on iteration:541    \t/3400, val loss:44.24911117553711\n",
      "training on iteration:542    \t/3400, val loss:44.31741714477539\n",
      "training on iteration:543    \t/3400, val loss:44.11438751220703\n",
      "training on iteration:544    \t/3400, val loss:43.97760009765625\n",
      "training on iteration:545    \t/3400, val loss:43.77653121948242\n",
      "training on iteration:546    \t/3400, val loss:43.7313346862793\n",
      "training on iteration:547    \t/3400, val loss:43.787986755371094\n",
      "training on iteration:548    \t/3400, val loss:43.90911865234375\n",
      "training on iteration:549    \t/3400, val loss:43.96694564819336\n",
      "training on iteration:550    \t/3400, val loss:44.082672119140625\n",
      "training on iteration:551    \t/3400, val loss:44.10841751098633\n",
      "training on iteration:552    \t/3400, val loss:44.10999298095703\n",
      "training on iteration:553    \t/3400, val loss:44.22129440307617\n",
      "training on iteration:554    \t/3400, val loss:44.05869674682617\n",
      "training on iteration:555    \t/3400, val loss:43.97774887084961\n",
      "training on iteration:556    \t/3400, val loss:43.99375534057617\n",
      "training on iteration:557    \t/3400, val loss:44.100730895996094\n",
      "training on iteration:558    \t/3400, val loss:44.178199768066406\n",
      "training on iteration:559    \t/3400, val loss:44.229644775390625\n",
      "training on iteration:560    \t/3400, val loss:44.01956558227539\n",
      "training on iteration:561    \t/3400, val loss:43.87459182739258\n",
      "training on iteration:562    \t/3400, val loss:43.670204162597656\n",
      "training on iteration:563    \t/3400, val loss:43.61800765991211\n",
      "training on iteration:564    \t/3400, val loss:43.66237258911133\n",
      "training on iteration:565    \t/3400, val loss:43.78047561645508\n",
      "training on iteration:566    \t/3400, val loss:43.86185073852539\n",
      "training on iteration:567    \t/3400, val loss:44.04880142211914\n",
      "training on iteration:568    \t/3400, val loss:44.140113830566406\n",
      "training on iteration:569    \t/3400, val loss:44.16604995727539\n",
      "training on iteration:570    \t/3400, val loss:44.21302032470703\n",
      "training on iteration:571    \t/3400, val loss:43.981597900390625\n",
      "training on iteration:572    \t/3400, val loss:43.84945297241211\n",
      "training on iteration:573    \t/3400, val loss:43.827606201171875\n",
      "training on iteration:574    \t/3400, val loss:43.90544891357422\n",
      "training on iteration:575    \t/3400, val loss:44.033382415771484\n",
      "training on iteration:576    \t/3400, val loss:44.15860366821289\n",
      "training on iteration:577    \t/3400, val loss:43.96515655517578\n",
      "training on iteration:578    \t/3400, val loss:43.82895278930664\n",
      "training on iteration:579    \t/3400, val loss:43.605018615722656\n",
      "training on iteration:580    \t/3400, val loss:43.54469680786133\n",
      "training on iteration:581    \t/3400, val loss:43.57948684692383\n",
      "training on iteration:582    \t/3400, val loss:43.68456268310547\n",
      "training on iteration:583    \t/3400, val loss:43.76298141479492\n",
      "training on iteration:584    \t/3400, val loss:43.937530517578125\n",
      "training on iteration:585    \t/3400, val loss:44.02826690673828\n",
      "training on iteration:586    \t/3400, val loss:44.0621223449707\n",
      "training on iteration:587    \t/3400, val loss:44.14144515991211\n",
      "training on iteration:588    \t/3400, val loss:43.921783447265625\n",
      "training on iteration:589    \t/3400, val loss:43.80194854736328\n",
      "training on iteration:590    \t/3400, val loss:43.77812957763672\n",
      "training on iteration:591    \t/3400, val loss:43.861854553222656\n",
      "training on iteration:592    \t/3400, val loss:43.95473861694336\n",
      "training on iteration:593    \t/3400, val loss:44.02643585205078\n",
      "training on iteration:594    \t/3400, val loss:43.84804916381836\n",
      "training on iteration:595    \t/3400, val loss:43.73081588745117\n",
      "training on iteration:596    \t/3400, val loss:43.52250289916992\n",
      "training on iteration:597    \t/3400, val loss:43.46415328979492\n",
      "training on iteration:598    \t/3400, val loss:43.49658203125\n",
      "training on iteration:599    \t/3400, val loss:43.601219177246094\n",
      "training on iteration:600    \t/3400, val loss:43.68485641479492\n",
      "training on iteration:601    \t/3400, val loss:43.90069580078125\n",
      "training on iteration:602    \t/3400, val loss:44.00850296020508\n",
      "training on iteration:603    \t/3400, val loss:44.0425910949707\n",
      "training on iteration:604    \t/3400, val loss:44.0994873046875\n",
      "training on iteration:605    \t/3400, val loss:43.838096618652344\n",
      "training on iteration:606    \t/3400, val loss:43.68545150756836\n",
      "training on iteration:607    \t/3400, val loss:43.65560531616211\n",
      "training on iteration:608    \t/3400, val loss:43.73165512084961\n",
      "training on iteration:609    \t/3400, val loss:43.84518051147461\n",
      "training on iteration:610    \t/3400, val loss:43.94025421142578\n",
      "training on iteration:611    \t/3400, val loss:43.78509521484375\n",
      "training on iteration:612    \t/3400, val loss:43.669776916503906\n",
      "training on iteration:613    \t/3400, val loss:43.44658279418945\n",
      "training on iteration:614    \t/3400, val loss:43.386253356933594\n",
      "training on iteration:615    \t/3400, val loss:43.42569351196289\n",
      "training on iteration:616    \t/3400, val loss:43.545467376708984\n",
      "training on iteration:617    \t/3400, val loss:43.620277404785156\n",
      "training on iteration:618    \t/3400, val loss:43.816036224365234\n",
      "training on iteration:619    \t/3400, val loss:43.90510940551758\n",
      "training on iteration:620    \t/3400, val loss:43.91915512084961\n",
      "training on iteration:621    \t/3400, val loss:43.97938919067383\n",
      "training on iteration:622    \t/3400, val loss:43.73904037475586\n",
      "training on iteration:623    \t/3400, val loss:43.605552673339844\n",
      "training on iteration:624    \t/3400, val loss:43.595699310302734\n",
      "training on iteration:625    \t/3400, val loss:43.68729019165039\n",
      "training on iteration:626    \t/3400, val loss:43.792396545410156\n",
      "training on iteration:627    \t/3400, val loss:43.87503433227539\n",
      "training on iteration:628    \t/3400, val loss:43.67127227783203\n",
      "training on iteration:629    \t/3400, val loss:43.5473747253418\n",
      "training on iteration:630    \t/3400, val loss:43.344661712646484\n",
      "training on iteration:631    \t/3400, val loss:43.294437408447266\n",
      "training on iteration:632    \t/3400, val loss:43.345375061035156\n",
      "training on iteration:633    \t/3400, val loss:43.48828887939453\n",
      "training on iteration:634    \t/3400, val loss:43.57985305786133\n",
      "training on iteration:635    \t/3400, val loss:43.78032684326172\n",
      "training on iteration:636    \t/3400, val loss:43.858158111572266\n",
      "training on iteration:637    \t/3400, val loss:43.85579299926758\n",
      "training on iteration:638    \t/3400, val loss:43.882415771484375\n",
      "training on iteration:639    \t/3400, val loss:43.6176872253418\n",
      "training on iteration:640    \t/3400, val loss:43.47209930419922\n",
      "training on iteration:641    \t/3400, val loss:43.46051025390625\n",
      "training on iteration:642    \t/3400, val loss:43.58773422241211\n",
      "training on iteration:643    \t/3400, val loss:43.74932098388672\n",
      "training on iteration:644    \t/3400, val loss:43.87717056274414\n",
      "training on iteration:645    \t/3400, val loss:43.69635009765625\n",
      "training on iteration:646    \t/3400, val loss:43.53897476196289\n",
      "training on iteration:647    \t/3400, val loss:43.28656768798828\n",
      "training on iteration:648    \t/3400, val loss:43.21612548828125\n",
      "training on iteration:649    \t/3400, val loss:43.24973678588867\n",
      "training on iteration:650    \t/3400, val loss:43.38460922241211\n",
      "training on iteration:651    \t/3400, val loss:43.50055694580078\n",
      "training on iteration:652    \t/3400, val loss:43.73227310180664\n",
      "training on iteration:653    \t/3400, val loss:43.83002471923828\n",
      "training on iteration:654    \t/3400, val loss:43.83633804321289\n",
      "training on iteration:655    \t/3400, val loss:43.85294723510742\n",
      "training on iteration:656    \t/3400, val loss:43.55750274658203\n",
      "training on iteration:657    \t/3400, val loss:43.39614486694336\n",
      "training on iteration:658    \t/3400, val loss:43.373104095458984\n",
      "training on iteration:659    \t/3400, val loss:43.48408126831055\n",
      "training on iteration:660    \t/3400, val loss:43.66179656982422\n",
      "training on iteration:661    \t/3400, val loss:43.82250213623047\n",
      "training on iteration:662    \t/3400, val loss:43.65117263793945\n",
      "training on iteration:663    \t/3400, val loss:43.4800910949707\n",
      "training on iteration:664    \t/3400, val loss:43.21796417236328\n",
      "training on iteration:665    \t/3400, val loss:43.14786148071289\n",
      "training on iteration:666    \t/3400, val loss:43.179847717285156\n",
      "training on iteration:667    \t/3400, val loss:43.31355667114258\n",
      "training on iteration:668    \t/3400, val loss:43.4307861328125\n",
      "training on iteration:669    \t/3400, val loss:43.652198791503906\n",
      "training on iteration:670    \t/3400, val loss:43.72966384887695\n",
      "training on iteration:671    \t/3400, val loss:43.7317008972168\n",
      "training on iteration:672    \t/3400, val loss:43.75350570678711\n",
      "training on iteration:673    \t/3400, val loss:43.46800994873047\n",
      "training on iteration:674    \t/3400, val loss:43.319557189941406\n",
      "training on iteration:675    \t/3400, val loss:43.31757736206055\n",
      "training on iteration:676    \t/3400, val loss:43.46622848510742\n",
      "training on iteration:677    \t/3400, val loss:43.63603973388672\n",
      "training on iteration:678    \t/3400, val loss:43.738765716552734\n",
      "training on iteration:679    \t/3400, val loss:43.51858901977539\n",
      "training on iteration:680    \t/3400, val loss:43.36345672607422\n",
      "training on iteration:681    \t/3400, val loss:43.13364791870117\n",
      "training on iteration:682    \t/3400, val loss:43.08428192138672\n",
      "training on iteration:683    \t/3400, val loss:43.14729690551758\n",
      "training on iteration:684    \t/3400, val loss:43.31962203979492\n",
      "training on iteration:685    \t/3400, val loss:43.422271728515625\n",
      "training on iteration:686    \t/3400, val loss:43.593841552734375\n",
      "training on iteration:687    \t/3400, val loss:43.62734603881836\n",
      "training on iteration:688    \t/3400, val loss:43.61212921142578\n",
      "training on iteration:689    \t/3400, val loss:43.667015075683594\n",
      "training on iteration:690    \t/3400, val loss:43.41815185546875\n",
      "training on iteration:691    \t/3400, val loss:43.28676223754883\n",
      "training on iteration:692    \t/3400, val loss:43.29983901977539\n",
      "training on iteration:693    \t/3400, val loss:43.44244384765625\n",
      "training on iteration:694    \t/3400, val loss:43.567161560058594\n",
      "training on iteration:695    \t/3400, val loss:43.60901641845703\n",
      "training on iteration:696    \t/3400, val loss:43.40061569213867\n",
      "training on iteration:697    \t/3400, val loss:43.28860092163086\n",
      "training on iteration:698    \t/3400, val loss:43.093605041503906\n",
      "training on iteration:699    \t/3400, val loss:43.067108154296875\n",
      "training on iteration:700    \t/3400, val loss:43.145790100097656\n",
      "training on iteration:701    \t/3400, val loss:43.309104919433594\n",
      "training on iteration:702    \t/3400, val loss:43.37882614135742\n",
      "training on iteration:703    \t/3400, val loss:43.49170684814453\n",
      "training on iteration:704    \t/3400, val loss:43.48883819580078\n",
      "training on iteration:705    \t/3400, val loss:43.47964096069336\n",
      "training on iteration:706    \t/3400, val loss:43.57600784301758\n",
      "training on iteration:707    \t/3400, val loss:43.365760803222656\n",
      "training on iteration:708    \t/3400, val loss:43.248165130615234\n",
      "training on iteration:709    \t/3400, val loss:43.27611541748047\n",
      "training on iteration:710    \t/3400, val loss:43.42236328125\n",
      "training on iteration:711    \t/3400, val loss:43.50989532470703\n",
      "training on iteration:712    \t/3400, val loss:43.51763916015625\n",
      "training on iteration:713    \t/3400, val loss:43.30181121826172\n",
      "training on iteration:714    \t/3400, val loss:43.18177032470703\n",
      "training on iteration:715    \t/3400, val loss:42.99702835083008\n",
      "training on iteration:716    \t/3400, val loss:42.98661422729492\n",
      "training on iteration:717    \t/3400, val loss:43.10258102416992\n",
      "training on iteration:718    \t/3400, val loss:43.31271743774414\n",
      "training on iteration:719    \t/3400, val loss:43.39617919921875\n",
      "training on iteration:720    \t/3400, val loss:43.48067092895508\n",
      "training on iteration:721    \t/3400, val loss:43.428382873535156\n",
      "training on iteration:722    \t/3400, val loss:43.388545989990234\n",
      "training on iteration:723    \t/3400, val loss:43.47606658935547\n",
      "training on iteration:724    \t/3400, val loss:43.266319274902344\n",
      "training on iteration:725    \t/3400, val loss:43.16344451904297\n",
      "training on iteration:726    \t/3400, val loss:43.22809600830078\n",
      "training on iteration:727    \t/3400, val loss:43.42402267456055\n",
      "training on iteration:728    \t/3400, val loss:43.549354553222656\n",
      "training on iteration:729    \t/3400, val loss:43.51679229736328\n",
      "training on iteration:730    \t/3400, val loss:43.21578598022461\n",
      "training on iteration:731    \t/3400, val loss:43.05196762084961\n",
      "training on iteration:732    \t/3400, val loss:42.87795639038086\n",
      "training on iteration:733    \t/3400, val loss:42.8643798828125\n",
      "training on iteration:734    \t/3400, val loss:42.9892578125\n",
      "training on iteration:735    \t/3400, val loss:43.26015090942383\n",
      "training on iteration:736    \t/3400, val loss:43.41817092895508\n",
      "training on iteration:737    \t/3400, val loss:43.52431106567383\n",
      "training on iteration:738    \t/3400, val loss:43.44862747192383\n",
      "training on iteration:739    \t/3400, val loss:43.345699310302734\n",
      "training on iteration:740    \t/3400, val loss:43.351558685302734\n",
      "training on iteration:741    \t/3400, val loss:43.09990692138672\n",
      "training on iteration:742    \t/3400, val loss:42.99692916870117\n",
      "training on iteration:743    \t/3400, val loss:43.078941345214844\n",
      "training on iteration:744    \t/3400, val loss:43.30894088745117\n",
      "training on iteration:745    \t/3400, val loss:43.50636672973633\n",
      "training on iteration:746    \t/3400, val loss:43.520050048828125\n",
      "training on iteration:747    \t/3400, val loss:43.24039840698242\n",
      "training on iteration:748    \t/3400, val loss:43.06853485107422\n",
      "training on iteration:749    \t/3400, val loss:42.86053466796875\n",
      "training on iteration:750    \t/3400, val loss:42.833709716796875\n",
      "training on iteration:751    \t/3400, val loss:42.94607162475586\n",
      "training on iteration:752    \t/3400, val loss:43.211952209472656\n",
      "training on iteration:753    \t/3400, val loss:43.36385726928711\n",
      "training on iteration:754    \t/3400, val loss:43.47003936767578\n",
      "training on iteration:755    \t/3400, val loss:43.39826202392578\n",
      "training on iteration:756    \t/3400, val loss:43.275115966796875\n",
      "training on iteration:757    \t/3400, val loss:43.247703552246094\n",
      "training on iteration:758    \t/3400, val loss:42.99490737915039\n",
      "training on iteration:759    \t/3400, val loss:42.90422439575195\n",
      "training on iteration:760    \t/3400, val loss:43.0150032043457\n",
      "training on iteration:761    \t/3400, val loss:43.28438949584961\n",
      "training on iteration:762    \t/3400, val loss:43.49182891845703\n",
      "training on iteration:763    \t/3400, val loss:43.49053955078125\n",
      "training on iteration:764    \t/3400, val loss:43.16982650756836\n",
      "training on iteration:765    \t/3400, val loss:42.976539611816406\n",
      "training on iteration:766    \t/3400, val loss:42.767242431640625\n",
      "training on iteration:767    \t/3400, val loss:42.73828887939453\n",
      "training on iteration:768    \t/3400, val loss:42.86415481567383\n",
      "training on iteration:769    \t/3400, val loss:43.16690444946289\n",
      "training on iteration:770    \t/3400, val loss:43.35360336303711\n",
      "training on iteration:771    \t/3400, val loss:43.48524856567383\n",
      "training on iteration:772    \t/3400, val loss:43.38631057739258\n",
      "training on iteration:773    \t/3400, val loss:43.264373779296875\n",
      "training on iteration:774    \t/3400, val loss:43.25513458251953\n",
      "training on iteration:775    \t/3400, val loss:43.00236129760742\n",
      "training on iteration:776    \t/3400, val loss:42.89529800415039\n",
      "training on iteration:777    \t/3400, val loss:43.001548767089844\n",
      "training on iteration:778    \t/3400, val loss:43.24026870727539\n",
      "training on iteration:779    \t/3400, val loss:43.39748001098633\n",
      "training on iteration:780    \t/3400, val loss:43.37318801879883\n",
      "training on iteration:781    \t/3400, val loss:43.090538024902344\n",
      "training on iteration:782    \t/3400, val loss:42.920772552490234\n",
      "training on iteration:783    \t/3400, val loss:42.72172164916992\n",
      "training on iteration:784    \t/3400, val loss:42.70319366455078\n",
      "training on iteration:785    \t/3400, val loss:42.85581588745117\n",
      "training on iteration:786    \t/3400, val loss:43.17893600463867\n",
      "training on iteration:787    \t/3400, val loss:43.335052490234375\n",
      "training on iteration:788    \t/3400, val loss:43.40764617919922\n",
      "training on iteration:789    \t/3400, val loss:43.26608657836914\n",
      "training on iteration:790    \t/3400, val loss:43.147247314453125\n",
      "training on iteration:791    \t/3400, val loss:43.13069534301758\n",
      "training on iteration:792    \t/3400, val loss:42.89168167114258\n",
      "training on iteration:793    \t/3400, val loss:42.81821060180664\n",
      "training on iteration:794    \t/3400, val loss:42.95457077026367\n",
      "training on iteration:795    \t/3400, val loss:43.20755386352539\n",
      "training on iteration:796    \t/3400, val loss:43.35029983520508\n",
      "training on iteration:797    \t/3400, val loss:43.28934097290039\n",
      "training on iteration:798    \t/3400, val loss:43.03978729248047\n",
      "training on iteration:799    \t/3400, val loss:42.88583755493164\n",
      "training on iteration:800    \t/3400, val loss:42.68244552612305\n",
      "training on iteration:801    \t/3400, val loss:42.66549301147461\n",
      "training on iteration:802    \t/3400, val loss:42.80601119995117\n",
      "training on iteration:803    \t/3400, val loss:43.11360549926758\n",
      "training on iteration:804    \t/3400, val loss:43.24848175048828\n",
      "training on iteration:805    \t/3400, val loss:43.31037139892578\n",
      "training on iteration:806    \t/3400, val loss:43.194671630859375\n",
      "training on iteration:807    \t/3400, val loss:43.09975814819336\n",
      "training on iteration:808    \t/3400, val loss:43.09770965576172\n",
      "training on iteration:809    \t/3400, val loss:42.86260223388672\n",
      "training on iteration:810    \t/3400, val loss:42.7803840637207\n",
      "training on iteration:811    \t/3400, val loss:42.89133834838867\n",
      "training on iteration:812    \t/3400, val loss:43.11443328857422\n",
      "training on iteration:813    \t/3400, val loss:43.257972717285156\n",
      "training on iteration:814    \t/3400, val loss:43.20262908935547\n",
      "training on iteration:815    \t/3400, val loss:42.95665740966797\n",
      "training on iteration:816    \t/3400, val loss:42.80132293701172\n",
      "training on iteration:817    \t/3400, val loss:42.615196228027344\n",
      "training on iteration:818    \t/3400, val loss:42.6158561706543\n",
      "training on iteration:819    \t/3400, val loss:42.77982711791992\n",
      "training on iteration:820    \t/3400, val loss:43.07806396484375\n",
      "training on iteration:821    \t/3400, val loss:43.195926666259766\n",
      "training on iteration:822    \t/3400, val loss:43.233924865722656\n",
      "training on iteration:823    \t/3400, val loss:43.091827392578125\n",
      "training on iteration:824    \t/3400, val loss:42.99857711791992\n",
      "training on iteration:825    \t/3400, val loss:43.012081146240234\n",
      "training on iteration:826    \t/3400, val loss:42.794883728027344\n",
      "training on iteration:827    \t/3400, val loss:42.73020553588867\n",
      "training on iteration:828    \t/3400, val loss:42.85144805908203\n",
      "training on iteration:829    \t/3400, val loss:43.07851028442383\n",
      "training on iteration:830    \t/3400, val loss:43.2012939453125\n",
      "training on iteration:831    \t/3400, val loss:43.13109588623047\n",
      "training on iteration:832    \t/3400, val loss:42.88334274291992\n",
      "training on iteration:833    \t/3400, val loss:42.72766876220703\n",
      "training on iteration:834    \t/3400, val loss:42.54452133178711\n",
      "training on iteration:835    \t/3400, val loss:42.55404281616211\n",
      "training on iteration:836    \t/3400, val loss:42.723907470703125\n",
      "training on iteration:837    \t/3400, val loss:43.026283264160156\n",
      "training on iteration:838    \t/3400, val loss:43.14492416381836\n",
      "training on iteration:839    \t/3400, val loss:43.1773796081543\n",
      "training on iteration:840    \t/3400, val loss:43.031551361083984\n",
      "training on iteration:841    \t/3400, val loss:42.939613342285156\n",
      "training on iteration:842    \t/3400, val loss:42.9647102355957\n",
      "training on iteration:843    \t/3400, val loss:42.749183654785156\n",
      "training on iteration:844    \t/3400, val loss:42.6992073059082\n",
      "training on iteration:845    \t/3400, val loss:42.818763732910156\n",
      "training on iteration:846    \t/3400, val loss:43.036434173583984\n",
      "training on iteration:847    \t/3400, val loss:43.15526580810547\n",
      "training on iteration:848    \t/3400, val loss:43.100311279296875\n",
      "training on iteration:849    \t/3400, val loss:42.85158920288086\n",
      "training on iteration:850    \t/3400, val loss:42.68671417236328\n",
      "training on iteration:851    \t/3400, val loss:42.49947738647461\n",
      "training on iteration:852    \t/3400, val loss:42.5263557434082\n",
      "training on iteration:853    \t/3400, val loss:42.69453048706055\n",
      "training on iteration:854    \t/3400, val loss:42.962947845458984\n",
      "training on iteration:855    \t/3400, val loss:43.07780838012695\n",
      "training on iteration:856    \t/3400, val loss:43.11328125\n",
      "training on iteration:857    \t/3400, val loss:42.95492172241211\n",
      "training on iteration:858    \t/3400, val loss:42.86174392700195\n",
      "training on iteration:859    \t/3400, val loss:42.87871551513672\n",
      "training on iteration:860    \t/3400, val loss:42.70331954956055\n",
      "training on iteration:861    \t/3400, val loss:42.66950225830078\n",
      "training on iteration:862    \t/3400, val loss:42.81686782836914\n",
      "training on iteration:863    \t/3400, val loss:43.02863693237305\n",
      "training on iteration:864    \t/3400, val loss:43.108802795410156\n",
      "training on iteration:865    \t/3400, val loss:42.98997116088867\n",
      "training on iteration:866    \t/3400, val loss:42.7330322265625\n",
      "training on iteration:867    \t/3400, val loss:42.56977081298828\n",
      "training on iteration:868    \t/3400, val loss:42.42180633544922\n",
      "training on iteration:869    \t/3400, val loss:42.46876907348633\n",
      "training on iteration:870    \t/3400, val loss:42.673133850097656\n",
      "training on iteration:871    \t/3400, val loss:42.95163345336914\n",
      "training on iteration:872    \t/3400, val loss:43.05180740356445\n",
      "training on iteration:873    \t/3400, val loss:43.0772590637207\n",
      "training on iteration:874    \t/3400, val loss:42.929500579833984\n",
      "training on iteration:875    \t/3400, val loss:42.80131149291992\n",
      "training on iteration:876    \t/3400, val loss:42.81528091430664\n",
      "training on iteration:877    \t/3400, val loss:42.633140563964844\n",
      "training on iteration:878    \t/3400, val loss:42.60404586791992\n",
      "training on iteration:879    \t/3400, val loss:42.7275276184082\n",
      "training on iteration:880    \t/3400, val loss:42.92747116088867\n",
      "training on iteration:881    \t/3400, val loss:43.05230712890625\n",
      "training on iteration:882    \t/3400, val loss:42.97617721557617\n",
      "training on iteration:883    \t/3400, val loss:42.71457290649414\n",
      "training on iteration:884    \t/3400, val loss:42.54099655151367\n",
      "training on iteration:885    \t/3400, val loss:42.39790725708008\n",
      "training on iteration:886    \t/3400, val loss:42.459754943847656\n",
      "training on iteration:887    \t/3400, val loss:42.67987060546875\n",
      "training on iteration:888    \t/3400, val loss:42.92300796508789\n",
      "training on iteration:889    \t/3400, val loss:42.967803955078125\n",
      "training on iteration:890    \t/3400, val loss:42.99077606201172\n",
      "training on iteration:891    \t/3400, val loss:42.83464813232422\n",
      "training on iteration:892    \t/3400, val loss:42.712120056152344\n",
      "training on iteration:893    \t/3400, val loss:42.73768997192383\n",
      "training on iteration:894    \t/3400, val loss:42.570804595947266\n",
      "training on iteration:895    \t/3400, val loss:42.54690933227539\n",
      "training on iteration:896    \t/3400, val loss:42.67587661743164\n",
      "training on iteration:897    \t/3400, val loss:42.92841720581055\n",
      "training on iteration:898    \t/3400, val loss:43.079315185546875\n",
      "training on iteration:899    \t/3400, val loss:42.96237564086914\n",
      "training on iteration:900    \t/3400, val loss:42.64564895629883\n",
      "training on iteration:901    \t/3400, val loss:42.47101974487305\n",
      "training on iteration:902    \t/3400, val loss:42.32117462158203\n",
      "training on iteration:903    \t/3400, val loss:42.36580276489258\n",
      "training on iteration:904    \t/3400, val loss:42.592987060546875\n",
      "training on iteration:905    \t/3400, val loss:42.93357467651367\n",
      "training on iteration:906    \t/3400, val loss:43.04024887084961\n",
      "training on iteration:907    \t/3400, val loss:43.017208099365234\n",
      "training on iteration:908    \t/3400, val loss:42.8021125793457\n",
      "training on iteration:909    \t/3400, val loss:42.66043472290039\n",
      "training on iteration:910    \t/3400, val loss:42.67616653442383\n",
      "training on iteration:911    \t/3400, val loss:42.53133773803711\n",
      "training on iteration:912    \t/3400, val loss:42.52602005004883\n",
      "training on iteration:913    \t/3400, val loss:42.68155288696289\n",
      "training on iteration:914    \t/3400, val loss:42.93285369873047\n",
      "training on iteration:915    \t/3400, val loss:43.007423400878906\n",
      "training on iteration:916    \t/3400, val loss:42.83858108520508\n",
      "training on iteration:917    \t/3400, val loss:42.5131721496582\n",
      "training on iteration:918    \t/3400, val loss:42.37443161010742\n",
      "training on iteration:919    \t/3400, val loss:42.294864654541016\n",
      "training on iteration:920    \t/3400, val loss:42.400264739990234\n",
      "training on iteration:921    \t/3400, val loss:42.687381744384766\n",
      "training on iteration:922    \t/3400, val loss:42.93518829345703\n",
      "training on iteration:923    \t/3400, val loss:42.92319107055664\n",
      "training on iteration:924    \t/3400, val loss:42.89529037475586\n",
      "training on iteration:925    \t/3400, val loss:42.69852066040039\n",
      "training on iteration:926    \t/3400, val loss:42.57457733154297\n",
      "training on iteration:927    \t/3400, val loss:42.631839752197266\n",
      "training on iteration:928    \t/3400, val loss:42.513099670410156\n",
      "training on iteration:929    \t/3400, val loss:42.49619674682617\n",
      "training on iteration:930    \t/3400, val loss:42.645572662353516\n",
      "training on iteration:931    \t/3400, val loss:42.93210220336914\n",
      "training on iteration:932    \t/3400, val loss:42.99820327758789\n",
      "training on iteration:933    \t/3400, val loss:42.79324722290039\n",
      "training on iteration:934    \t/3400, val loss:42.44902420043945\n",
      "training on iteration:935    \t/3400, val loss:42.31085968017578\n",
      "training on iteration:936    \t/3400, val loss:42.22461700439453\n",
      "training on iteration:937    \t/3400, val loss:42.30750274658203\n",
      "training on iteration:938    \t/3400, val loss:42.59147262573242\n",
      "training on iteration:939    \t/3400, val loss:42.96842575073242\n",
      "training on iteration:940    \t/3400, val loss:43.02977752685547\n",
      "training on iteration:941    \t/3400, val loss:42.942073822021484\n",
      "training on iteration:942    \t/3400, val loss:42.661468505859375\n",
      "training on iteration:943    \t/3400, val loss:42.49578857421875\n",
      "training on iteration:944    \t/3400, val loss:42.52166748046875\n",
      "training on iteration:945    \t/3400, val loss:42.42052459716797\n",
      "training on iteration:946    \t/3400, val loss:42.43613815307617\n",
      "training on iteration:947    \t/3400, val loss:42.65496063232422\n",
      "training on iteration:948    \t/3400, val loss:42.91587448120117\n",
      "training on iteration:949    \t/3400, val loss:42.91425323486328\n",
      "training on iteration:950    \t/3400, val loss:42.677555084228516\n",
      "training on iteration:951    \t/3400, val loss:42.366302490234375\n",
      "training on iteration:952    \t/3400, val loss:42.26755905151367\n",
      "training on iteration:953    \t/3400, val loss:42.201358795166016\n",
      "training on iteration:954    \t/3400, val loss:42.29317855834961\n",
      "training on iteration:955    \t/3400, val loss:42.58978271484375\n",
      "training on iteration:956    \t/3400, val loss:42.92312240600586\n",
      "training on iteration:957    \t/3400, val loss:42.94922637939453\n",
      "training on iteration:958    \t/3400, val loss:42.90279006958008\n",
      "training on iteration:959    \t/3400, val loss:42.62465286254883\n",
      "training on iteration:960    \t/3400, val loss:42.464412689208984\n",
      "training on iteration:961    \t/3400, val loss:42.52901840209961\n",
      "training on iteration:962    \t/3400, val loss:42.41596603393555\n",
      "training on iteration:963    \t/3400, val loss:42.38678741455078\n",
      "training on iteration:964    \t/3400, val loss:42.54941177368164\n",
      "training on iteration:965    \t/3400, val loss:42.82321548461914\n",
      "training on iteration:966    \t/3400, val loss:42.8880729675293\n",
      "training on iteration:967    \t/3400, val loss:42.66204071044922\n",
      "training on iteration:968    \t/3400, val loss:42.34878921508789\n",
      "training on iteration:969    \t/3400, val loss:42.25058364868164\n",
      "training on iteration:970    \t/3400, val loss:42.169822692871094\n",
      "training on iteration:971    \t/3400, val loss:42.248897552490234\n",
      "training on iteration:972    \t/3400, val loss:42.520694732666016\n",
      "training on iteration:973    \t/3400, val loss:42.89825439453125\n",
      "training on iteration:974    \t/3400, val loss:42.95451736450195\n",
      "training on iteration:975    \t/3400, val loss:42.888427734375\n",
      "training on iteration:976    \t/3400, val loss:42.56881332397461\n",
      "training on iteration:977    \t/3400, val loss:42.40114974975586\n",
      "training on iteration:978    \t/3400, val loss:42.46466827392578\n",
      "training on iteration:979    \t/3400, val loss:42.36495590209961\n",
      "training on iteration:980    \t/3400, val loss:42.367557525634766\n",
      "training on iteration:981    \t/3400, val loss:42.58932876586914\n",
      "training on iteration:982    \t/3400, val loss:42.88347244262695\n",
      "training on iteration:983    \t/3400, val loss:42.91524124145508\n",
      "training on iteration:984    \t/3400, val loss:42.61813735961914\n",
      "training on iteration:985    \t/3400, val loss:42.260986328125\n",
      "training on iteration:986    \t/3400, val loss:42.15350341796875\n",
      "training on iteration:987    \t/3400, val loss:42.08871841430664\n",
      "training on iteration:988    \t/3400, val loss:42.18766403198242\n",
      "training on iteration:989    \t/3400, val loss:42.51409149169922\n",
      "training on iteration:990    \t/3400, val loss:42.94305419921875\n",
      "training on iteration:991    \t/3400, val loss:42.99055862426758\n",
      "training on iteration:992    \t/3400, val loss:42.84294509887695\n",
      "training on iteration:993    \t/3400, val loss:42.47998809814453\n",
      "training on iteration:994    \t/3400, val loss:42.30971145629883\n",
      "training on iteration:995    \t/3400, val loss:42.380821228027344\n",
      "training on iteration:996    \t/3400, val loss:42.29535675048828\n",
      "training on iteration:997    \t/3400, val loss:42.30550003051758\n",
      "training on iteration:998    \t/3400, val loss:42.55682373046875\n",
      "training on iteration:999    \t/3400, val loss:42.86594009399414\n",
      "training on iteration:1000    \t/3400, val loss:42.815528869628906\n",
      "training on iteration:1001    \t/3400, val loss:42.48585891723633\n",
      "training on iteration:1002    \t/3400, val loss:42.188297271728516\n",
      "training on iteration:1003    \t/3400, val loss:42.11179733276367\n",
      "training on iteration:1004    \t/3400, val loss:42.063751220703125\n",
      "training on iteration:1005    \t/3400, val loss:42.188446044921875\n",
      "training on iteration:1006    \t/3400, val loss:42.534061431884766\n",
      "training on iteration:1007    \t/3400, val loss:42.87846755981445\n",
      "training on iteration:1008    \t/3400, val loss:42.84606170654297\n",
      "training on iteration:1009    \t/3400, val loss:42.77581024169922\n",
      "training on iteration:1010    \t/3400, val loss:42.48195266723633\n",
      "training on iteration:1011    \t/3400, val loss:42.34363555908203\n",
      "training on iteration:1012    \t/3400, val loss:42.41597366333008\n",
      "training on iteration:1013    \t/3400, val loss:42.296791076660156\n",
      "training on iteration:1014    \t/3400, val loss:42.27610397338867\n",
      "training on iteration:1015    \t/3400, val loss:42.4731330871582\n",
      "training on iteration:1016    \t/3400, val loss:42.75305938720703\n",
      "training on iteration:1017    \t/3400, val loss:42.803192138671875\n",
      "training on iteration:1018    \t/3400, val loss:42.534053802490234\n",
      "training on iteration:1019    \t/3400, val loss:42.20106887817383\n",
      "training on iteration:1020    \t/3400, val loss:42.104148864746094\n",
      "training on iteration:1021    \t/3400, val loss:42.03154754638672\n",
      "training on iteration:1022    \t/3400, val loss:42.116859436035156\n",
      "training on iteration:1023    \t/3400, val loss:42.40243148803711\n",
      "training on iteration:1024    \t/3400, val loss:42.7734260559082\n",
      "training on iteration:1025    \t/3400, val loss:42.82130813598633\n",
      "training on iteration:1026    \t/3400, val loss:42.785179138183594\n",
      "training on iteration:1027    \t/3400, val loss:42.430538177490234\n",
      "training on iteration:1028    \t/3400, val loss:42.26796340942383\n",
      "training on iteration:1029    \t/3400, val loss:42.32647705078125\n",
      "training on iteration:1030    \t/3400, val loss:42.22006607055664\n",
      "training on iteration:1031    \t/3400, val loss:42.27439498901367\n",
      "training on iteration:1032    \t/3400, val loss:42.582401275634766\n",
      "training on iteration:1033    \t/3400, val loss:42.813316345214844\n",
      "training on iteration:1034    \t/3400, val loss:42.66706085205078\n",
      "training on iteration:1035    \t/3400, val loss:42.32390213012695\n",
      "training on iteration:1036    \t/3400, val loss:42.07522964477539\n",
      "training on iteration:1037    \t/3400, val loss:42.04026412963867\n",
      "training on iteration:1038    \t/3400, val loss:42.02486801147461\n",
      "training on iteration:1039    \t/3400, val loss:42.208961486816406\n",
      "training on iteration:1040    \t/3400, val loss:42.576637268066406\n",
      "training on iteration:1041    \t/3400, val loss:42.76858139038086\n",
      "training on iteration:1042    \t/3400, val loss:42.60996627807617\n",
      "training on iteration:1043    \t/3400, val loss:42.50390625\n",
      "training on iteration:1044    \t/3400, val loss:42.288639068603516\n",
      "training on iteration:1045    \t/3400, val loss:42.21120834350586\n",
      "training on iteration:1046    \t/3400, val loss:42.35007858276367\n",
      "training on iteration:1047    \t/3400, val loss:42.25778579711914\n",
      "training on iteration:1048    \t/3400, val loss:42.24019241333008\n",
      "training on iteration:1049    \t/3400, val loss:42.408203125\n",
      "training on iteration:1050    \t/3400, val loss:42.60959243774414\n",
      "training on iteration:1051    \t/3400, val loss:42.61966323852539\n",
      "training on iteration:1052    \t/3400, val loss:42.393165588378906\n",
      "training on iteration:1053    \t/3400, val loss:42.11932373046875\n",
      "training on iteration:1054    \t/3400, val loss:42.047855377197266\n",
      "training on iteration:1055    \t/3400, val loss:41.978511810302734\n",
      "training on iteration:1056    \t/3400, val loss:42.08816146850586\n",
      "training on iteration:1057    \t/3400, val loss:42.39310836791992\n",
      "training on iteration:1058    \t/3400, val loss:42.70043182373047\n",
      "training on iteration:1059    \t/3400, val loss:42.65267562866211\n",
      "training on iteration:1060    \t/3400, val loss:42.59674072265625\n",
      "training on iteration:1061    \t/3400, val loss:42.31074905395508\n",
      "training on iteration:1062    \t/3400, val loss:42.21349334716797\n",
      "training on iteration:1063    \t/3400, val loss:42.29271697998047\n",
      "training on iteration:1064    \t/3400, val loss:42.16646194458008\n",
      "training on iteration:1065    \t/3400, val loss:42.201419830322266\n",
      "training on iteration:1066    \t/3400, val loss:42.47191619873047\n",
      "training on iteration:1067    \t/3400, val loss:42.68248748779297\n",
      "training on iteration:1068    \t/3400, val loss:42.60942840576172\n",
      "training on iteration:1069    \t/3400, val loss:42.30096435546875\n",
      "training on iteration:1070    \t/3400, val loss:42.02540969848633\n",
      "training on iteration:1071    \t/3400, val loss:41.975677490234375\n",
      "training on iteration:1072    \t/3400, val loss:41.9534797668457\n",
      "training on iteration:1073    \t/3400, val loss:42.14810562133789\n",
      "training on iteration:1074    \t/3400, val loss:42.50644302368164\n",
      "training on iteration:1075    \t/3400, val loss:42.660037994384766\n",
      "training on iteration:1076    \t/3400, val loss:42.46619415283203\n",
      "training on iteration:1077    \t/3400, val loss:42.366058349609375\n",
      "training on iteration:1078    \t/3400, val loss:42.16250228881836\n",
      "training on iteration:1079    \t/3400, val loss:42.15334701538086\n",
      "training on iteration:1080    \t/3400, val loss:42.339481353759766\n",
      "training on iteration:1081    \t/3400, val loss:42.230201721191406\n",
      "training on iteration:1082    \t/3400, val loss:42.168243408203125\n",
      "training on iteration:1083    \t/3400, val loss:42.285030364990234\n",
      "training on iteration:1084    \t/3400, val loss:42.4901008605957\n",
      "training on iteration:1085    \t/3400, val loss:42.51150131225586\n",
      "training on iteration:1086    \t/3400, val loss:42.33241653442383\n",
      "training on iteration:1087    \t/3400, val loss:42.04605484008789\n",
      "training on iteration:1088    \t/3400, val loss:41.97846221923828\n",
      "training on iteration:1089    \t/3400, val loss:41.927574157714844\n",
      "training on iteration:1090    \t/3400, val loss:42.067012786865234\n",
      "training on iteration:1091    \t/3400, val loss:42.41265869140625\n",
      "training on iteration:1092    \t/3400, val loss:42.61803436279297\n",
      "training on iteration:1093    \t/3400, val loss:42.47047805786133\n",
      "training on iteration:1094    \t/3400, val loss:42.39265823364258\n",
      "training on iteration:1095    \t/3400, val loss:42.167877197265625\n",
      "training on iteration:1096    \t/3400, val loss:42.111019134521484\n",
      "training on iteration:1097    \t/3400, val loss:42.24777603149414\n",
      "training on iteration:1098    \t/3400, val loss:42.14163589477539\n",
      "training on iteration:1099    \t/3400, val loss:42.10329055786133\n",
      "training on iteration:1100    \t/3400, val loss:42.25999069213867\n",
      "training on iteration:1101    \t/3400, val loss:42.478878021240234\n",
      "training on iteration:1102    \t/3400, val loss:42.48415756225586\n",
      "training on iteration:1103    \t/3400, val loss:42.29758071899414\n",
      "training on iteration:1104    \t/3400, val loss:42.014827728271484\n",
      "training on iteration:1105    \t/3400, val loss:41.97077560424805\n",
      "training on iteration:1106    \t/3400, val loss:41.91290283203125\n",
      "training on iteration:1107    \t/3400, val loss:42.04056167602539\n",
      "training on iteration:1108    \t/3400, val loss:42.38883590698242\n",
      "training on iteration:1109    \t/3400, val loss:42.62900924682617\n",
      "training on iteration:1110    \t/3400, val loss:42.46406173706055\n",
      "training on iteration:1111    \t/3400, val loss:42.3656005859375\n",
      "training on iteration:1112    \t/3400, val loss:42.134788513183594\n",
      "training on iteration:1113    \t/3400, val loss:42.06001663208008\n",
      "training on iteration:1114    \t/3400, val loss:42.185054779052734\n",
      "training on iteration:1115    \t/3400, val loss:42.07967758178711\n",
      "training on iteration:1116    \t/3400, val loss:42.068111419677734\n",
      "training on iteration:1117    \t/3400, val loss:42.27035903930664\n",
      "training on iteration:1118    \t/3400, val loss:42.51680374145508\n",
      "training on iteration:1119    \t/3400, val loss:42.490177154541016\n",
      "training on iteration:1120    \t/3400, val loss:42.24457931518555\n",
      "training on iteration:1121    \t/3400, val loss:41.95743179321289\n",
      "training on iteration:1122    \t/3400, val loss:41.91971969604492\n",
      "training on iteration:1123    \t/3400, val loss:41.86895751953125\n",
      "training on iteration:1124    \t/3400, val loss:42.00360107421875\n",
      "training on iteration:1125    \t/3400, val loss:42.344459533691406\n",
      "training on iteration:1126    \t/3400, val loss:42.5768928527832\n",
      "training on iteration:1127    \t/3400, val loss:42.4338493347168\n",
      "training on iteration:1128    \t/3400, val loss:42.324161529541016\n",
      "training on iteration:1129    \t/3400, val loss:42.07634735107422\n",
      "training on iteration:1130    \t/3400, val loss:42.04140853881836\n",
      "training on iteration:1131    \t/3400, val loss:42.23729705810547\n",
      "training on iteration:1132    \t/3400, val loss:42.13071060180664\n",
      "training on iteration:1133    \t/3400, val loss:42.06740951538086\n",
      "training on iteration:1134    \t/3400, val loss:42.17842483520508\n",
      "training on iteration:1135    \t/3400, val loss:42.3546028137207\n",
      "training on iteration:1136    \t/3400, val loss:42.35128402709961\n",
      "training on iteration:1137    \t/3400, val loss:42.22279739379883\n",
      "training on iteration:1138    \t/3400, val loss:41.97738265991211\n",
      "training on iteration:1139    \t/3400, val loss:41.94475173950195\n",
      "training on iteration:1140    \t/3400, val loss:41.86696243286133\n",
      "training on iteration:1141    \t/3400, val loss:41.96331787109375\n",
      "training on iteration:1142    \t/3400, val loss:42.26011276245117\n",
      "training on iteration:1143    \t/3400, val loss:42.48805618286133\n",
      "training on iteration:1144    \t/3400, val loss:42.38372802734375\n",
      "training on iteration:1145    \t/3400, val loss:42.32711410522461\n",
      "training on iteration:1146    \t/3400, val loss:42.086997985839844\n",
      "training on iteration:1147    \t/3400, val loss:42.032833099365234\n",
      "training on iteration:1148    \t/3400, val loss:42.16259765625\n",
      "training on iteration:1149    \t/3400, val loss:42.00620651245117\n",
      "training on iteration:1150    \t/3400, val loss:41.96587371826172\n",
      "training on iteration:1151    \t/3400, val loss:42.161720275878906\n",
      "training on iteration:1152    \t/3400, val loss:42.4356689453125\n",
      "training on iteration:1153    \t/3400, val loss:42.43313217163086\n",
      "training on iteration:1154    \t/3400, val loss:42.20669937133789\n",
      "training on iteration:1155    \t/3400, val loss:41.89806365966797\n",
      "training on iteration:1156    \t/3400, val loss:41.844993591308594\n",
      "training on iteration:1157    \t/3400, val loss:41.7932243347168\n",
      "training on iteration:1158    \t/3400, val loss:41.93832015991211\n",
      "training on iteration:1159    \t/3400, val loss:42.31571578979492\n",
      "training on iteration:1160    \t/3400, val loss:42.52692413330078\n",
      "training on iteration:1161    \t/3400, val loss:42.33281707763672\n",
      "training on iteration:1162    \t/3400, val loss:42.20787811279297\n",
      "training on iteration:1163    \t/3400, val loss:41.98401641845703\n",
      "training on iteration:1164    \t/3400, val loss:41.96791458129883\n",
      "training on iteration:1165    \t/3400, val loss:42.16513442993164\n",
      "training on iteration:1166    \t/3400, val loss:42.071353912353516\n",
      "training on iteration:1167    \t/3400, val loss:41.99655532836914\n",
      "training on iteration:1168    \t/3400, val loss:42.08989334106445\n",
      "training on iteration:1169    \t/3400, val loss:42.25088882446289\n",
      "training on iteration:1170    \t/3400, val loss:42.27040481567383\n",
      "training on iteration:1171    \t/3400, val loss:42.16985321044922\n",
      "training on iteration:1172    \t/3400, val loss:41.89838409423828\n",
      "training on iteration:1173    \t/3400, val loss:41.85593032836914\n",
      "training on iteration:1174    \t/3400, val loss:41.78596878051758\n",
      "training on iteration:1175    \t/3400, val loss:41.91012191772461\n",
      "training on iteration:1176    \t/3400, val loss:42.26375961303711\n",
      "training on iteration:1177    \t/3400, val loss:42.504581451416016\n",
      "training on iteration:1178    \t/3400, val loss:42.3491096496582\n",
      "training on iteration:1179    \t/3400, val loss:42.24661636352539\n",
      "training on iteration:1180    \t/3400, val loss:41.97983932495117\n",
      "training on iteration:1181    \t/3400, val loss:41.93637466430664\n",
      "training on iteration:1182    \t/3400, val loss:42.101078033447266\n",
      "training on iteration:1183    \t/3400, val loss:41.98470687866211\n",
      "training on iteration:1184    \t/3400, val loss:41.95857620239258\n",
      "training on iteration:1185    \t/3400, val loss:42.11808395385742\n",
      "training on iteration:1186    \t/3400, val loss:42.301239013671875\n",
      "training on iteration:1187    \t/3400, val loss:42.21989059448242\n",
      "training on iteration:1188    \t/3400, val loss:42.09005355834961\n",
      "training on iteration:1189    \t/3400, val loss:41.82376480102539\n",
      "training on iteration:1190    \t/3400, val loss:41.78962326049805\n",
      "training on iteration:1191    \t/3400, val loss:41.74981689453125\n",
      "training on iteration:1192    \t/3400, val loss:41.92728805541992\n",
      "training on iteration:1193    \t/3400, val loss:42.2943115234375\n",
      "training on iteration:1194    \t/3400, val loss:42.4720458984375\n",
      "training on iteration:1195    \t/3400, val loss:42.24066925048828\n",
      "training on iteration:1196    \t/3400, val loss:42.10651397705078\n",
      "training on iteration:1197    \t/3400, val loss:41.90543746948242\n",
      "training on iteration:1198    \t/3400, val loss:41.92782211303711\n",
      "training on iteration:1199    \t/3400, val loss:42.142303466796875\n",
      "training on iteration:1200    \t/3400, val loss:42.02283477783203\n",
      "training on iteration:1201    \t/3400, val loss:41.94815444946289\n",
      "training on iteration:1202    \t/3400, val loss:42.05709457397461\n",
      "training on iteration:1203    \t/3400, val loss:42.23084259033203\n",
      "training on iteration:1204    \t/3400, val loss:42.19560623168945\n",
      "training on iteration:1205    \t/3400, val loss:42.07987594604492\n",
      "training on iteration:1206    \t/3400, val loss:41.83649444580078\n",
      "training on iteration:1207    \t/3400, val loss:41.81631088256836\n",
      "training on iteration:1208    \t/3400, val loss:41.76231002807617\n",
      "training on iteration:1209    \t/3400, val loss:41.90009307861328\n",
      "training on iteration:1210    \t/3400, val loss:42.22239303588867\n",
      "training on iteration:1211    \t/3400, val loss:42.401527404785156\n",
      "training on iteration:1212    \t/3400, val loss:42.21807098388672\n",
      "training on iteration:1213    \t/3400, val loss:42.10453414916992\n",
      "training on iteration:1214    \t/3400, val loss:41.87725830078125\n",
      "training on iteration:1215    \t/3400, val loss:41.87943649291992\n",
      "training on iteration:1216    \t/3400, val loss:42.06680679321289\n",
      "training on iteration:1217    \t/3400, val loss:41.93044662475586\n",
      "training on iteration:1218    \t/3400, val loss:41.88582229614258\n",
      "training on iteration:1219    \t/3400, val loss:42.03591537475586\n",
      "training on iteration:1220    \t/3400, val loss:42.2340087890625\n",
      "training on iteration:1221    \t/3400, val loss:42.19428253173828\n",
      "training on iteration:1222    \t/3400, val loss:42.064247131347656\n",
      "training on iteration:1223    \t/3400, val loss:41.76178741455078\n",
      "training on iteration:1224    \t/3400, val loss:41.71411895751953\n",
      "training on iteration:1225    \t/3400, val loss:41.67827606201172\n",
      "training on iteration:1226    \t/3400, val loss:41.88694381713867\n",
      "training on iteration:1227    \t/3400, val loss:42.26395797729492\n",
      "training on iteration:1228    \t/3400, val loss:42.380672454833984\n",
      "training on iteration:1229    \t/3400, val loss:42.09812545776367\n",
      "training on iteration:1230    \t/3400, val loss:41.97676467895508\n",
      "training on iteration:1231    \t/3400, val loss:41.80089569091797\n",
      "training on iteration:1232    \t/3400, val loss:41.85508346557617\n",
      "training on iteration:1233    \t/3400, val loss:42.159183502197266\n",
      "training on iteration:1234    \t/3400, val loss:42.04991912841797\n",
      "training on iteration:1235    \t/3400, val loss:41.95566177368164\n",
      "training on iteration:1236    \t/3400, val loss:42.009246826171875\n",
      "training on iteration:1237    \t/3400, val loss:42.099205017089844\n",
      "training on iteration:1238    \t/3400, val loss:42.003360748291016\n",
      "training on iteration:1239    \t/3400, val loss:41.91448211669922\n",
      "training on iteration:1240    \t/3400, val loss:41.73784637451172\n",
      "training on iteration:1241    \t/3400, val loss:41.75418472290039\n",
      "training on iteration:1242    \t/3400, val loss:41.74022674560547\n",
      "training on iteration:1243    \t/3400, val loss:41.92578125\n",
      "training on iteration:1244    \t/3400, val loss:42.183292388916016\n",
      "training on iteration:1245    \t/3400, val loss:42.278160095214844\n",
      "training on iteration:1246    \t/3400, val loss:42.047210693359375\n",
      "training on iteration:1247    \t/3400, val loss:41.95120620727539\n",
      "training on iteration:1248    \t/3400, val loss:41.79383087158203\n",
      "training on iteration:1249    \t/3400, val loss:41.86056900024414\n",
      "training on iteration:1250    \t/3400, val loss:42.14095687866211\n",
      "training on iteration:1251    \t/3400, val loss:41.96464157104492\n",
      "training on iteration:1252    \t/3400, val loss:41.85294723510742\n",
      "training on iteration:1253    \t/3400, val loss:41.901634216308594\n",
      "training on iteration:1254    \t/3400, val loss:42.06293869018555\n",
      "training on iteration:1255    \t/3400, val loss:42.038143157958984\n",
      "training on iteration:1256    \t/3400, val loss:41.98263931274414\n",
      "training on iteration:1257    \t/3400, val loss:41.71440505981445\n",
      "training on iteration:1258    \t/3400, val loss:41.67873001098633\n",
      "training on iteration:1259    \t/3400, val loss:41.64153289794922\n",
      "training on iteration:1260    \t/3400, val loss:41.85517120361328\n",
      "training on iteration:1261    \t/3400, val loss:42.187461853027344\n",
      "training on iteration:1262    \t/3400, val loss:42.31073760986328\n",
      "training on iteration:1263    \t/3400, val loss:42.03654861450195\n",
      "training on iteration:1264    \t/3400, val loss:41.88274383544922\n",
      "training on iteration:1265    \t/3400, val loss:41.719058990478516\n",
      "training on iteration:1266    \t/3400, val loss:41.8099479675293\n",
      "training on iteration:1267    \t/3400, val loss:42.15261459350586\n",
      "training on iteration:1268    \t/3400, val loss:42.04060745239258\n",
      "training on iteration:1269    \t/3400, val loss:41.89558029174805\n",
      "training on iteration:1270    \t/3400, val loss:41.91190719604492\n",
      "training on iteration:1271    \t/3400, val loss:41.99364471435547\n",
      "training on iteration:1272    \t/3400, val loss:41.95112991333008\n",
      "training on iteration:1273    \t/3400, val loss:41.9736213684082\n",
      "training on iteration:1274    \t/3400, val loss:41.740604400634766\n",
      "training on iteration:1275    \t/3400, val loss:41.684303283691406\n",
      "training on iteration:1276    \t/3400, val loss:41.628662109375\n",
      "training on iteration:1277    \t/3400, val loss:41.803985595703125\n",
      "training on iteration:1278    \t/3400, val loss:42.12606430053711\n",
      "training on iteration:1279    \t/3400, val loss:42.24712371826172\n",
      "training on iteration:1280    \t/3400, val loss:41.989295959472656\n",
      "training on iteration:1281    \t/3400, val loss:41.84669876098633\n",
      "training on iteration:1282    \t/3400, val loss:41.69326400756836\n",
      "training on iteration:1283    \t/3400, val loss:41.765750885009766\n",
      "training on iteration:1284    \t/3400, val loss:42.068607330322266\n",
      "training on iteration:1285    \t/3400, val loss:41.92905044555664\n",
      "training on iteration:1286    \t/3400, val loss:41.79988479614258\n",
      "training on iteration:1287    \t/3400, val loss:41.832115173339844\n",
      "training on iteration:1288    \t/3400, val loss:42.00676727294922\n",
      "training on iteration:1289    \t/3400, val loss:41.9979362487793\n",
      "training on iteration:1290    \t/3400, val loss:41.96086883544922\n",
      "training on iteration:1291    \t/3400, val loss:41.661197662353516\n",
      "training on iteration:1292    \t/3400, val loss:41.584800720214844\n",
      "training on iteration:1293    \t/3400, val loss:41.53021240234375\n",
      "training on iteration:1294    \t/3400, val loss:41.75338363647461\n",
      "training on iteration:1295    \t/3400, val loss:42.17816162109375\n",
      "training on iteration:1296    \t/3400, val loss:42.330406188964844\n",
      "training on iteration:1297    \t/3400, val loss:41.99289321899414\n",
      "training on iteration:1298    \t/3400, val loss:41.77016830444336\n",
      "training on iteration:1299    \t/3400, val loss:41.60486602783203\n",
      "training on iteration:1300    \t/3400, val loss:41.71361541748047\n",
      "training on iteration:1301    \t/3400, val loss:42.13922882080078\n",
      "training on iteration:1302    \t/3400, val loss:42.0200309753418\n",
      "training on iteration:1303    \t/3400, val loss:41.82590103149414\n",
      "training on iteration:1304    \t/3400, val loss:41.75644302368164\n",
      "training on iteration:1305    \t/3400, val loss:41.8580207824707\n",
      "training on iteration:1306    \t/3400, val loss:41.903377532958984\n",
      "training on iteration:1307    \t/3400, val loss:41.98238754272461\n",
      "training on iteration:1308    \t/3400, val loss:41.719417572021484\n",
      "training on iteration:1309    \t/3400, val loss:41.619388580322266\n",
      "training on iteration:1310    \t/3400, val loss:41.53899383544922\n",
      "training on iteration:1311    \t/3400, val loss:41.702056884765625\n",
      "training on iteration:1312    \t/3400, val loss:42.016082763671875\n",
      "training on iteration:1313    \t/3400, val loss:42.20553970336914\n",
      "training on iteration:1314    \t/3400, val loss:41.96942138671875\n",
      "training on iteration:1315    \t/3400, val loss:41.80189895629883\n",
      "training on iteration:1316    \t/3400, val loss:41.62155532836914\n",
      "training on iteration:1317    \t/3400, val loss:41.69485092163086\n",
      "training on iteration:1318    \t/3400, val loss:42.01620101928711\n",
      "training on iteration:1319    \t/3400, val loss:41.94344711303711\n",
      "training on iteration:1320    \t/3400, val loss:41.828067779541016\n",
      "training on iteration:1321    \t/3400, val loss:41.82292938232422\n",
      "training on iteration:1322    \t/3400, val loss:41.92382049560547\n",
      "training on iteration:1323    \t/3400, val loss:41.84980010986328\n",
      "training on iteration:1324    \t/3400, val loss:41.82942581176758\n",
      "training on iteration:1325    \t/3400, val loss:41.61702346801758\n",
      "training on iteration:1326    \t/3400, val loss:41.58063888549805\n",
      "training on iteration:1327    \t/3400, val loss:41.55474853515625\n",
      "training on iteration:1328    \t/3400, val loss:41.719749450683594\n",
      "training on iteration:1329    \t/3400, val loss:42.00395584106445\n",
      "training on iteration:1330    \t/3400, val loss:42.13208770751953\n",
      "training on iteration:1331    \t/3400, val loss:41.8895263671875\n",
      "training on iteration:1332    \t/3400, val loss:41.7548942565918\n",
      "training on iteration:1333    \t/3400, val loss:41.59564971923828\n",
      "training on iteration:1334    \t/3400, val loss:41.70443344116211\n",
      "training on iteration:1335    \t/3400, val loss:42.013492584228516\n",
      "training on iteration:1336    \t/3400, val loss:41.86328887939453\n",
      "training on iteration:1337    \t/3400, val loss:41.71380615234375\n",
      "training on iteration:1338    \t/3400, val loss:41.70690155029297\n",
      "training on iteration:1339    \t/3400, val loss:41.85059356689453\n",
      "training on iteration:1340    \t/3400, val loss:41.895450592041016\n",
      "training on iteration:1341    \t/3400, val loss:41.93252944946289\n",
      "training on iteration:1342    \t/3400, val loss:41.64647674560547\n",
      "training on iteration:1343    \t/3400, val loss:41.55645751953125\n",
      "training on iteration:1344    \t/3400, val loss:41.49615478515625\n",
      "training on iteration:1345    \t/3400, val loss:41.698272705078125\n",
      "training on iteration:1346    \t/3400, val loss:42.0679931640625\n",
      "training on iteration:1347    \t/3400, val loss:42.18133544921875\n",
      "training on iteration:1348    \t/3400, val loss:41.84391403198242\n",
      "training on iteration:1349    \t/3400, val loss:41.68800735473633\n",
      "training on iteration:1350    \t/3400, val loss:41.543670654296875\n",
      "training on iteration:1351    \t/3400, val loss:41.7213134765625\n",
      "training on iteration:1352    \t/3400, val loss:42.13358688354492\n",
      "training on iteration:1353    \t/3400, val loss:41.89371109008789\n",
      "training on iteration:1354    \t/3400, val loss:41.649085998535156\n",
      "training on iteration:1355    \t/3400, val loss:41.610347747802734\n",
      "training on iteration:1356    \t/3400, val loss:41.7745361328125\n",
      "training on iteration:1357    \t/3400, val loss:41.917545318603516\n",
      "training on iteration:1358    \t/3400, val loss:42.021484375\n",
      "training on iteration:1359    \t/3400, val loss:41.62194061279297\n",
      "training on iteration:1360    \t/3400, val loss:41.468570709228516\n",
      "training on iteration:1361    \t/3400, val loss:41.38808822631836\n",
      "training on iteration:1362    \t/3400, val loss:41.59651565551758\n",
      "training on iteration:1363    \t/3400, val loss:42.10038375854492\n",
      "training on iteration:1364    \t/3400, val loss:42.31855392456055\n",
      "training on iteration:1365    \t/3400, val loss:41.925926208496094\n",
      "training on iteration:1366    \t/3400, val loss:41.63996124267578\n",
      "training on iteration:1367    \t/3400, val loss:41.44542694091797\n",
      "training on iteration:1368    \t/3400, val loss:41.55047607421875\n",
      "training on iteration:1369    \t/3400, val loss:42.02976608276367\n",
      "training on iteration:1370    \t/3400, val loss:41.987281799316406\n",
      "training on iteration:1371    \t/3400, val loss:41.766658782958984\n",
      "training on iteration:1372    \t/3400, val loss:41.64169692993164\n",
      "training on iteration:1373    \t/3400, val loss:41.68903350830078\n",
      "training on iteration:1374    \t/3400, val loss:41.80535125732422\n",
      "training on iteration:1375    \t/3400, val loss:41.92593002319336\n",
      "training on iteration:1376    \t/3400, val loss:41.61419677734375\n",
      "training on iteration:1377    \t/3400, val loss:41.48411178588867\n",
      "training on iteration:1378    \t/3400, val loss:41.41460418701172\n",
      "training on iteration:1379    \t/3400, val loss:41.60070037841797\n",
      "training on iteration:1380    \t/3400, val loss:41.97966384887695\n",
      "training on iteration:1381    \t/3400, val loss:42.156219482421875\n",
      "training on iteration:1382    \t/3400, val loss:41.82308578491211\n",
      "training on iteration:1383    \t/3400, val loss:41.62494659423828\n",
      "training on iteration:1384    \t/3400, val loss:41.475425720214844\n",
      "training on iteration:1385    \t/3400, val loss:41.635746002197266\n",
      "training on iteration:1386    \t/3400, val loss:42.07646942138672\n",
      "training on iteration:1387    \t/3400, val loss:41.892696380615234\n",
      "training on iteration:1388    \t/3400, val loss:41.611724853515625\n",
      "training on iteration:1389    \t/3400, val loss:41.53760528564453\n",
      "training on iteration:1390    \t/3400, val loss:41.70690155029297\n",
      "training on iteration:1391    \t/3400, val loss:41.93948745727539\n",
      "training on iteration:1392    \t/3400, val loss:42.065032958984375\n",
      "training on iteration:1393    \t/3400, val loss:41.610984802246094\n",
      "training on iteration:1394    \t/3400, val loss:41.440879821777344\n",
      "training on iteration:1395    \t/3400, val loss:41.357147216796875\n",
      "training on iteration:1396    \t/3400, val loss:41.5701904296875\n",
      "training on iteration:1397    \t/3400, val loss:42.05060577392578\n",
      "training on iteration:1398    \t/3400, val loss:42.24186325073242\n",
      "training on iteration:1399    \t/3400, val loss:41.81769561767578\n",
      "training on iteration:1400    \t/3400, val loss:41.54303741455078\n",
      "training on iteration:1401    \t/3400, val loss:41.39978790283203\n",
      "training on iteration:1402    \t/3400, val loss:41.529815673828125\n",
      "training on iteration:1403    \t/3400, val loss:42.01114273071289\n",
      "training on iteration:1404    \t/3400, val loss:41.94646072387695\n",
      "training on iteration:1405    \t/3400, val loss:41.6832275390625\n",
      "training on iteration:1406    \t/3400, val loss:41.529762268066406\n",
      "training on iteration:1407    \t/3400, val loss:41.574195861816406\n",
      "training on iteration:1408    \t/3400, val loss:41.76348876953125\n",
      "training on iteration:1409    \t/3400, val loss:41.99456787109375\n",
      "training on iteration:1410    \t/3400, val loss:41.6468620300293\n",
      "training on iteration:1411    \t/3400, val loss:41.464298248291016\n",
      "training on iteration:1412    \t/3400, val loss:41.3651123046875\n",
      "training on iteration:1413    \t/3400, val loss:41.5510139465332\n",
      "training on iteration:1414    \t/3400, val loss:41.93312454223633\n",
      "training on iteration:1415    \t/3400, val loss:42.12223434448242\n",
      "training on iteration:1416    \t/3400, val loss:41.75826644897461\n",
      "training on iteration:1417    \t/3400, val loss:41.54481887817383\n",
      "training on iteration:1418    \t/3400, val loss:41.40549850463867\n",
      "training on iteration:1419    \t/3400, val loss:41.570701599121094\n",
      "training on iteration:1420    \t/3400, val loss:42.01743698120117\n",
      "training on iteration:1421    \t/3400, val loss:41.82334518432617\n",
      "training on iteration:1422    \t/3400, val loss:41.52772903442383\n",
      "training on iteration:1423    \t/3400, val loss:41.45526885986328\n",
      "training on iteration:1424    \t/3400, val loss:41.63286590576172\n",
      "training on iteration:1425    \t/3400, val loss:41.9150390625\n",
      "training on iteration:1426    \t/3400, val loss:42.03776168823242\n",
      "training on iteration:1427    \t/3400, val loss:41.53468704223633\n",
      "training on iteration:1428    \t/3400, val loss:41.33761978149414\n",
      "training on iteration:1429    \t/3400, val loss:41.276275634765625\n",
      "training on iteration:1430    \t/3400, val loss:41.535606384277344\n",
      "training on iteration:1431    \t/3400, val loss:42.092674255371094\n",
      "training on iteration:1432    \t/3400, val loss:42.25492477416992\n",
      "training on iteration:1433    \t/3400, val loss:41.80512237548828\n",
      "training on iteration:1434    \t/3400, val loss:41.50200653076172\n",
      "training on iteration:1435    \t/3400, val loss:41.36516571044922\n",
      "training on iteration:1436    \t/3400, val loss:41.50341033935547\n",
      "training on iteration:1437    \t/3400, val loss:42.0086669921875\n",
      "training on iteration:1438    \t/3400, val loss:41.93704605102539\n",
      "training on iteration:1439    \t/3400, val loss:41.58185958862305\n",
      "training on iteration:1440    \t/3400, val loss:41.41991424560547\n",
      "training on iteration:1441    \t/3400, val loss:41.504676818847656\n",
      "training on iteration:1442    \t/3400, val loss:41.73006820678711\n",
      "training on iteration:1443    \t/3400, val loss:41.984004974365234\n",
      "training on iteration:1444    \t/3400, val loss:41.59265899658203\n",
      "training on iteration:1445    \t/3400, val loss:41.4129638671875\n",
      "training on iteration:1446    \t/3400, val loss:41.29459762573242\n",
      "training on iteration:1447    \t/3400, val loss:41.468505859375\n",
      "training on iteration:1448    \t/3400, val loss:41.92251205444336\n",
      "training on iteration:1449    \t/3400, val loss:42.1116828918457\n",
      "training on iteration:1450    \t/3400, val loss:41.70556640625\n",
      "training on iteration:1451    \t/3400, val loss:41.46379852294922\n",
      "training on iteration:1452    \t/3400, val loss:41.32987594604492\n",
      "training on iteration:1453    \t/3400, val loss:41.50442123413086\n",
      "training on iteration:1454    \t/3400, val loss:42.00711441040039\n",
      "training on iteration:1455    \t/3400, val loss:41.892337799072266\n",
      "training on iteration:1456    \t/3400, val loss:41.59168243408203\n",
      "training on iteration:1457    \t/3400, val loss:41.46580123901367\n",
      "training on iteration:1458    \t/3400, val loss:41.53416061401367\n",
      "training on iteration:1459    \t/3400, val loss:41.700950622558594\n",
      "training on iteration:1460    \t/3400, val loss:41.88923263549805\n",
      "training on iteration:1461    \t/3400, val loss:41.499263763427734\n",
      "training on iteration:1462    \t/3400, val loss:41.34760284423828\n",
      "training on iteration:1463    \t/3400, val loss:41.26922607421875\n",
      "training on iteration:1464    \t/3400, val loss:41.50692367553711\n",
      "training on iteration:1465    \t/3400, val loss:41.97850799560547\n",
      "training on iteration:1466    \t/3400, val loss:42.15515899658203\n",
      "training on iteration:1467    \t/3400, val loss:41.70558547973633\n",
      "training on iteration:1468    \t/3400, val loss:41.419525146484375\n",
      "training on iteration:1469    \t/3400, val loss:41.28655242919922\n",
      "training on iteration:1470    \t/3400, val loss:41.446556091308594\n",
      "training on iteration:1471    \t/3400, val loss:41.95775604248047\n",
      "training on iteration:1472    \t/3400, val loss:41.841243743896484\n",
      "training on iteration:1473    \t/3400, val loss:41.50325393676758\n",
      "training on iteration:1474    \t/3400, val loss:41.35347366333008\n",
      "training on iteration:1475    \t/3400, val loss:41.43612289428711\n",
      "training on iteration:1476    \t/3400, val loss:41.68726348876953\n",
      "training on iteration:1477    \t/3400, val loss:41.98526382446289\n",
      "training on iteration:1478    \t/3400, val loss:41.565738677978516\n",
      "training on iteration:1479    \t/3400, val loss:41.350318908691406\n",
      "training on iteration:1480    \t/3400, val loss:41.23955154418945\n",
      "training on iteration:1481    \t/3400, val loss:41.448768615722656\n",
      "training on iteration:1482    \t/3400, val loss:41.9202880859375\n",
      "training on iteration:1483    \t/3400, val loss:42.11096954345703\n",
      "training on iteration:1484    \t/3400, val loss:41.67325973510742\n",
      "training on iteration:1485    \t/3400, val loss:41.39771270751953\n",
      "training on iteration:1486    \t/3400, val loss:41.259159088134766\n",
      "training on iteration:1487    \t/3400, val loss:41.41682052612305\n",
      "training on iteration:1488    \t/3400, val loss:41.93355178833008\n",
      "training on iteration:1489    \t/3400, val loss:41.865882873535156\n",
      "training on iteration:1490    \t/3400, val loss:41.50909423828125\n",
      "training on iteration:1491    \t/3400, val loss:41.338932037353516\n",
      "training on iteration:1492    \t/3400, val loss:41.41954803466797\n",
      "training on iteration:1493    \t/3400, val loss:41.67099380493164\n",
      "training on iteration:1494    \t/3400, val loss:41.96086883544922\n",
      "training on iteration:1495    \t/3400, val loss:41.553184509277344\n",
      "training on iteration:1496    \t/3400, val loss:41.3385124206543\n",
      "training on iteration:1497    \t/3400, val loss:41.226593017578125\n",
      "training on iteration:1498    \t/3400, val loss:41.45738983154297\n",
      "training on iteration:1499    \t/3400, val loss:41.95294952392578\n",
      "training on iteration:1500    \t/3400, val loss:42.082332611083984\n",
      "training on iteration:1501    \t/3400, val loss:41.58633804321289\n",
      "training on iteration:1502    \t/3400, val loss:41.341041564941406\n",
      "training on iteration:1503    \t/3400, val loss:41.27218246459961\n",
      "training on iteration:1504    \t/3400, val loss:41.50081253051758\n",
      "training on iteration:1505    \t/3400, val loss:42.01021957397461\n",
      "training on iteration:1506    \t/3400, val loss:41.801910400390625\n",
      "training on iteration:1507    \t/3400, val loss:41.412540435791016\n",
      "training on iteration:1508    \t/3400, val loss:41.26505661010742\n",
      "training on iteration:1509    \t/3400, val loss:41.36702346801758\n",
      "training on iteration:1510    \t/3400, val loss:41.684391021728516\n",
      "training on iteration:1511    \t/3400, val loss:41.99721908569336\n",
      "training on iteration:1512    \t/3400, val loss:41.51129150390625\n",
      "training on iteration:1513    \t/3400, val loss:41.26749038696289\n",
      "training on iteration:1514    \t/3400, val loss:41.16888427734375\n",
      "training on iteration:1515    \t/3400, val loss:41.44400405883789\n",
      "training on iteration:1516    \t/3400, val loss:42.029720306396484\n",
      "training on iteration:1517    \t/3400, val loss:42.13446807861328\n",
      "training on iteration:1518    \t/3400, val loss:41.60901641845703\n",
      "training on iteration:1519    \t/3400, val loss:41.325496673583984\n",
      "training on iteration:1520    \t/3400, val loss:41.22330093383789\n",
      "training on iteration:1521    \t/3400, val loss:41.46522903442383\n",
      "training on iteration:1522    \t/3400, val loss:41.99351119995117\n",
      "training on iteration:1523    \t/3400, val loss:41.76700210571289\n",
      "training on iteration:1524    \t/3400, val loss:41.34688186645508\n",
      "training on iteration:1525    \t/3400, val loss:41.22225570678711\n",
      "training on iteration:1526    \t/3400, val loss:41.36769485473633\n",
      "training on iteration:1527    \t/3400, val loss:41.7638053894043\n",
      "training on iteration:1528    \t/3400, val loss:42.025543212890625\n",
      "training on iteration:1529    \t/3400, val loss:41.4793701171875\n",
      "training on iteration:1530    \t/3400, val loss:41.21240234375\n",
      "training on iteration:1531    \t/3400, val loss:41.13719940185547\n",
      "training on iteration:1532    \t/3400, val loss:41.462432861328125\n",
      "training on iteration:1533    \t/3400, val loss:42.060585021972656\n",
      "training on iteration:1534    \t/3400, val loss:42.13649368286133\n",
      "training on iteration:1535    \t/3400, val loss:41.55574417114258\n",
      "training on iteration:1536    \t/3400, val loss:41.26681900024414\n",
      "training on iteration:1537    \t/3400, val loss:41.197052001953125\n",
      "training on iteration:1538    \t/3400, val loss:41.50157928466797\n",
      "training on iteration:1539    \t/3400, val loss:42.0948486328125\n",
      "training on iteration:1540    \t/3400, val loss:41.83413314819336\n",
      "training on iteration:1541    \t/3400, val loss:41.37701416015625\n",
      "training on iteration:1542    \t/3400, val loss:41.194244384765625\n",
      "training on iteration:1543    \t/3400, val loss:41.30064010620117\n",
      "training on iteration:1544    \t/3400, val loss:41.647315979003906\n",
      "training on iteration:1545    \t/3400, val loss:41.98320770263672\n",
      "training on iteration:1546    \t/3400, val loss:41.48490524291992\n",
      "training on iteration:1547    \t/3400, val loss:41.20672607421875\n",
      "training on iteration:1548    \t/3400, val loss:41.112403869628906\n",
      "training on iteration:1549    \t/3400, val loss:41.393428802490234\n",
      "training on iteration:1550    \t/3400, val loss:41.97688674926758\n",
      "training on iteration:1551    \t/3400, val loss:42.06853485107422\n",
      "training on iteration:1552    \t/3400, val loss:41.516578674316406\n",
      "training on iteration:1553    \t/3400, val loss:41.269264221191406\n",
      "training on iteration:1554    \t/3400, val loss:41.21315383911133\n",
      "training on iteration:1555    \t/3400, val loss:41.4991569519043\n",
      "training on iteration:1556    \t/3400, val loss:41.99433135986328\n",
      "training on iteration:1557    \t/3400, val loss:41.715126037597656\n",
      "training on iteration:1558    \t/3400, val loss:41.3018913269043\n",
      "training on iteration:1559    \t/3400, val loss:41.15936279296875\n",
      "training on iteration:1560    \t/3400, val loss:41.28070068359375\n",
      "training on iteration:1561    \t/3400, val loss:41.64583969116211\n",
      "training on iteration:1562    \t/3400, val loss:42.00519561767578\n",
      "training on iteration:1563    \t/3400, val loss:41.50558853149414\n",
      "training on iteration:1564    \t/3400, val loss:41.23843765258789\n",
      "training on iteration:1565    \t/3400, val loss:41.131996154785156\n",
      "training on iteration:1566    \t/3400, val loss:41.42911148071289\n",
      "training on iteration:1567    \t/3400, val loss:42.00471878051758\n",
      "training on iteration:1568    \t/3400, val loss:42.07154083251953\n",
      "training on iteration:1569    \t/3400, val loss:41.459930419921875\n",
      "training on iteration:1570    \t/3400, val loss:41.185791015625\n",
      "training on iteration:1571    \t/3400, val loss:41.13893508911133\n",
      "training on iteration:1572    \t/3400, val loss:41.42578887939453\n",
      "training on iteration:1573    \t/3400, val loss:42.02384567260742\n",
      "training on iteration:1574    \t/3400, val loss:41.82143783569336\n",
      "training on iteration:1575    \t/3400, val loss:41.34765625\n",
      "training on iteration:1576    \t/3400, val loss:41.13896942138672\n",
      "training on iteration:1577    \t/3400, val loss:41.228458404541016\n",
      "training on iteration:1578    \t/3400, val loss:41.595924377441406\n",
      "training on iteration:1579    \t/3400, val loss:42.013221740722656\n",
      "training on iteration:1580    \t/3400, val loss:41.57793426513672\n",
      "training on iteration:1581    \t/3400, val loss:41.271240234375\n",
      "training on iteration:1582    \t/3400, val loss:41.12325668334961\n",
      "training on iteration:1583    \t/3400, val loss:41.330352783203125\n",
      "training on iteration:1584    \t/3400, val loss:41.826515197753906\n",
      "training on iteration:1585    \t/3400, val loss:41.989505767822266\n",
      "training on iteration:1586    \t/3400, val loss:41.51594924926758\n",
      "training on iteration:1587    \t/3400, val loss:41.24723434448242\n",
      "training on iteration:1588    \t/3400, val loss:41.151668548583984\n",
      "training on iteration:1589    \t/3400, val loss:41.379493713378906\n",
      "training on iteration:1590    \t/3400, val loss:41.90461349487305\n",
      "training on iteration:1591    \t/3400, val loss:41.67192840576172\n",
      "training on iteration:1592    \t/3400, val loss:41.256591796875\n",
      "training on iteration:1593    \t/3400, val loss:41.11650848388672\n",
      "training on iteration:1594    \t/3400, val loss:41.22676467895508\n",
      "training on iteration:1595    \t/3400, val loss:41.61079025268555\n",
      "training on iteration:1596    \t/3400, val loss:42.01123046875\n",
      "training on iteration:1597    \t/3400, val loss:41.521636962890625\n",
      "training on iteration:1598    \t/3400, val loss:41.21353530883789\n",
      "training on iteration:1599    \t/3400, val loss:41.09267044067383\n",
      "training on iteration:1600    \t/3400, val loss:41.32845687866211\n",
      "training on iteration:1601    \t/3400, val loss:41.86698532104492\n",
      "training on iteration:1602    \t/3400, val loss:41.98361587524414\n",
      "training on iteration:1603    \t/3400, val loss:41.458316802978516\n",
      "training on iteration:1604    \t/3400, val loss:41.172916412353516\n",
      "training on iteration:1605    \t/3400, val loss:41.09674072265625\n",
      "training on iteration:1606    \t/3400, val loss:41.361968994140625\n",
      "training on iteration:1607    \t/3400, val loss:41.909515380859375\n",
      "training on iteration:1608    \t/3400, val loss:41.70809555053711\n",
      "training on iteration:1609    \t/3400, val loss:41.27288818359375\n",
      "training on iteration:1610    \t/3400, val loss:41.120670318603516\n",
      "training on iteration:1611    \t/3400, val loss:41.23279571533203\n",
      "training on iteration:1612    \t/3400, val loss:41.588375091552734\n",
      "training on iteration:1613    \t/3400, val loss:41.97827911376953\n",
      "training on iteration:1614    \t/3400, val loss:41.496726989746094\n",
      "training on iteration:1615    \t/3400, val loss:41.177066802978516\n",
      "training on iteration:1616    \t/3400, val loss:41.0687141418457\n",
      "training on iteration:1617    \t/3400, val loss:41.32016372680664\n",
      "training on iteration:1618    \t/3400, val loss:41.82748031616211\n",
      "training on iteration:1619    \t/3400, val loss:41.92852020263672\n",
      "training on iteration:1620    \t/3400, val loss:41.43174362182617\n",
      "training on iteration:1621    \t/3400, val loss:41.20923614501953\n",
      "training on iteration:1622    \t/3400, val loss:41.15473556518555\n",
      "training on iteration:1623    \t/3400, val loss:41.45048141479492\n",
      "training on iteration:1624    \t/3400, val loss:41.90245819091797\n",
      "training on iteration:1625    \t/3400, val loss:41.57649612426758\n",
      "training on iteration:1626    \t/3400, val loss:41.19451141357422\n",
      "training on iteration:1627    \t/3400, val loss:41.074562072753906\n",
      "training on iteration:1628    \t/3400, val loss:41.25484085083008\n",
      "training on iteration:1629    \t/3400, val loss:41.6978759765625\n",
      "training on iteration:1630    \t/3400, val loss:41.98679733276367\n",
      "training on iteration:1631    \t/3400, val loss:41.3955078125\n",
      "training on iteration:1632    \t/3400, val loss:41.085018157958984\n",
      "training on iteration:1633    \t/3400, val loss:40.99272918701172\n",
      "training on iteration:1634    \t/3400, val loss:41.28157424926758\n",
      "training on iteration:1635    \t/3400, val loss:41.91641616821289\n",
      "training on iteration:1636    \t/3400, val loss:42.031429290771484\n",
      "training on iteration:1637    \t/3400, val loss:41.411041259765625\n",
      "training on iteration:1638    \t/3400, val loss:41.14097213745117\n",
      "training on iteration:1639    \t/3400, val loss:41.100921630859375\n",
      "training on iteration:1640    \t/3400, val loss:41.44768524169922\n",
      "training on iteration:1641    \t/3400, val loss:41.959373474121094\n",
      "training on iteration:1642    \t/3400, val loss:41.59926986694336\n",
      "training on iteration:1643    \t/3400, val loss:41.15261459350586\n",
      "training on iteration:1644    \t/3400, val loss:41.04344940185547\n",
      "training on iteration:1645    \t/3400, val loss:41.270931243896484\n",
      "training on iteration:1646    \t/3400, val loss:41.776153564453125\n",
      "training on iteration:1647    \t/3400, val loss:41.98431396484375\n",
      "training on iteration:1648    \t/3400, val loss:41.333763122558594\n",
      "training on iteration:1649    \t/3400, val loss:41.0299072265625\n",
      "training on iteration:1650    \t/3400, val loss:40.96941375732422\n",
      "training on iteration:1651    \t/3400, val loss:41.29619598388672\n",
      "training on iteration:1652    \t/3400, val loss:41.929012298583984\n",
      "training on iteration:1653    \t/3400, val loss:41.99677276611328\n",
      "training on iteration:1654    \t/3400, val loss:41.375186920166016\n",
      "training on iteration:1655    \t/3400, val loss:41.15325164794922\n",
      "training on iteration:1656    \t/3400, val loss:41.12721633911133\n",
      "training on iteration:1657    \t/3400, val loss:41.470458984375\n",
      "training on iteration:1658    \t/3400, val loss:41.953372955322266\n",
      "training on iteration:1659    \t/3400, val loss:41.54364776611328\n",
      "training on iteration:1660    \t/3400, val loss:41.0767936706543\n",
      "training on iteration:1661    \t/3400, val loss:40.98625946044922\n",
      "training on iteration:1662    \t/3400, val loss:41.25292205810547\n",
      "training on iteration:1663    \t/3400, val loss:41.81878662109375\n",
      "training on iteration:1664    \t/3400, val loss:41.99261474609375\n",
      "training on iteration:1665    \t/3400, val loss:41.32109451293945\n",
      "training on iteration:1666    \t/3400, val loss:41.02170944213867\n",
      "training on iteration:1667    \t/3400, val loss:40.94928741455078\n",
      "training on iteration:1668    \t/3400, val loss:41.251548767089844\n",
      "training on iteration:1669    \t/3400, val loss:41.926918029785156\n",
      "training on iteration:1670    \t/3400, val loss:42.03152847290039\n",
      "training on iteration:1671    \t/3400, val loss:41.36420440673828\n",
      "training on iteration:1672    \t/3400, val loss:41.087432861328125\n",
      "training on iteration:1673    \t/3400, val loss:41.043556213378906\n",
      "training on iteration:1674    \t/3400, val loss:41.348388671875\n",
      "training on iteration:1675    \t/3400, val loss:41.88982009887695\n",
      "training on iteration:1676    \t/3400, val loss:41.60580062866211\n",
      "training on iteration:1677    \t/3400, val loss:41.141624450683594\n",
      "training on iteration:1678    \t/3400, val loss:40.98936080932617\n",
      "training on iteration:1679    \t/3400, val loss:41.1720085144043\n",
      "training on iteration:1680    \t/3400, val loss:41.66897201538086\n",
      "training on iteration:1681    \t/3400, val loss:41.94045639038086\n",
      "training on iteration:1682    \t/3400, val loss:41.33353805541992\n",
      "training on iteration:1683    \t/3400, val loss:41.01823043823242\n",
      "training on iteration:1684    \t/3400, val loss:40.935482025146484\n",
      "training on iteration:1685    \t/3400, val loss:41.21531295776367\n",
      "training on iteration:1686    \t/3400, val loss:41.80777359008789\n",
      "training on iteration:1687    \t/3400, val loss:41.917930603027344\n",
      "training on iteration:1688    \t/3400, val loss:41.3223876953125\n",
      "training on iteration:1689    \t/3400, val loss:41.091495513916016\n",
      "training on iteration:1690    \t/3400, val loss:41.05785369873047\n",
      "training on iteration:1691    \t/3400, val loss:41.39394760131836\n",
      "training on iteration:1692    \t/3400, val loss:41.890411376953125\n",
      "training on iteration:1693    \t/3400, val loss:41.54642105102539\n",
      "training on iteration:1694    \t/3400, val loss:41.07787322998047\n",
      "training on iteration:1695    \t/3400, val loss:40.95536422729492\n",
      "training on iteration:1696    \t/3400, val loss:41.1764030456543\n",
      "training on iteration:1697    \t/3400, val loss:41.71770095825195\n",
      "training on iteration:1698    \t/3400, val loss:41.95488739013672\n",
      "training on iteration:1699    \t/3400, val loss:41.31635284423828\n",
      "training on iteration:1700    \t/3400, val loss:40.99654006958008\n",
      "training on iteration:1701    \t/3400, val loss:40.91678237915039\n",
      "training on iteration:1702    \t/3400, val loss:41.199188232421875\n",
      "training on iteration:1703    \t/3400, val loss:41.85152053833008\n",
      "training on iteration:1704    \t/3400, val loss:41.97908401489258\n",
      "training on iteration:1705    \t/3400, val loss:41.335182189941406\n",
      "training on iteration:1706    \t/3400, val loss:41.04587173461914\n",
      "training on iteration:1707    \t/3400, val loss:40.99286651611328\n",
      "training on iteration:1708    \t/3400, val loss:41.30310821533203\n",
      "training on iteration:1709    \t/3400, val loss:41.83720016479492\n",
      "training on iteration:1710    \t/3400, val loss:41.53693389892578\n",
      "training on iteration:1711    \t/3400, val loss:41.07732391357422\n",
      "training on iteration:1712    \t/3400, val loss:40.95854949951172\n",
      "training on iteration:1713    \t/3400, val loss:41.174129486083984\n",
      "training on iteration:1714    \t/3400, val loss:41.70755386352539\n",
      "training on iteration:1715    \t/3400, val loss:41.92221450805664\n",
      "training on iteration:1716    \t/3400, val loss:41.2823486328125\n",
      "training on iteration:1717    \t/3400, val loss:40.966514587402344\n",
      "training on iteration:1718    \t/3400, val loss:40.89130783081055\n",
      "training on iteration:1719    \t/3400, val loss:41.1694450378418\n",
      "training on iteration:1720    \t/3400, val loss:41.769615173339844\n",
      "training on iteration:1721    \t/3400, val loss:41.89456558227539\n",
      "training on iteration:1722    \t/3400, val loss:41.30353546142578\n",
      "training on iteration:1723    \t/3400, val loss:41.08549118041992\n",
      "training on iteration:1724    \t/3400, val loss:41.03273391723633\n",
      "training on iteration:1725    \t/3400, val loss:41.31314468383789\n",
      "training on iteration:1726    \t/3400, val loss:41.79755783081055\n",
      "training on iteration:1727    \t/3400, val loss:41.510738372802734\n",
      "training on iteration:1728    \t/3400, val loss:41.043724060058594\n",
      "training on iteration:1729    \t/3400, val loss:40.91039276123047\n",
      "training on iteration:1730    \t/3400, val loss:41.1510124206543\n",
      "training on iteration:1731    \t/3400, val loss:41.73392868041992\n",
      "training on iteration:1732    \t/3400, val loss:41.9404296875\n",
      "training on iteration:1733    \t/3400, val loss:41.262088775634766\n",
      "training on iteration:1734    \t/3400, val loss:40.93479537963867\n",
      "training on iteration:1735    \t/3400, val loss:40.8532829284668\n",
      "training on iteration:1736    \t/3400, val loss:41.15147018432617\n",
      "training on iteration:1737    \t/3400, val loss:41.80459213256836\n",
      "training on iteration:1738    \t/3400, val loss:41.93082809448242\n",
      "training on iteration:1739    \t/3400, val loss:41.37010955810547\n",
      "training on iteration:1740    \t/3400, val loss:41.14936065673828\n",
      "training on iteration:1741    \t/3400, val loss:41.070762634277344\n",
      "training on iteration:1742    \t/3400, val loss:41.30026626586914\n",
      "training on iteration:1743    \t/3400, val loss:41.72826385498047\n",
      "training on iteration:1744    \t/3400, val loss:41.43446731567383\n",
      "training on iteration:1745    \t/3400, val loss:41.005462646484375\n",
      "training on iteration:1746    \t/3400, val loss:40.89937973022461\n",
      "training on iteration:1747    \t/3400, val loss:41.113887786865234\n",
      "training on iteration:1748    \t/3400, val loss:41.64008331298828\n",
      "training on iteration:1749    \t/3400, val loss:41.913143157958984\n",
      "training on iteration:1750    \t/3400, val loss:41.31018829345703\n",
      "training on iteration:1751    \t/3400, val loss:40.97758102416992\n",
      "training on iteration:1752    \t/3400, val loss:40.88569259643555\n",
      "training on iteration:1753    \t/3400, val loss:41.14224624633789\n",
      "training on iteration:1754    \t/3400, val loss:41.70922088623047\n",
      "training on iteration:1755    \t/3400, val loss:41.82725143432617\n",
      "training on iteration:1756    \t/3400, val loss:41.24440383911133\n",
      "training on iteration:1757    \t/3400, val loss:41.02817153930664\n",
      "training on iteration:1758    \t/3400, val loss:41.00906753540039\n",
      "training on iteration:1759    \t/3400, val loss:41.308475494384766\n",
      "training on iteration:1760    \t/3400, val loss:41.743873596191406\n",
      "training on iteration:1761    \t/3400, val loss:41.39144515991211\n",
      "training on iteration:1762    \t/3400, val loss:40.982704162597656\n",
      "training on iteration:1763    \t/3400, val loss:40.88161849975586\n",
      "training on iteration:1764    \t/3400, val loss:41.11703109741211\n",
      "training on iteration:1765    \t/3400, val loss:41.66365432739258\n",
      "training on iteration:1766    \t/3400, val loss:41.89007568359375\n",
      "training on iteration:1767    \t/3400, val loss:41.23665237426758\n",
      "training on iteration:1768    \t/3400, val loss:40.91883087158203\n",
      "training on iteration:1769    \t/3400, val loss:40.84421157836914\n",
      "training on iteration:1770    \t/3400, val loss:41.15487289428711\n",
      "training on iteration:1771    \t/3400, val loss:41.79397201538086\n",
      "training on iteration:1772    \t/3400, val loss:41.87877655029297\n",
      "training on iteration:1773    \t/3400, val loss:41.25170135498047\n",
      "training on iteration:1774    \t/3400, val loss:40.995269775390625\n",
      "training on iteration:1775    \t/3400, val loss:40.95694351196289\n",
      "training on iteration:1776    \t/3400, val loss:41.273887634277344\n",
      "training on iteration:1777    \t/3400, val loss:41.78867721557617\n",
      "training on iteration:1778    \t/3400, val loss:41.48158645629883\n",
      "training on iteration:1779    \t/3400, val loss:40.99855422973633\n",
      "training on iteration:1780    \t/3400, val loss:40.86024856567383\n",
      "training on iteration:1781    \t/3400, val loss:41.07154083251953\n",
      "training on iteration:1782    \t/3400, val loss:41.68572235107422\n",
      "training on iteration:1783    \t/3400, val loss:41.93132781982422\n",
      "training on iteration:1784    \t/3400, val loss:41.25654983520508\n",
      "training on iteration:1785    \t/3400, val loss:40.903778076171875\n",
      "training on iteration:1786    \t/3400, val loss:40.81497573852539\n",
      "training on iteration:1787    \t/3400, val loss:41.09026336669922\n",
      "training on iteration:1788    \t/3400, val loss:41.739288330078125\n",
      "training on iteration:1789    \t/3400, val loss:41.86342239379883\n",
      "training on iteration:1790    \t/3400, val loss:41.22910690307617\n",
      "training on iteration:1791    \t/3400, val loss:41.008644104003906\n",
      "training on iteration:1792    \t/3400, val loss:40.94845962524414\n",
      "training on iteration:1793    \t/3400, val loss:41.2342414855957\n",
      "training on iteration:1794    \t/3400, val loss:41.72146987915039\n",
      "training on iteration:1795    \t/3400, val loss:41.44392776489258\n",
      "training on iteration:1796    \t/3400, val loss:40.97404098510742\n",
      "training on iteration:1797    \t/3400, val loss:40.84401321411133\n",
      "training on iteration:1798    \t/3400, val loss:41.05556106567383\n",
      "training on iteration:1799    \t/3400, val loss:41.64643478393555\n",
      "training on iteration:1800    \t/3400, val loss:41.87357711791992\n",
      "training on iteration:1801    \t/3400, val loss:41.223388671875\n",
      "training on iteration:1802    \t/3400, val loss:40.89125442504883\n",
      "training on iteration:1803    \t/3400, val loss:40.81175994873047\n",
      "training on iteration:1804    \t/3400, val loss:41.08590316772461\n",
      "training on iteration:1805    \t/3400, val loss:41.67350387573242\n",
      "training on iteration:1806    \t/3400, val loss:41.80703353881836\n",
      "training on iteration:1807    \t/3400, val loss:41.22838592529297\n",
      "training on iteration:1808    \t/3400, val loss:41.02173614501953\n",
      "training on iteration:1809    \t/3400, val loss:40.96764373779297\n",
      "training on iteration:1810    \t/3400, val loss:41.249900817871094\n",
      "training on iteration:1811    \t/3400, val loss:41.67789077758789\n",
      "training on iteration:1812    \t/3400, val loss:41.334720611572266\n",
      "training on iteration:1813    \t/3400, val loss:40.89404296875\n",
      "training on iteration:1814    \t/3400, val loss:40.81496810913086\n",
      "training on iteration:1815    \t/3400, val loss:41.10029220581055\n",
      "training on iteration:1816    \t/3400, val loss:41.75491714477539\n",
      "training on iteration:1817    \t/3400, val loss:41.90597152709961\n",
      "training on iteration:1818    \t/3400, val loss:41.19625473022461\n",
      "training on iteration:1819    \t/3400, val loss:40.85607147216797\n",
      "training on iteration:1820    \t/3400, val loss:40.778167724609375\n",
      "training on iteration:1821    \t/3400, val loss:41.07880783081055\n",
      "training on iteration:1822    \t/3400, val loss:41.71940994262695\n",
      "training on iteration:1823    \t/3400, val loss:41.82158279418945\n",
      "training on iteration:1824    \t/3400, val loss:41.20636749267578\n",
      "training on iteration:1825    \t/3400, val loss:40.999122619628906\n",
      "training on iteration:1826    \t/3400, val loss:40.95195388793945\n",
      "training on iteration:1827    \t/3400, val loss:41.239559173583984\n",
      "training on iteration:1828    \t/3400, val loss:41.67749786376953\n",
      "training on iteration:1829    \t/3400, val loss:41.34779739379883\n",
      "training on iteration:1830    \t/3400, val loss:40.899234771728516\n",
      "training on iteration:1831    \t/3400, val loss:40.80961227416992\n",
      "training on iteration:1832    \t/3400, val loss:41.06191635131836\n",
      "training on iteration:1833    \t/3400, val loss:41.70629119873047\n",
      "training on iteration:1834    \t/3400, val loss:41.90361404418945\n",
      "training on iteration:1835    \t/3400, val loss:41.18767166137695\n",
      "training on iteration:1836    \t/3400, val loss:40.84255599975586\n",
      "training on iteration:1837    \t/3400, val loss:40.7557373046875\n",
      "training on iteration:1838    \t/3400, val loss:41.02541732788086\n",
      "training on iteration:1839    \t/3400, val loss:41.66501235961914\n",
      "training on iteration:1840    \t/3400, val loss:41.82332992553711\n",
      "training on iteration:1841    \t/3400, val loss:41.229618072509766\n",
      "training on iteration:1842    \t/3400, val loss:41.006614685058594\n",
      "training on iteration:1843    \t/3400, val loss:40.939720153808594\n",
      "training on iteration:1844    \t/3400, val loss:41.168304443359375\n",
      "training on iteration:1845    \t/3400, val loss:41.59647750854492\n",
      "training on iteration:1846    \t/3400, val loss:41.28517532348633\n",
      "training on iteration:1847    \t/3400, val loss:40.86223602294922\n",
      "training on iteration:1848    \t/3400, val loss:40.787017822265625\n",
      "training on iteration:1849    \t/3400, val loss:41.063758850097656\n",
      "training on iteration:1850    \t/3400, val loss:41.70426559448242\n",
      "training on iteration:1851    \t/3400, val loss:41.86760711669922\n",
      "training on iteration:1852    \t/3400, val loss:41.1556510925293\n",
      "training on iteration:1853    \t/3400, val loss:40.80308151245117\n",
      "training on iteration:1854    \t/3400, val loss:40.733612060546875\n",
      "training on iteration:1855    \t/3400, val loss:41.061161041259766\n",
      "training on iteration:1856    \t/3400, val loss:41.68073272705078\n",
      "training on iteration:1857    \t/3400, val loss:41.76011276245117\n",
      "training on iteration:1858    \t/3400, val loss:41.15342330932617\n",
      "training on iteration:1859    \t/3400, val loss:40.953487396240234\n",
      "training on iteration:1860    \t/3400, val loss:40.90377426147461\n",
      "training on iteration:1861    \t/3400, val loss:41.16965103149414\n",
      "training on iteration:1862    \t/3400, val loss:41.60701370239258\n",
      "training on iteration:1863    \t/3400, val loss:41.30183792114258\n",
      "training on iteration:1864    \t/3400, val loss:40.86946487426758\n",
      "training on iteration:1865    \t/3400, val loss:40.77791976928711\n",
      "training on iteration:1866    \t/3400, val loss:41.02688980102539\n",
      "training on iteration:1867    \t/3400, val loss:41.6755485534668\n",
      "training on iteration:1868    \t/3400, val loss:41.84760665893555\n",
      "training on iteration:1869    \t/3400, val loss:41.13383865356445\n",
      "training on iteration:1870    \t/3400, val loss:40.79446029663086\n",
      "training on iteration:1871    \t/3400, val loss:40.72472381591797\n",
      "training on iteration:1872    \t/3400, val loss:41.040794372558594\n",
      "training on iteration:1873    \t/3400, val loss:41.68804168701172\n",
      "training on iteration:1874    \t/3400, val loss:41.75463104248047\n",
      "training on iteration:1875    \t/3400, val loss:41.11410903930664\n",
      "training on iteration:1876    \t/3400, val loss:40.90812301635742\n",
      "training on iteration:1877    \t/3400, val loss:40.87760543823242\n",
      "training on iteration:1878    \t/3400, val loss:41.19057083129883\n",
      "training on iteration:1879    \t/3400, val loss:41.67397689819336\n",
      "training on iteration:1880    \t/3400, val loss:41.28231430053711\n",
      "training on iteration:1881    \t/3400, val loss:40.8209114074707\n",
      "training on iteration:1882    \t/3400, val loss:40.74374771118164\n",
      "training on iteration:1883    \t/3400, val loss:41.02442169189453\n",
      "training on iteration:1884    \t/3400, val loss:41.69286346435547\n",
      "training on iteration:1885    \t/3400, val loss:41.833740234375\n",
      "training on iteration:1886    \t/3400, val loss:41.10519790649414\n",
      "training on iteration:1887    \t/3400, val loss:40.77310562133789\n",
      "training on iteration:1888    \t/3400, val loss:40.70210647583008\n",
      "training on iteration:1889    \t/3400, val loss:41.00870895385742\n",
      "training on iteration:1890    \t/3400, val loss:41.66766357421875\n",
      "training on iteration:1891    \t/3400, val loss:41.79669189453125\n",
      "training on iteration:1892    \t/3400, val loss:41.14119338989258\n",
      "training on iteration:1893    \t/3400, val loss:40.90708541870117\n",
      "training on iteration:1894    \t/3400, val loss:40.841270446777344\n",
      "training on iteration:1895    \t/3400, val loss:41.13041305541992\n",
      "training on iteration:1896    \t/3400, val loss:41.64877700805664\n",
      "training on iteration:1897    \t/3400, val loss:41.32027816772461\n",
      "training on iteration:1898    \t/3400, val loss:40.84801483154297\n",
      "training on iteration:1899    \t/3400, val loss:40.73982620239258\n",
      "training on iteration:1900    \t/3400, val loss:40.97097396850586\n",
      "training on iteration:1901    \t/3400, val loss:41.58359146118164\n",
      "training on iteration:1902    \t/3400, val loss:41.79353713989258\n",
      "training on iteration:1903    \t/3400, val loss:41.10197067260742\n",
      "training on iteration:1904    \t/3400, val loss:40.771728515625\n",
      "training on iteration:1905    \t/3400, val loss:40.695316314697266\n",
      "training on iteration:1906    \t/3400, val loss:40.97005844116211\n",
      "training on iteration:1907    \t/3400, val loss:41.6441650390625\n",
      "training on iteration:1908    \t/3400, val loss:41.82209014892578\n",
      "training on iteration:1909    \t/3400, val loss:41.129539489746094\n",
      "training on iteration:1910    \t/3400, val loss:40.85953903198242\n",
      "training on iteration:1911    \t/3400, val loss:40.790523529052734\n",
      "training on iteration:1912    \t/3400, val loss:41.07347869873047\n",
      "training on iteration:1913    \t/3400, val loss:41.614986419677734\n",
      "training on iteration:1914    \t/3400, val loss:41.300018310546875\n",
      "training on iteration:1915    \t/3400, val loss:40.82223129272461\n",
      "training on iteration:1916    \t/3400, val loss:40.718509674072266\n",
      "training on iteration:1917    \t/3400, val loss:40.95867919921875\n",
      "training on iteration:1918    \t/3400, val loss:41.56765365600586\n",
      "training on iteration:1919    \t/3400, val loss:41.768699645996094\n",
      "training on iteration:1920    \t/3400, val loss:41.08226776123047\n",
      "training on iteration:1921    \t/3400, val loss:40.758941650390625\n",
      "training on iteration:1922    \t/3400, val loss:40.68779754638672\n",
      "training on iteration:1923    \t/3400, val loss:40.991790771484375\n",
      "training on iteration:1924    \t/3400, val loss:41.64917755126953\n",
      "training on iteration:1925    \t/3400, val loss:41.7731819152832\n",
      "training on iteration:1926    \t/3400, val loss:41.10799026489258\n",
      "training on iteration:1927    \t/3400, val loss:40.852264404296875\n",
      "training on iteration:1928    \t/3400, val loss:40.78532791137695\n",
      "training on iteration:1929    \t/3400, val loss:41.137908935546875\n",
      "training on iteration:1930    \t/3400, val loss:41.64342498779297\n",
      "training on iteration:1931    \t/3400, val loss:41.23394012451172\n",
      "training on iteration:1932    \t/3400, val loss:40.77898406982422\n",
      "training on iteration:1933    \t/3400, val loss:40.69989013671875\n",
      "training on iteration:1934    \t/3400, val loss:40.96413040161133\n",
      "training on iteration:1935    \t/3400, val loss:41.62943649291992\n",
      "training on iteration:1936    \t/3400, val loss:41.76577377319336\n",
      "training on iteration:1937    \t/3400, val loss:41.06382369995117\n",
      "training on iteration:1938    \t/3400, val loss:40.734718322753906\n",
      "training on iteration:1939    \t/3400, val loss:40.65450668334961\n",
      "training on iteration:1940    \t/3400, val loss:40.94882583618164\n",
      "training on iteration:1941    \t/3400, val loss:41.62602996826172\n",
      "training on iteration:1942    \t/3400, val loss:41.78165817260742\n",
      "training on iteration:1943    \t/3400, val loss:41.131690979003906\n",
      "training on iteration:1944    \t/3400, val loss:40.856468200683594\n",
      "training on iteration:1945    \t/3400, val loss:40.757720947265625\n",
      "training on iteration:1946    \t/3400, val loss:41.03730010986328\n",
      "training on iteration:1947    \t/3400, val loss:41.54928970336914\n",
      "training on iteration:1948    \t/3400, val loss:41.22258377075195\n",
      "training on iteration:1949    \t/3400, val loss:40.78386688232422\n",
      "training on iteration:1950    \t/3400, val loss:40.70930862426758\n",
      "training on iteration:1951    \t/3400, val loss:40.96622848510742\n",
      "training on iteration:1952    \t/3400, val loss:41.56930160522461\n",
      "training on iteration:1953    \t/3400, val loss:41.74502944946289\n",
      "training on iteration:1954    \t/3400, val loss:41.02129364013672\n",
      "training on iteration:1955    \t/3400, val loss:40.701175689697266\n",
      "training on iteration:1956    \t/3400, val loss:40.6363639831543\n",
      "training on iteration:1957    \t/3400, val loss:40.92901611328125\n",
      "training on iteration:1958    \t/3400, val loss:41.64399337768555\n",
      "training on iteration:1959    \t/3400, val loss:41.810482025146484\n",
      "training on iteration:1960    \t/3400, val loss:41.1044921875\n",
      "training on iteration:1961    \t/3400, val loss:40.82567596435547\n",
      "training on iteration:1962    \t/3400, val loss:40.75153732299805\n",
      "training on iteration:1963    \t/3400, val loss:41.02350997924805\n",
      "training on iteration:1964    \t/3400, val loss:41.54877471923828\n",
      "training on iteration:1965    \t/3400, val loss:41.24212646484375\n",
      "training on iteration:1966    \t/3400, val loss:40.768375396728516\n",
      "training on iteration:1967    \t/3400, val loss:40.67081832885742\n",
      "training on iteration:1968    \t/3400, val loss:40.9299430847168\n",
      "training on iteration:1969    \t/3400, val loss:41.574684143066406\n",
      "training on iteration:1970    \t/3400, val loss:41.77951431274414\n",
      "training on iteration:1971    \t/3400, val loss:41.080806732177734\n",
      "training on iteration:1972    \t/3400, val loss:40.72119140625\n",
      "training on iteration:1973    \t/3400, val loss:40.618160247802734\n",
      "training on iteration:1974    \t/3400, val loss:40.91011428833008\n",
      "training on iteration:1975    \t/3400, val loss:41.52292251586914\n",
      "training on iteration:1976    \t/3400, val loss:41.66924285888672\n",
      "training on iteration:1977    \t/3400, val loss:41.06380081176758\n",
      "training on iteration:1978    \t/3400, val loss:40.83295440673828\n",
      "training on iteration:1979    \t/3400, val loss:40.742855072021484\n",
      "training on iteration:1980    \t/3400, val loss:41.037750244140625\n",
      "training on iteration:1981    \t/3400, val loss:41.57985305786133\n",
      "training on iteration:1982    \t/3400, val loss:41.21836471557617\n",
      "training on iteration:1983    \t/3400, val loss:40.758262634277344\n",
      "training on iteration:1984    \t/3400, val loss:40.674556732177734\n",
      "training on iteration:1985    \t/3400, val loss:40.932701110839844\n",
      "training on iteration:1986    \t/3400, val loss:41.57297897338867\n",
      "training on iteration:1987    \t/3400, val loss:41.76310348510742\n",
      "training on iteration:1988    \t/3400, val loss:41.07941436767578\n",
      "training on iteration:1989    \t/3400, val loss:40.71333694458008\n",
      "training on iteration:1990    \t/3400, val loss:40.60951614379883\n",
      "training on iteration:1991    \t/3400, val loss:40.85797882080078\n",
      "training on iteration:1992    \t/3400, val loss:41.53333282470703\n",
      "training on iteration:1993    \t/3400, val loss:41.77005386352539\n",
      "training on iteration:1994    \t/3400, val loss:41.09138870239258\n",
      "training on iteration:1995    \t/3400, val loss:40.79037094116211\n",
      "training on iteration:1996    \t/3400, val loss:40.6949348449707\n",
      "training on iteration:1997    \t/3400, val loss:40.97480010986328\n",
      "training on iteration:1998    \t/3400, val loss:41.471221923828125\n",
      "training on iteration:1999    \t/3400, val loss:41.13719940185547\n",
      "training on iteration:2000    \t/3400, val loss:40.704368591308594\n",
      "training on iteration:2001    \t/3400, val loss:40.63622283935547\n",
      "training on iteration:2002    \t/3400, val loss:40.91070556640625\n",
      "training on iteration:2003    \t/3400, val loss:41.54683303833008\n",
      "training on iteration:2004    \t/3400, val loss:41.733707427978516\n",
      "training on iteration:2005    \t/3400, val loss:41.00810623168945\n",
      "training on iteration:2006    \t/3400, val loss:40.67211151123047\n",
      "training on iteration:2007    \t/3400, val loss:40.59541702270508\n",
      "training on iteration:2008    \t/3400, val loss:40.89264678955078\n",
      "training on iteration:2009    \t/3400, val loss:41.604393005371094\n",
      "training on iteration:2010    \t/3400, val loss:41.729286193847656\n",
      "training on iteration:2011    \t/3400, val loss:41.0274658203125\n",
      "training on iteration:2012    \t/3400, val loss:40.76171112060547\n",
      "training on iteration:2013    \t/3400, val loss:40.68634033203125\n",
      "training on iteration:2014    \t/3400, val loss:41.023033142089844\n",
      "training on iteration:2015    \t/3400, val loss:41.61862564086914\n",
      "training on iteration:2016    \t/3400, val loss:41.21232986450195\n",
      "training on iteration:2017    \t/3400, val loss:40.68973159790039\n",
      "training on iteration:2018    \t/3400, val loss:40.60407257080078\n",
      "training on iteration:2019    \t/3400, val loss:40.898529052734375\n",
      "training on iteration:2020    \t/3400, val loss:41.58846664428711\n",
      "training on iteration:2021    \t/3400, val loss:41.761783599853516\n",
      "training on iteration:2022    \t/3400, val loss:41.018028259277344\n",
      "training on iteration:2023    \t/3400, val loss:40.663246154785156\n",
      "training on iteration:2024    \t/3400, val loss:40.58236312866211\n",
      "training on iteration:2025    \t/3400, val loss:40.882320404052734\n",
      "training on iteration:2026    \t/3400, val loss:41.591468811035156\n",
      "training on iteration:2027    \t/3400, val loss:41.690956115722656\n",
      "training on iteration:2028    \t/3400, val loss:40.97403335571289\n",
      "training on iteration:2029    \t/3400, val loss:40.73651123046875\n",
      "training on iteration:2030    \t/3400, val loss:40.66975784301758\n",
      "training on iteration:2031    \t/3400, val loss:41.023494720458984\n",
      "training on iteration:2032    \t/3400, val loss:41.55868911743164\n",
      "training on iteration:2033    \t/3400, val loss:41.15193557739258\n",
      "training on iteration:2034    \t/3400, val loss:40.669864654541016\n",
      "training on iteration:2035    \t/3400, val loss:40.613651275634766\n",
      "training on iteration:2036    \t/3400, val loss:40.89992904663086\n",
      "training on iteration:2037    \t/3400, val loss:41.607704162597656\n",
      "training on iteration:2038    \t/3400, val loss:41.68557357788086\n",
      "training on iteration:2039    \t/3400, val loss:40.93985366821289\n",
      "training on iteration:2040    \t/3400, val loss:40.625099182128906\n",
      "training on iteration:2041    \t/3400, val loss:40.56326675415039\n",
      "training on iteration:2042    \t/3400, val loss:40.87725830078125\n",
      "training on iteration:2043    \t/3400, val loss:41.5760612487793\n",
      "training on iteration:2044    \t/3400, val loss:41.67699432373047\n",
      "training on iteration:2045    \t/3400, val loss:40.95491409301758\n",
      "training on iteration:2046    \t/3400, val loss:40.70124053955078\n",
      "training on iteration:2047    \t/3400, val loss:40.63944625854492\n",
      "training on iteration:2048    \t/3400, val loss:40.97738265991211\n",
      "training on iteration:2049    \t/3400, val loss:41.44001007080078\n",
      "training on iteration:2050    \t/3400, val loss:41.058589935302734\n",
      "training on iteration:2051    \t/3400, val loss:40.64862060546875\n",
      "training on iteration:2052    \t/3400, val loss:40.633155822753906\n",
      "training on iteration:2053    \t/3400, val loss:40.95773696899414\n",
      "training on iteration:2054    \t/3400, val loss:41.5678825378418\n",
      "training on iteration:2055    \t/3400, val loss:41.60097122192383\n",
      "training on iteration:2056    \t/3400, val loss:40.863468170166016\n",
      "training on iteration:2057    \t/3400, val loss:40.5831413269043\n",
      "training on iteration:2058    \t/3400, val loss:40.56000518798828\n",
      "training on iteration:2059    \t/3400, val loss:40.89434814453125\n",
      "training on iteration:2060    \t/3400, val loss:41.558311462402344\n",
      "training on iteration:2061    \t/3400, val loss:41.67498779296875\n",
      "training on iteration:2062    \t/3400, val loss:40.964820861816406\n",
      "training on iteration:2063    \t/3400, val loss:40.70039749145508\n",
      "training on iteration:2064    \t/3400, val loss:40.615867614746094\n",
      "training on iteration:2065    \t/3400, val loss:40.92931365966797\n",
      "training on iteration:2066    \t/3400, val loss:41.371482849121094\n",
      "training on iteration:2067    \t/3400, val loss:41.0020751953125\n",
      "training on iteration:2068    \t/3400, val loss:40.62600326538086\n",
      "training on iteration:2069    \t/3400, val loss:40.65781021118164\n",
      "training on iteration:2070    \t/3400, val loss:41.03110122680664\n",
      "training on iteration:2071    \t/3400, val loss:41.60101318359375\n",
      "training on iteration:2072    \t/3400, val loss:41.54715347290039\n",
      "training on iteration:2073    \t/3400, val loss:40.80776596069336\n",
      "training on iteration:2074    \t/3400, val loss:40.571144104003906\n",
      "training on iteration:2075    \t/3400, val loss:40.56175994873047\n",
      "training on iteration:2076    \t/3400, val loss:40.912452697753906\n",
      "training on iteration:2077    \t/3400, val loss:41.49228286743164\n",
      "training on iteration:2078    \t/3400, val loss:41.569007873535156\n",
      "training on iteration:2079    \t/3400, val loss:40.93076705932617\n",
      "training on iteration:2080    \t/3400, val loss:40.70485305786133\n",
      "training on iteration:2081    \t/3400, val loss:40.621971130371094\n",
      "training on iteration:2082    \t/3400, val loss:40.985511779785156\n",
      "training on iteration:2083    \t/3400, val loss:41.4046630859375\n",
      "training on iteration:2084    \t/3400, val loss:40.9615478515625\n",
      "training on iteration:2085    \t/3400, val loss:40.577392578125\n",
      "training on iteration:2086    \t/3400, val loss:40.62553787231445\n",
      "training on iteration:2087    \t/3400, val loss:41.03516387939453\n",
      "training on iteration:2088    \t/3400, val loss:41.59389114379883\n",
      "training on iteration:2089    \t/3400, val loss:41.4833984375\n",
      "training on iteration:2090    \t/3400, val loss:40.76588821411133\n",
      "training on iteration:2091    \t/3400, val loss:40.5394287109375\n",
      "training on iteration:2092    \t/3400, val loss:40.55302047729492\n",
      "training on iteration:2093    \t/3400, val loss:40.92706298828125\n",
      "training on iteration:2094    \t/3400, val loss:41.477630615234375\n",
      "training on iteration:2095    \t/3400, val loss:41.52985763549805\n",
      "training on iteration:2096    \t/3400, val loss:40.92538070678711\n",
      "training on iteration:2097    \t/3400, val loss:40.69821548461914\n",
      "training on iteration:2098    \t/3400, val loss:40.60800552368164\n",
      "training on iteration:2099    \t/3400, val loss:40.96697998046875\n",
      "training on iteration:2100    \t/3400, val loss:41.37345504760742\n",
      "training on iteration:2101    \t/3400, val loss:40.9367790222168\n",
      "training on iteration:2102    \t/3400, val loss:40.56841278076172\n",
      "training on iteration:2103    \t/3400, val loss:40.65149688720703\n",
      "training on iteration:2104    \t/3400, val loss:41.042564392089844\n",
      "training on iteration:2105    \t/3400, val loss:41.53046798706055\n",
      "training on iteration:2106    \t/3400, val loss:41.46419143676758\n",
      "training on iteration:2107    \t/3400, val loss:40.783203125\n",
      "training on iteration:2108    \t/3400, val loss:40.54753494262695\n",
      "training on iteration:2109    \t/3400, val loss:40.55778884887695\n",
      "training on iteration:2110    \t/3400, val loss:40.93463134765625\n",
      "training on iteration:2111    \t/3400, val loss:41.48185348510742\n",
      "training on iteration:2112    \t/3400, val loss:41.56243896484375\n",
      "training on iteration:2113    \t/3400, val loss:40.93960189819336\n",
      "training on iteration:2114    \t/3400, val loss:40.69900894165039\n",
      "training on iteration:2115    \t/3400, val loss:40.57686233520508\n",
      "training on iteration:2116    \t/3400, val loss:40.892425537109375\n",
      "training on iteration:2117    \t/3400, val loss:41.28942108154297\n",
      "training on iteration:2118    \t/3400, val loss:40.94255828857422\n",
      "training on iteration:2119    \t/3400, val loss:40.57281494140625\n",
      "training on iteration:2120    \t/3400, val loss:40.63920211791992\n",
      "training on iteration:2121    \t/3400, val loss:41.01541519165039\n",
      "training on iteration:2122    \t/3400, val loss:41.52524948120117\n",
      "training on iteration:2123    \t/3400, val loss:41.48627471923828\n",
      "training on iteration:2124    \t/3400, val loss:40.75458526611328\n",
      "training on iteration:2125    \t/3400, val loss:40.49773406982422\n",
      "training on iteration:2126    \t/3400, val loss:40.49655532836914\n",
      "training on iteration:2127    \t/3400, val loss:40.830047607421875\n",
      "training on iteration:2128    \t/3400, val loss:41.47854995727539\n",
      "training on iteration:2129    \t/3400, val loss:41.643436431884766\n",
      "training on iteration:2130    \t/3400, val loss:40.95821762084961\n",
      "training on iteration:2131    \t/3400, val loss:40.676692962646484\n",
      "training on iteration:2132    \t/3400, val loss:40.54415512084961\n",
      "training on iteration:2133    \t/3400, val loss:40.8924560546875\n",
      "training on iteration:2134    \t/3400, val loss:41.343719482421875\n",
      "training on iteration:2135    \t/3400, val loss:40.945037841796875\n",
      "training on iteration:2136    \t/3400, val loss:40.554847717285156\n",
      "training on iteration:2137    \t/3400, val loss:40.64033889770508\n",
      "training on iteration:2138    \t/3400, val loss:41.036170959472656\n",
      "training on iteration:2139    \t/3400, val loss:41.51028060913086\n",
      "training on iteration:2140    \t/3400, val loss:41.42650604248047\n",
      "training on iteration:2141    \t/3400, val loss:40.707130432128906\n",
      "training on iteration:2142    \t/3400, val loss:40.47866439819336\n",
      "training on iteration:2143    \t/3400, val loss:40.49808883666992\n",
      "training on iteration:2144    \t/3400, val loss:40.891395568847656\n",
      "training on iteration:2145    \t/3400, val loss:41.551300048828125\n",
      "training on iteration:2146    \t/3400, val loss:41.629329681396484\n",
      "training on iteration:2147    \t/3400, val loss:40.947906494140625\n",
      "training on iteration:2148    \t/3400, val loss:40.65123748779297\n",
      "training on iteration:2149    \t/3400, val loss:40.520362854003906\n",
      "training on iteration:2150    \t/3400, val loss:40.91420364379883\n",
      "training on iteration:2151    \t/3400, val loss:41.33877944946289\n",
      "training on iteration:2152    \t/3400, val loss:40.89274978637695\n",
      "training on iteration:2153    \t/3400, val loss:40.51046371459961\n",
      "training on iteration:2154    \t/3400, val loss:40.62204360961914\n",
      "training on iteration:2155    \t/3400, val loss:41.07622146606445\n",
      "training on iteration:2156    \t/3400, val loss:41.56467056274414\n",
      "training on iteration:2157    \t/3400, val loss:41.387996673583984\n",
      "training on iteration:2158    \t/3400, val loss:40.67014694213867\n",
      "training on iteration:2159    \t/3400, val loss:40.46050262451172\n",
      "training on iteration:2160    \t/3400, val loss:40.493595123291016\n",
      "training on iteration:2161    \t/3400, val loss:40.881832122802734\n",
      "training on iteration:2162    \t/3400, val loss:41.49519729614258\n",
      "training on iteration:2163    \t/3400, val loss:41.54404067993164\n",
      "training on iteration:2164    \t/3400, val loss:40.9388427734375\n",
      "training on iteration:2165    \t/3400, val loss:40.64604568481445\n",
      "training on iteration:2166    \t/3400, val loss:40.51100540161133\n",
      "training on iteration:2167    \t/3400, val loss:40.89303207397461\n",
      "training on iteration:2168    \t/3400, val loss:41.28299331665039\n",
      "training on iteration:2169    \t/3400, val loss:40.84663772583008\n",
      "training on iteration:2170    \t/3400, val loss:40.51162338256836\n",
      "training on iteration:2171    \t/3400, val loss:40.62863540649414\n",
      "training on iteration:2172    \t/3400, val loss:41.02113723754883\n",
      "training on iteration:2173    \t/3400, val loss:41.46735763549805\n",
      "training on iteration:2174    \t/3400, val loss:41.32262420654297\n",
      "training on iteration:2175    \t/3400, val loss:40.65572738647461\n",
      "training on iteration:2176    \t/3400, val loss:40.455379486083984\n",
      "training on iteration:2177    \t/3400, val loss:40.49482345581055\n",
      "training on iteration:2178    \t/3400, val loss:40.8843994140625\n",
      "training on iteration:2179    \t/3400, val loss:41.478878021240234\n",
      "training on iteration:2180    \t/3400, val loss:41.52212142944336\n",
      "training on iteration:2181    \t/3400, val loss:40.932525634765625\n",
      "training on iteration:2182    \t/3400, val loss:40.64927291870117\n",
      "training on iteration:2183    \t/3400, val loss:40.51460647583008\n",
      "training on iteration:2184    \t/3400, val loss:40.916465759277344\n",
      "training on iteration:2185    \t/3400, val loss:41.27240753173828\n",
      "training on iteration:2186    \t/3400, val loss:40.7734375\n",
      "training on iteration:2187    \t/3400, val loss:40.464195251464844\n",
      "training on iteration:2188    \t/3400, val loss:40.6171989440918\n",
      "training on iteration:2189    \t/3400, val loss:41.07373046875\n",
      "training on iteration:2190    \t/3400, val loss:41.52045822143555\n",
      "training on iteration:2191    \t/3400, val loss:41.2549934387207\n",
      "training on iteration:2192    \t/3400, val loss:40.57876205444336\n",
      "training on iteration:2193    \t/3400, val loss:40.42202377319336\n",
      "training on iteration:2194    \t/3400, val loss:40.508270263671875\n",
      "training on iteration:2195    \t/3400, val loss:40.96111297607422\n",
      "training on iteration:2196    \t/3400, val loss:41.54287338256836\n",
      "training on iteration:2197    \t/3400, val loss:41.493526458740234\n",
      "training on iteration:2198    \t/3400, val loss:40.90427017211914\n",
      "training on iteration:2199    \t/3400, val loss:40.64641571044922\n",
      "training on iteration:2200    \t/3400, val loss:40.51874542236328\n",
      "training on iteration:2201    \t/3400, val loss:40.9175910949707\n",
      "training on iteration:2202    \t/3400, val loss:41.249267578125\n",
      "training on iteration:2203    \t/3400, val loss:40.73839569091797\n",
      "training on iteration:2204    \t/3400, val loss:40.448665618896484\n",
      "training on iteration:2205    \t/3400, val loss:40.60243606567383\n",
      "training on iteration:2206    \t/3400, val loss:41.06511688232422\n",
      "training on iteration:2207    \t/3400, val loss:41.52667999267578\n",
      "training on iteration:2208    \t/3400, val loss:41.24555206298828\n",
      "training on iteration:2209    \t/3400, val loss:40.548851013183594\n",
      "training on iteration:2210    \t/3400, val loss:40.40428924560547\n",
      "training on iteration:2211    \t/3400, val loss:40.48668670654297\n",
      "training on iteration:2212    \t/3400, val loss:40.88944625854492\n",
      "training on iteration:2213    \t/3400, val loss:41.458255767822266\n",
      "training on iteration:2214    \t/3400, val loss:41.3958625793457\n",
      "training on iteration:2215    \t/3400, val loss:40.837032318115234\n",
      "training on iteration:2216    \t/3400, val loss:40.58889389038086\n",
      "training on iteration:2217    \t/3400, val loss:40.49441146850586\n",
      "training on iteration:2218    \t/3400, val loss:40.86564254760742\n",
      "training on iteration:2219    \t/3400, val loss:41.14163589477539\n",
      "training on iteration:2220    \t/3400, val loss:40.71201705932617\n",
      "training on iteration:2221    \t/3400, val loss:40.46558380126953\n",
      "training on iteration:2222    \t/3400, val loss:40.639034271240234\n",
      "training on iteration:2223    \t/3400, val loss:41.03954315185547\n",
      "training on iteration:2224    \t/3400, val loss:41.38035202026367\n",
      "training on iteration:2225    \t/3400, val loss:41.17490768432617\n",
      "training on iteration:2226    \t/3400, val loss:40.55213165283203\n",
      "training on iteration:2227    \t/3400, val loss:40.40758514404297\n",
      "training on iteration:2228    \t/3400, val loss:40.4837646484375\n",
      "training on iteration:2229    \t/3400, val loss:40.87272262573242\n",
      "training on iteration:2230    \t/3400, val loss:41.39533233642578\n",
      "training on iteration:2231    \t/3400, val loss:41.34772491455078\n",
      "training on iteration:2232    \t/3400, val loss:40.804622650146484\n",
      "training on iteration:2233    \t/3400, val loss:40.57259750366211\n",
      "training on iteration:2234    \t/3400, val loss:40.49365997314453\n",
      "training on iteration:2235    \t/3400, val loss:40.86135482788086\n",
      "training on iteration:2236    \t/3400, val loss:41.11547088623047\n",
      "training on iteration:2237    \t/3400, val loss:40.68633270263672\n",
      "training on iteration:2238    \t/3400, val loss:40.45079803466797\n",
      "training on iteration:2239    \t/3400, val loss:40.62405014038086\n",
      "training on iteration:2240    \t/3400, val loss:41.01526641845703\n",
      "training on iteration:2241    \t/3400, val loss:41.357913970947266\n",
      "training on iteration:2242    \t/3400, val loss:41.161678314208984\n",
      "training on iteration:2243    \t/3400, val loss:40.545448303222656\n",
      "training on iteration:2244    \t/3400, val loss:40.39933395385742\n",
      "training on iteration:2245    \t/3400, val loss:40.48081588745117\n",
      "training on iteration:2246    \t/3400, val loss:40.85846710205078\n",
      "training on iteration:2247    \t/3400, val loss:41.37932205200195\n",
      "training on iteration:2248    \t/3400, val loss:41.29975128173828\n",
      "training on iteration:2249    \t/3400, val loss:40.75072479248047\n",
      "training on iteration:2250    \t/3400, val loss:40.53977584838867\n",
      "training on iteration:2251    \t/3400, val loss:40.5289421081543\n",
      "training on iteration:2252    \t/3400, val loss:40.96181869506836\n",
      "training on iteration:2253    \t/3400, val loss:41.16292190551758\n",
      "training on iteration:2254    \t/3400, val loss:40.61202621459961\n",
      "training on iteration:2255    \t/3400, val loss:40.38357925415039\n",
      "training on iteration:2256    \t/3400, val loss:40.58406448364258\n",
      "training on iteration:2257    \t/3400, val loss:41.098472595214844\n",
      "training on iteration:2258    \t/3400, val loss:41.59237289428711\n",
      "training on iteration:2259    \t/3400, val loss:41.21485137939453\n",
      "training on iteration:2260    \t/3400, val loss:40.49313735961914\n",
      "training on iteration:2261    \t/3400, val loss:40.37163543701172\n",
      "training on iteration:2262    \t/3400, val loss:40.47279739379883\n",
      "training on iteration:2263    \t/3400, val loss:40.9228515625\n",
      "training on iteration:2264    \t/3400, val loss:41.4870719909668\n",
      "training on iteration:2265    \t/3400, val loss:41.311946868896484\n",
      "training on iteration:2266    \t/3400, val loss:40.65479278564453\n",
      "training on iteration:2267    \t/3400, val loss:40.43254470825195\n",
      "training on iteration:2268    \t/3400, val loss:40.44762420654297\n",
      "training on iteration:2269    \t/3400, val loss:40.90812301635742\n",
      "training on iteration:2270    \t/3400, val loss:41.19709014892578\n",
      "training on iteration:2271    \t/3400, val loss:40.67724609375\n",
      "training on iteration:2272    \t/3400, val loss:40.39328384399414\n",
      "training on iteration:2273    \t/3400, val loss:40.51244354248047\n",
      "training on iteration:2274    \t/3400, val loss:40.95951461791992\n",
      "training on iteration:2275    \t/3400, val loss:41.52826690673828\n",
      "training on iteration:2276    \t/3400, val loss:41.231319427490234\n",
      "training on iteration:2277    \t/3400, val loss:40.51739501953125\n",
      "training on iteration:2278    \t/3400, val loss:40.375858306884766\n",
      "training on iteration:2279    \t/3400, val loss:40.4683837890625\n",
      "training on iteration:2280    \t/3400, val loss:40.98287582397461\n",
      "training on iteration:2281    \t/3400, val loss:41.57566452026367\n",
      "training on iteration:2282    \t/3400, val loss:41.27870559692383\n",
      "training on iteration:2283    \t/3400, val loss:40.527828216552734\n",
      "training on iteration:2284    \t/3400, val loss:40.3492431640625\n",
      "training on iteration:2285    \t/3400, val loss:40.419193267822266\n",
      "training on iteration:2286    \t/3400, val loss:41.011138916015625\n",
      "training on iteration:2287    \t/3400, val loss:41.33640670776367\n",
      "training on iteration:2288    \t/3400, val loss:40.675559997558594\n",
      "training on iteration:2289    \t/3400, val loss:40.33341598510742\n",
      "training on iteration:2290    \t/3400, val loss:40.426509857177734\n",
      "training on iteration:2291    \t/3400, val loss:40.920772552490234\n",
      "training on iteration:2292    \t/3400, val loss:41.5809326171875\n",
      "training on iteration:2293    \t/3400, val loss:41.31151580810547\n",
      "training on iteration:2294    \t/3400, val loss:40.526058197021484\n",
      "training on iteration:2295    \t/3400, val loss:40.360633850097656\n",
      "training on iteration:2296    \t/3400, val loss:40.41754150390625\n",
      "training on iteration:2297    \t/3400, val loss:40.87064743041992\n",
      "training on iteration:2298    \t/3400, val loss:41.57441329956055\n",
      "training on iteration:2299    \t/3400, val loss:41.347869873046875\n",
      "training on iteration:2300    \t/3400, val loss:40.52223587036133\n",
      "training on iteration:2301    \t/3400, val loss:40.31053924560547\n",
      "training on iteration:2302    \t/3400, val loss:40.34806442260742\n",
      "training on iteration:2303    \t/3400, val loss:40.95561599731445\n",
      "training on iteration:2304    \t/3400, val loss:41.42792892456055\n",
      "training on iteration:2305    \t/3400, val loss:40.78635025024414\n",
      "training on iteration:2306    \t/3400, val loss:40.343048095703125\n",
      "training on iteration:2307    \t/3400, val loss:40.376590728759766\n",
      "training on iteration:2308    \t/3400, val loss:40.8231315612793\n",
      "training on iteration:2309    \t/3400, val loss:41.650177001953125\n",
      "training on iteration:2310    \t/3400, val loss:41.435916900634766\n",
      "training on iteration:2311    \t/3400, val loss:40.51859664916992\n",
      "training on iteration:2312    \t/3400, val loss:40.334312438964844\n",
      "training on iteration:2313    \t/3400, val loss:40.38753890991211\n",
      "training on iteration:2314    \t/3400, val loss:40.867034912109375\n",
      "training on iteration:2315    \t/3400, val loss:41.69110870361328\n",
      "training on iteration:2316    \t/3400, val loss:41.49053192138672\n",
      "training on iteration:2317    \t/3400, val loss:40.58548355102539\n",
      "training on iteration:2318    \t/3400, val loss:40.319435119628906\n",
      "training on iteration:2319    \t/3400, val loss:40.32680130004883\n",
      "training on iteration:2320    \t/3400, val loss:40.87497329711914\n",
      "training on iteration:2321    \t/3400, val loss:41.3340950012207\n",
      "training on iteration:2322    \t/3400, val loss:40.74056625366211\n",
      "training on iteration:2323    \t/3400, val loss:40.33144760131836\n",
      "training on iteration:2324    \t/3400, val loss:40.401161193847656\n",
      "training on iteration:2325    \t/3400, val loss:40.85023880004883\n",
      "training on iteration:2326    \t/3400, val loss:41.578731536865234\n",
      "training on iteration:2327    \t/3400, val loss:41.37333297729492\n",
      "training on iteration:2328    \t/3400, val loss:40.55916976928711\n",
      "training on iteration:2329    \t/3400, val loss:40.35354995727539\n",
      "training on iteration:2330    \t/3400, val loss:40.38819122314453\n",
      "training on iteration:2331    \t/3400, val loss:40.887149810791016\n",
      "training on iteration:2332    \t/3400, val loss:41.68313217163086\n",
      "training on iteration:2333    \t/3400, val loss:41.4339599609375\n",
      "training on iteration:2334    \t/3400, val loss:40.509647369384766\n",
      "training on iteration:2335    \t/3400, val loss:40.26723098754883\n",
      "training on iteration:2336    \t/3400, val loss:40.30498123168945\n",
      "training on iteration:2337    \t/3400, val loss:40.971866607666016\n",
      "training on iteration:2338    \t/3400, val loss:41.47104263305664\n",
      "training on iteration:2339    \t/3400, val loss:40.75992202758789\n",
      "training on iteration:2340    \t/3400, val loss:40.29288101196289\n",
      "training on iteration:2341    \t/3400, val loss:40.3405647277832\n",
      "training on iteration:2342    \t/3400, val loss:40.80081558227539\n",
      "training on iteration:2343    \t/3400, val loss:41.658660888671875\n",
      "training on iteration:2344    \t/3400, val loss:41.44797134399414\n",
      "training on iteration:2345    \t/3400, val loss:40.521785736083984\n",
      "training on iteration:2346    \t/3400, val loss:40.333763122558594\n",
      "training on iteration:2347    \t/3400, val loss:40.38156509399414\n",
      "training on iteration:2348    \t/3400, val loss:40.83073806762695\n",
      "training on iteration:2349    \t/3400, val loss:41.636474609375\n",
      "training on iteration:2350    \t/3400, val loss:41.48569107055664\n",
      "training on iteration:2351    \t/3400, val loss:40.511837005615234\n",
      "training on iteration:2352    \t/3400, val loss:40.23843002319336\n",
      "training on iteration:2353    \t/3400, val loss:40.25130081176758\n",
      "training on iteration:2354    \t/3400, val loss:40.80116653442383\n",
      "training on iteration:2355    \t/3400, val loss:41.358367919921875\n",
      "training on iteration:2356    \t/3400, val loss:40.783538818359375\n",
      "training on iteration:2357    \t/3400, val loss:40.31721878051758\n",
      "training on iteration:2358    \t/3400, val loss:40.32365036010742\n",
      "training on iteration:2359    \t/3400, val loss:40.684051513671875\n",
      "training on iteration:2360    \t/3400, val loss:41.42534255981445\n",
      "training on iteration:2361    \t/3400, val loss:41.43547821044922\n",
      "training on iteration:2362    \t/3400, val loss:40.618534088134766\n",
      "training on iteration:2363    \t/3400, val loss:40.369869232177734\n",
      "training on iteration:2364    \t/3400, val loss:40.35573959350586\n",
      "training on iteration:2365    \t/3400, val loss:40.733619689941406\n",
      "training on iteration:2366    \t/3400, val loss:41.522796630859375\n",
      "training on iteration:2367    \t/3400, val loss:41.43604278564453\n",
      "training on iteration:2368    \t/3400, val loss:40.527164459228516\n",
      "training on iteration:2369    \t/3400, val loss:40.271270751953125\n",
      "training on iteration:2370    \t/3400, val loss:40.259613037109375\n",
      "training on iteration:2371    \t/3400, val loss:40.83934783935547\n",
      "training on iteration:2372    \t/3400, val loss:41.37281036376953\n",
      "training on iteration:2373    \t/3400, val loss:40.75954055786133\n",
      "training on iteration:2374    \t/3400, val loss:40.26777267456055\n",
      "training on iteration:2375    \t/3400, val loss:40.31969451904297\n",
      "training on iteration:2376    \t/3400, val loss:40.772159576416016\n",
      "training on iteration:2377    \t/3400, val loss:41.57958984375\n",
      "training on iteration:2378    \t/3400, val loss:41.39423751831055\n",
      "training on iteration:2379    \t/3400, val loss:40.51076126098633\n",
      "training on iteration:2380    \t/3400, val loss:40.32038497924805\n",
      "training on iteration:2381    \t/3400, val loss:40.35646438598633\n",
      "training on iteration:2382    \t/3400, val loss:40.8089599609375\n",
      "training on iteration:2383    \t/3400, val loss:41.63329315185547\n",
      "training on iteration:2384    \t/3400, val loss:41.434165954589844\n",
      "training on iteration:2385    \t/3400, val loss:40.485748291015625\n",
      "training on iteration:2386    \t/3400, val loss:40.208251953125\n",
      "training on iteration:2387    \t/3400, val loss:40.225189208984375\n",
      "training on iteration:2388    \t/3400, val loss:40.78915023803711\n",
      "training on iteration:2389    \t/3400, val loss:41.33396530151367\n",
      "training on iteration:2390    \t/3400, val loss:40.75289535522461\n",
      "training on iteration:2391    \t/3400, val loss:40.27796173095703\n",
      "training on iteration:2392    \t/3400, val loss:40.28250503540039\n",
      "training on iteration:2393    \t/3400, val loss:40.643306732177734\n",
      "training on iteration:2394    \t/3400, val loss:41.41305160522461\n",
      "training on iteration:2395    \t/3400, val loss:41.41498565673828\n",
      "training on iteration:2396    \t/3400, val loss:40.554046630859375\n",
      "training on iteration:2397    \t/3400, val loss:40.31647872924805\n",
      "training on iteration:2398    \t/3400, val loss:40.32119369506836\n",
      "training on iteration:2399    \t/3400, val loss:40.754520416259766\n",
      "training on iteration:2400    \t/3400, val loss:41.572208404541016\n",
      "training on iteration:2401    \t/3400, val loss:41.40635299682617\n",
      "training on iteration:2402    \t/3400, val loss:40.43538284301758\n",
      "training on iteration:2403    \t/3400, val loss:40.180702209472656\n",
      "training on iteration:2404    \t/3400, val loss:40.21476364135742\n",
      "training on iteration:2405    \t/3400, val loss:40.81072998046875\n",
      "training on iteration:2406    \t/3400, val loss:41.346683502197266\n",
      "training on iteration:2407    \t/3400, val loss:40.69443893432617\n",
      "training on iteration:2408    \t/3400, val loss:40.25027847290039\n",
      "training on iteration:2409    \t/3400, val loss:40.28345489501953\n",
      "training on iteration:2410    \t/3400, val loss:40.7312126159668\n",
      "training on iteration:2411    \t/3400, val loss:41.55729293823242\n",
      "training on iteration:2412    \t/3400, val loss:41.35818099975586\n",
      "training on iteration:2413    \t/3400, val loss:40.480899810791016\n",
      "training on iteration:2414    \t/3400, val loss:40.29842758178711\n",
      "training on iteration:2415    \t/3400, val loss:40.33150100708008\n",
      "training on iteration:2416    \t/3400, val loss:40.79165267944336\n",
      "training on iteration:2417    \t/3400, val loss:41.56770706176758\n",
      "training on iteration:2418    \t/3400, val loss:41.359619140625\n",
      "training on iteration:2419    \t/3400, val loss:40.4239501953125\n",
      "training on iteration:2420    \t/3400, val loss:40.18675994873047\n",
      "training on iteration:2421    \t/3400, val loss:40.21158218383789\n",
      "training on iteration:2422    \t/3400, val loss:40.78305435180664\n",
      "training on iteration:2423    \t/3400, val loss:41.33549118041992\n",
      "training on iteration:2424    \t/3400, val loss:40.73244857788086\n",
      "training on iteration:2425    \t/3400, val loss:40.252933502197266\n",
      "training on iteration:2426    \t/3400, val loss:40.27732467651367\n",
      "training on iteration:2427    \t/3400, val loss:40.669944763183594\n",
      "training on iteration:2428    \t/3400, val loss:41.46744155883789\n",
      "training on iteration:2429    \t/3400, val loss:41.37998580932617\n",
      "training on iteration:2430    \t/3400, val loss:40.50023651123047\n",
      "training on iteration:2431    \t/3400, val loss:40.29354476928711\n",
      "training on iteration:2432    \t/3400, val loss:40.3123664855957\n",
      "training on iteration:2433    \t/3400, val loss:40.79254913330078\n",
      "training on iteration:2434    \t/3400, val loss:41.58791732788086\n",
      "training on iteration:2435    \t/3400, val loss:41.34845733642578\n",
      "training on iteration:2436    \t/3400, val loss:40.38875198364258\n",
      "training on iteration:2437    \t/3400, val loss:40.148582458496094\n",
      "training on iteration:2438    \t/3400, val loss:40.19610595703125\n",
      "training on iteration:2439    \t/3400, val loss:40.83771514892578\n",
      "training on iteration:2440    \t/3400, val loss:41.38842010498047\n",
      "training on iteration:2441    \t/3400, val loss:40.68197250366211\n",
      "training on iteration:2442    \t/3400, val loss:40.20473861694336\n",
      "training on iteration:2443    \t/3400, val loss:40.25752639770508\n",
      "training on iteration:2444    \t/3400, val loss:40.708984375\n",
      "training on iteration:2445    \t/3400, val loss:41.541141510009766\n",
      "training on iteration:2446    \t/3400, val loss:41.35335159301758\n",
      "training on iteration:2447    \t/3400, val loss:40.44987106323242\n",
      "training on iteration:2448    \t/3400, val loss:40.26891326904297\n",
      "training on iteration:2449    \t/3400, val loss:40.31058120727539\n",
      "training on iteration:2450    \t/3400, val loss:40.808414459228516\n",
      "training on iteration:2451    \t/3400, val loss:41.64202880859375\n",
      "training on iteration:2452    \t/3400, val loss:41.36906051635742\n",
      "training on iteration:2453    \t/3400, val loss:40.362548828125\n",
      "training on iteration:2454    \t/3400, val loss:40.118438720703125\n",
      "training on iteration:2455    \t/3400, val loss:40.16656494140625\n",
      "training on iteration:2456    \t/3400, val loss:40.80194091796875\n",
      "training on iteration:2457    \t/3400, val loss:41.37017059326172\n",
      "training on iteration:2458    \t/3400, val loss:40.686058044433594\n",
      "training on iteration:2459    \t/3400, val loss:40.19796371459961\n",
      "training on iteration:2460    \t/3400, val loss:40.21300506591797\n",
      "training on iteration:2461    \t/3400, val loss:40.62636184692383\n",
      "training on iteration:2462    \t/3400, val loss:41.48323440551758\n",
      "training on iteration:2463    \t/3400, val loss:41.39830780029297\n",
      "training on iteration:2464    \t/3400, val loss:40.47452163696289\n",
      "training on iteration:2465    \t/3400, val loss:40.27481460571289\n",
      "training on iteration:2466    \t/3400, val loss:40.285743713378906\n",
      "training on iteration:2467    \t/3400, val loss:40.74289321899414\n",
      "training on iteration:2468    \t/3400, val loss:41.54058837890625\n",
      "training on iteration:2469    \t/3400, val loss:41.305458068847656\n",
      "training on iteration:2470    \t/3400, val loss:40.33989334106445\n",
      "training on iteration:2471    \t/3400, val loss:40.12214279174805\n",
      "training on iteration:2472    \t/3400, val loss:40.16648483276367\n",
      "training on iteration:2473    \t/3400, val loss:40.77228546142578\n",
      "training on iteration:2474    \t/3400, val loss:41.30585861206055\n",
      "training on iteration:2475    \t/3400, val loss:40.65946578979492\n",
      "training on iteration:2476    \t/3400, val loss:40.18054962158203\n",
      "training on iteration:2477    \t/3400, val loss:40.22031784057617\n",
      "training on iteration:2478    \t/3400, val loss:40.66030502319336\n",
      "training on iteration:2479    \t/3400, val loss:41.516910552978516\n",
      "training on iteration:2480    \t/3400, val loss:41.35745620727539\n",
      "training on iteration:2481    \t/3400, val loss:40.45938491821289\n",
      "training on iteration:2482    \t/3400, val loss:40.26178741455078\n",
      "training on iteration:2483    \t/3400, val loss:40.26594924926758\n",
      "training on iteration:2484    \t/3400, val loss:40.70122528076172\n",
      "training on iteration:2485    \t/3400, val loss:41.4993896484375\n",
      "training on iteration:2486    \t/3400, val loss:41.29677963256836\n",
      "training on iteration:2487    \t/3400, val loss:40.337642669677734\n",
      "training on iteration:2488    \t/3400, val loss:40.109405517578125\n",
      "training on iteration:2489    \t/3400, val loss:40.15571212768555\n",
      "training on iteration:2490    \t/3400, val loss:40.75252914428711\n",
      "training on iteration:2491    \t/3400, val loss:41.31063461303711\n",
      "training on iteration:2492    \t/3400, val loss:40.62651824951172\n",
      "training on iteration:2493    \t/3400, val loss:40.1674919128418\n",
      "training on iteration:2494    \t/3400, val loss:40.22146987915039\n",
      "training on iteration:2495    \t/3400, val loss:40.69903564453125\n",
      "training on iteration:2496    \t/3400, val loss:41.506553649902344\n",
      "training on iteration:2497    \t/3400, val loss:41.29219436645508\n",
      "training on iteration:2498    \t/3400, val loss:40.42447280883789\n",
      "training on iteration:2499    \t/3400, val loss:40.25580978393555\n",
      "training on iteration:2500    \t/3400, val loss:40.28830337524414\n",
      "training on iteration:2501    \t/3400, val loss:40.73575973510742\n",
      "training on iteration:2502    \t/3400, val loss:41.46126174926758\n",
      "training on iteration:2503    \t/3400, val loss:41.195701599121094\n",
      "training on iteration:2504    \t/3400, val loss:40.31852340698242\n",
      "training on iteration:2505    \t/3400, val loss:40.090087890625\n",
      "training on iteration:2506    \t/3400, val loss:40.11792755126953\n",
      "training on iteration:2507    \t/3400, val loss:40.67606735229492\n",
      "training on iteration:2508    \t/3400, val loss:41.2346305847168\n",
      "training on iteration:2509    \t/3400, val loss:40.63957977294922\n",
      "training on iteration:2510    \t/3400, val loss:40.18106460571289\n",
      "training on iteration:2511    \t/3400, val loss:40.218509674072266\n",
      "training on iteration:2512    \t/3400, val loss:40.6103401184082\n",
      "training on iteration:2513    \t/3400, val loss:41.38661575317383\n",
      "training on iteration:2514    \t/3400, val loss:41.32741165161133\n",
      "training on iteration:2515    \t/3400, val loss:40.47949981689453\n",
      "training on iteration:2516    \t/3400, val loss:40.25970458984375\n",
      "training on iteration:2517    \t/3400, val loss:40.2545051574707\n",
      "training on iteration:2518    \t/3400, val loss:40.660400390625\n",
      "training on iteration:2519    \t/3400, val loss:41.38549041748047\n",
      "training on iteration:2520    \t/3400, val loss:41.190608978271484\n",
      "training on iteration:2521    \t/3400, val loss:40.28719711303711\n",
      "training on iteration:2522    \t/3400, val loss:40.06998825073242\n",
      "training on iteration:2523    \t/3400, val loss:40.115928649902344\n",
      "training on iteration:2524    \t/3400, val loss:40.69622802734375\n",
      "training on iteration:2525    \t/3400, val loss:41.20086669921875\n",
      "training on iteration:2526    \t/3400, val loss:40.573326110839844\n",
      "training on iteration:2527    \t/3400, val loss:40.1585693359375\n",
      "training on iteration:2528    \t/3400, val loss:40.22519302368164\n",
      "training on iteration:2529    \t/3400, val loss:40.705841064453125\n",
      "training on iteration:2530    \t/3400, val loss:41.46323776245117\n",
      "training on iteration:2531    \t/3400, val loss:41.1987419128418\n",
      "training on iteration:2532    \t/3400, val loss:40.37656021118164\n",
      "training on iteration:2533    \t/3400, val loss:40.236717224121094\n",
      "training on iteration:2534    \t/3400, val loss:40.289794921875\n",
      "training on iteration:2535    \t/3400, val loss:40.76068878173828\n",
      "training on iteration:2536    \t/3400, val loss:41.36956787109375\n",
      "training on iteration:2537    \t/3400, val loss:41.02379608154297\n",
      "training on iteration:2538    \t/3400, val loss:40.23174285888672\n",
      "training on iteration:2539    \t/3400, val loss:40.067195892333984\n",
      "training on iteration:2540    \t/3400, val loss:40.14296340942383\n",
      "training on iteration:2541    \t/3400, val loss:40.77884292602539\n",
      "training on iteration:2542    \t/3400, val loss:41.21866226196289\n",
      "training on iteration:2543    \t/3400, val loss:40.53837966918945\n",
      "training on iteration:2544    \t/3400, val loss:40.15293502807617\n",
      "training on iteration:2545    \t/3400, val loss:40.233402252197266\n",
      "training on iteration:2546    \t/3400, val loss:40.69646453857422\n",
      "training on iteration:2547    \t/3400, val loss:41.41189956665039\n",
      "training on iteration:2548    \t/3400, val loss:41.15763473510742\n",
      "training on iteration:2549    \t/3400, val loss:40.394073486328125\n",
      "training on iteration:2550    \t/3400, val loss:40.234832763671875\n",
      "training on iteration:2551    \t/3400, val loss:40.26917266845703\n",
      "training on iteration:2552    \t/3400, val loss:40.69603729248047\n",
      "training on iteration:2553    \t/3400, val loss:41.32672882080078\n",
      "training on iteration:2554    \t/3400, val loss:41.01970672607422\n",
      "training on iteration:2555    \t/3400, val loss:40.21614456176758\n",
      "training on iteration:2556    \t/3400, val loss:40.04381561279297\n",
      "training on iteration:2557    \t/3400, val loss:40.12274169921875\n",
      "training on iteration:2558    \t/3400, val loss:40.74653244018555\n",
      "training on iteration:2559    \t/3400, val loss:41.19779586791992\n",
      "training on iteration:2560    \t/3400, val loss:40.5408935546875\n",
      "training on iteration:2561    \t/3400, val loss:40.16619873046875\n",
      "training on iteration:2562    \t/3400, val loss:40.21489715576172\n",
      "training on iteration:2563    \t/3400, val loss:40.67367172241211\n",
      "training on iteration:2564    \t/3400, val loss:41.4305305480957\n",
      "training on iteration:2565    \t/3400, val loss:41.174991607666016\n",
      "training on iteration:2566    \t/3400, val loss:40.368255615234375\n",
      "training on iteration:2567    \t/3400, val loss:40.217159271240234\n",
      "training on iteration:2568    \t/3400, val loss:40.24605941772461\n",
      "training on iteration:2569    \t/3400, val loss:40.67737579345703\n",
      "training on iteration:2570    \t/3400, val loss:41.25466537475586\n",
      "training on iteration:2571    \t/3400, val loss:40.92692184448242\n",
      "training on iteration:2572    \t/3400, val loss:40.1685791015625\n",
      "training on iteration:2573    \t/3400, val loss:40.02930450439453\n",
      "training on iteration:2574    \t/3400, val loss:40.12683868408203\n",
      "training on iteration:2575    \t/3400, val loss:40.74238967895508\n",
      "training on iteration:2576    \t/3400, val loss:41.16571807861328\n",
      "training on iteration:2577    \t/3400, val loss:40.480533599853516\n",
      "training on iteration:2578    \t/3400, val loss:40.1226806640625\n",
      "training on iteration:2579    \t/3400, val loss:40.23029708862305\n",
      "training on iteration:2580    \t/3400, val loss:40.75423049926758\n",
      "training on iteration:2581    \t/3400, val loss:41.443660736083984\n",
      "training on iteration:2582    \t/3400, val loss:41.07651138305664\n",
      "training on iteration:2583    \t/3400, val loss:40.32763671875\n",
      "training on iteration:2584    \t/3400, val loss:40.21295928955078\n",
      "training on iteration:2585    \t/3400, val loss:40.28205490112305\n",
      "training on iteration:2586    \t/3400, val loss:40.74039840698242\n",
      "training on iteration:2587    \t/3400, val loss:41.22175216674805\n",
      "training on iteration:2588    \t/3400, val loss:40.84258270263672\n",
      "training on iteration:2589    \t/3400, val loss:40.15895462036133\n",
      "training on iteration:2590    \t/3400, val loss:40.02033233642578\n",
      "training on iteration:2591    \t/3400, val loss:40.13761520385742\n",
      "training on iteration:2592    \t/3400, val loss:40.755958557128906\n",
      "training on iteration:2593    \t/3400, val loss:41.05704116821289\n",
      "training on iteration:2594    \t/3400, val loss:40.37415313720703\n",
      "training on iteration:2595    \t/3400, val loss:40.116737365722656\n",
      "training on iteration:2596    \t/3400, val loss:40.21856689453125\n",
      "training on iteration:2597    \t/3400, val loss:40.71866226196289\n",
      "training on iteration:2598    \t/3400, val loss:41.32363510131836\n",
      "training on iteration:2599    \t/3400, val loss:40.99506378173828\n",
      "training on iteration:2600    \t/3400, val loss:40.322181701660156\n",
      "training on iteration:2601    \t/3400, val loss:40.2331428527832\n",
      "training on iteration:2602    \t/3400, val loss:40.30305099487305\n",
      "training on iteration:2603    \t/3400, val loss:40.73624801635742\n",
      "training on iteration:2604    \t/3400, val loss:41.12255859375\n",
      "training on iteration:2605    \t/3400, val loss:40.67731857299805\n",
      "training on iteration:2606    \t/3400, val loss:40.0799674987793\n",
      "training on iteration:2607    \t/3400, val loss:40.0014533996582\n",
      "training on iteration:2608    \t/3400, val loss:40.15144729614258\n",
      "training on iteration:2609    \t/3400, val loss:40.77149200439453\n",
      "training on iteration:2610    \t/3400, val loss:41.05908203125\n",
      "training on iteration:2611    \t/3400, val loss:40.35651397705078\n",
      "training on iteration:2612    \t/3400, val loss:40.089664459228516\n",
      "training on iteration:2613    \t/3400, val loss:40.22874069213867\n",
      "training on iteration:2614    \t/3400, val loss:40.728477478027344\n",
      "training on iteration:2615    \t/3400, val loss:41.28672409057617\n",
      "training on iteration:2616    \t/3400, val loss:40.97320556640625\n",
      "training on iteration:2617    \t/3400, val loss:40.30164337158203\n",
      "training on iteration:2618    \t/3400, val loss:40.20501708984375\n",
      "training on iteration:2619    \t/3400, val loss:40.28427505493164\n",
      "training on iteration:2620    \t/3400, val loss:40.70841598510742\n",
      "training on iteration:2621    \t/3400, val loss:41.13886260986328\n",
      "training on iteration:2622    \t/3400, val loss:40.73734664916992\n",
      "training on iteration:2623    \t/3400, val loss:40.104736328125\n",
      "training on iteration:2624    \t/3400, val loss:40.00729751586914\n",
      "training on iteration:2625    \t/3400, val loss:40.14799880981445\n",
      "training on iteration:2626    \t/3400, val loss:40.774227142333984\n",
      "training on iteration:2627    \t/3400, val loss:41.00509262084961\n",
      "training on iteration:2628    \t/3400, val loss:40.306396484375\n",
      "training on iteration:2629    \t/3400, val loss:40.08190155029297\n",
      "training on iteration:2630    \t/3400, val loss:40.25246810913086\n",
      "training on iteration:2631    \t/3400, val loss:40.7884407043457\n",
      "training on iteration:2632    \t/3400, val loss:41.27460479736328\n",
      "training on iteration:2633    \t/3400, val loss:40.92296600341797\n",
      "training on iteration:2634    \t/3400, val loss:40.281272888183594\n",
      "training on iteration:2635    \t/3400, val loss:40.19673538208008\n",
      "training on iteration:2636    \t/3400, val loss:40.29132843017578\n",
      "training on iteration:2637    \t/3400, val loss:40.788330078125\n",
      "training on iteration:2638    \t/3400, val loss:41.17879867553711\n",
      "training on iteration:2639    \t/3400, val loss:40.703067779541016\n",
      "training on iteration:2640    \t/3400, val loss:40.077720642089844\n",
      "training on iteration:2641    \t/3400, val loss:39.99420928955078\n",
      "training on iteration:2642    \t/3400, val loss:40.17720413208008\n",
      "training on iteration:2643    \t/3400, val loss:40.86362838745117\n",
      "training on iteration:2644    \t/3400, val loss:41.02996826171875\n",
      "training on iteration:2645    \t/3400, val loss:40.27202224731445\n",
      "training on iteration:2646    \t/3400, val loss:40.0593147277832\n",
      "training on iteration:2647    \t/3400, val loss:40.22849655151367\n",
      "training on iteration:2648    \t/3400, val loss:40.83100509643555\n",
      "training on iteration:2649    \t/3400, val loss:41.31296157836914\n",
      "training on iteration:2650    \t/3400, val loss:40.85418701171875\n",
      "training on iteration:2651    \t/3400, val loss:40.23439407348633\n",
      "training on iteration:2652    \t/3400, val loss:40.179988861083984\n",
      "training on iteration:2653    \t/3400, val loss:40.32381820678711\n",
      "training on iteration:2654    \t/3400, val loss:40.86931610107422\n",
      "training on iteration:2655    \t/3400, val loss:41.19004440307617\n",
      "training on iteration:2656    \t/3400, val loss:40.61933135986328\n",
      "training on iteration:2657    \t/3400, val loss:40.03702926635742\n",
      "training on iteration:2658    \t/3400, val loss:39.98240280151367\n",
      "training on iteration:2659    \t/3400, val loss:40.21608352661133\n",
      "training on iteration:2660    \t/3400, val loss:40.89589309692383\n",
      "training on iteration:2661    \t/3400, val loss:40.9980583190918\n",
      "training on iteration:2662    \t/3400, val loss:40.22667694091797\n",
      "training on iteration:2663    \t/3400, val loss:40.04900360107422\n",
      "training on iteration:2664    \t/3400, val loss:40.23908233642578\n",
      "training on iteration:2665    \t/3400, val loss:40.886985778808594\n",
      "training on iteration:2666    \t/3400, val loss:41.29266357421875\n",
      "training on iteration:2667    \t/3400, val loss:40.81161880493164\n",
      "training on iteration:2668    \t/3400, val loss:40.22728729248047\n",
      "training on iteration:2669    \t/3400, val loss:40.17645263671875\n",
      "training on iteration:2670    \t/3400, val loss:40.31650924682617\n",
      "training on iteration:2671    \t/3400, val loss:40.89034652709961\n",
      "training on iteration:2672    \t/3400, val loss:41.16498565673828\n",
      "training on iteration:2673    \t/3400, val loss:40.56815719604492\n",
      "training on iteration:2674    \t/3400, val loss:40.0223274230957\n",
      "training on iteration:2675    \t/3400, val loss:39.98193359375\n",
      "training on iteration:2676    \t/3400, val loss:40.22393798828125\n",
      "training on iteration:2677    \t/3400, val loss:40.87058639526367\n",
      "training on iteration:2678    \t/3400, val loss:40.92197036743164\n",
      "training on iteration:2679    \t/3400, val loss:40.18463897705078\n",
      "training on iteration:2680    \t/3400, val loss:40.04480743408203\n",
      "training on iteration:2681    \t/3400, val loss:40.24867630004883\n",
      "training on iteration:2682    \t/3400, val loss:40.893028259277344\n",
      "training on iteration:2683    \t/3400, val loss:41.21406173706055\n",
      "training on iteration:2684    \t/3400, val loss:40.74203872680664\n",
      "training on iteration:2685    \t/3400, val loss:40.21869659423828\n",
      "training on iteration:2686    \t/3400, val loss:40.1788330078125\n",
      "training on iteration:2687    \t/3400, val loss:40.33700180053711\n",
      "training on iteration:2688    \t/3400, val loss:40.90811538696289\n",
      "training on iteration:2689    \t/3400, val loss:41.140140533447266\n",
      "training on iteration:2690    \t/3400, val loss:40.51564407348633\n",
      "training on iteration:2691    \t/3400, val loss:39.9879035949707\n",
      "training on iteration:2692    \t/3400, val loss:39.95220947265625\n",
      "training on iteration:2693    \t/3400, val loss:40.23690414428711\n",
      "training on iteration:2694    \t/3400, val loss:40.91357421875\n",
      "training on iteration:2695    \t/3400, val loss:40.89495086669922\n",
      "training on iteration:2696    \t/3400, val loss:40.1561164855957\n",
      "training on iteration:2697    \t/3400, val loss:40.05315399169922\n",
      "training on iteration:2698    \t/3400, val loss:40.24781036376953\n",
      "training on iteration:2699    \t/3400, val loss:40.951416015625\n",
      "training on iteration:2700    \t/3400, val loss:41.31003952026367\n",
      "training on iteration:2701    \t/3400, val loss:40.69525909423828\n",
      "training on iteration:2702    \t/3400, val loss:40.18760299682617\n",
      "training on iteration:2703    \t/3400, val loss:40.15678024291992\n",
      "training on iteration:2704    \t/3400, val loss:40.334232330322266\n",
      "training on iteration:2705    \t/3400, val loss:40.96046829223633\n",
      "training on iteration:2706    \t/3400, val loss:41.138118743896484\n",
      "training on iteration:2707    \t/3400, val loss:40.45531463623047\n",
      "training on iteration:2708    \t/3400, val loss:39.94861602783203\n",
      "training on iteration:2709    \t/3400, val loss:39.9322395324707\n",
      "training on iteration:2710    \t/3400, val loss:40.24507522583008\n",
      "training on iteration:2711    \t/3400, val loss:41.00497817993164\n",
      "training on iteration:2712    \t/3400, val loss:41.029541015625\n",
      "training on iteration:2713    \t/3400, val loss:40.192813873291016\n",
      "training on iteration:2714    \t/3400, val loss:40.04018783569336\n",
      "training on iteration:2715    \t/3400, val loss:40.20170974731445\n",
      "training on iteration:2716    \t/3400, val loss:40.92045211791992\n",
      "training on iteration:2717    \t/3400, val loss:41.35139846801758\n",
      "training on iteration:2718    \t/3400, val loss:40.76808547973633\n",
      "training on iteration:2719    \t/3400, val loss:40.178184509277344\n",
      "training on iteration:2720    \t/3400, val loss:40.1309814453125\n",
      "training on iteration:2721    \t/3400, val loss:40.281776428222656\n",
      "training on iteration:2722    \t/3400, val loss:40.908233642578125\n",
      "training on iteration:2723    \t/3400, val loss:41.19463348388672\n",
      "training on iteration:2724    \t/3400, val loss:40.528709411621094\n",
      "training on iteration:2725    \t/3400, val loss:39.97138595581055\n",
      "training on iteration:2726    \t/3400, val loss:39.93451690673828\n",
      "training on iteration:2727    \t/3400, val loss:40.20999526977539\n",
      "training on iteration:2728    \t/3400, val loss:40.91606521606445\n",
      "training on iteration:2729    \t/3400, val loss:40.96338653564453\n",
      "training on iteration:2730    \t/3400, val loss:40.19183349609375\n",
      "training on iteration:2731    \t/3400, val loss:40.057762145996094\n",
      "training on iteration:2732    \t/3400, val loss:40.22541427612305\n",
      "training on iteration:2733    \t/3400, val loss:40.895240783691406\n",
      "training on iteration:2734    \t/3400, val loss:41.24566650390625\n",
      "training on iteration:2735    \t/3400, val loss:40.65914535522461\n",
      "training on iteration:2736    \t/3400, val loss:40.13441848754883\n",
      "training on iteration:2737    \t/3400, val loss:40.10630416870117\n",
      "training on iteration:2738    \t/3400, val loss:40.28731918334961\n",
      "training on iteration:2739    \t/3400, val loss:40.96694564819336\n",
      "training on iteration:2740    \t/3400, val loss:41.21638107299805\n",
      "training on iteration:2741    \t/3400, val loss:40.47359085083008\n",
      "training on iteration:2742    \t/3400, val loss:39.94329071044922\n",
      "training on iteration:2743    \t/3400, val loss:39.92136001586914\n",
      "training on iteration:2744    \t/3400, val loss:40.2099609375\n",
      "training on iteration:2745    \t/3400, val loss:40.98954391479492\n",
      "training on iteration:2746    \t/3400, val loss:41.06646728515625\n",
      "training on iteration:2747    \t/3400, val loss:40.23485565185547\n",
      "training on iteration:2748    \t/3400, val loss:40.058860778808594\n",
      "training on iteration:2749    \t/3400, val loss:40.203792572021484\n",
      "training on iteration:2750    \t/3400, val loss:40.89992141723633\n",
      "training on iteration:2751    \t/3400, val loss:41.350807189941406\n",
      "training on iteration:2752    \t/3400, val loss:40.7042236328125\n",
      "training on iteration:2753    \t/3400, val loss:40.10858154296875\n",
      "training on iteration:2754    \t/3400, val loss:40.06709671020508\n",
      "training on iteration:2755    \t/3400, val loss:40.207969665527344\n",
      "training on iteration:2756    \t/3400, val loss:40.90623092651367\n",
      "training on iteration:2757    \t/3400, val loss:41.21828079223633\n",
      "training on iteration:2758    \t/3400, val loss:40.467071533203125\n",
      "training on iteration:2759    \t/3400, val loss:39.9095458984375\n",
      "training on iteration:2760    \t/3400, val loss:39.88785934448242\n",
      "training on iteration:2761    \t/3400, val loss:40.193477630615234\n",
      "training on iteration:2762    \t/3400, val loss:41.0813102722168\n",
      "training on iteration:2763    \t/3400, val loss:41.17707061767578\n",
      "training on iteration:2764    \t/3400, val loss:40.24081039428711\n",
      "training on iteration:2765    \t/3400, val loss:40.053955078125\n",
      "training on iteration:2766    \t/3400, val loss:40.174434661865234\n",
      "training on iteration:2767    \t/3400, val loss:40.93760299682617\n",
      "training on iteration:2768    \t/3400, val loss:41.483734130859375\n",
      "training on iteration:2769    \t/3400, val loss:40.769737243652344\n",
      "training on iteration:2770    \t/3400, val loss:40.12076950073242\n",
      "training on iteration:2771    \t/3400, val loss:40.07442855834961\n",
      "training on iteration:2772    \t/3400, val loss:40.193729400634766\n",
      "training on iteration:2773    \t/3400, val loss:40.9134407043457\n",
      "training on iteration:2774    \t/3400, val loss:41.26080322265625\n",
      "training on iteration:2775    \t/3400, val loss:40.51710891723633\n",
      "training on iteration:2776    \t/3400, val loss:39.89872360229492\n",
      "training on iteration:2777    \t/3400, val loss:39.86418914794922\n",
      "training on iteration:2778    \t/3400, val loss:40.13364028930664\n",
      "training on iteration:2779    \t/3400, val loss:41.06288528442383\n",
      "training on iteration:2780    \t/3400, val loss:41.22260665893555\n",
      "training on iteration:2781    \t/3400, val loss:40.26668167114258\n",
      "training on iteration:2782    \t/3400, val loss:40.06058120727539\n",
      "training on iteration:2783    \t/3400, val loss:40.16706466674805\n",
      "training on iteration:2784    \t/3400, val loss:40.92269515991211\n",
      "training on iteration:2785    \t/3400, val loss:41.51829147338867\n",
      "training on iteration:2786    \t/3400, val loss:40.81107711791992\n",
      "training on iteration:2787    \t/3400, val loss:40.12963104248047\n",
      "training on iteration:2788    \t/3400, val loss:40.06447219848633\n",
      "training on iteration:2789    \t/3400, val loss:40.16207504272461\n",
      "training on iteration:2790    \t/3400, val loss:40.82721710205078\n",
      "training on iteration:2791    \t/3400, val loss:41.17476272583008\n",
      "training on iteration:2792    \t/3400, val loss:40.4771842956543\n",
      "training on iteration:2793    \t/3400, val loss:39.89480972290039\n",
      "training on iteration:2794    \t/3400, val loss:39.86317825317383\n",
      "training on iteration:2795    \t/3400, val loss:40.12307357788086\n",
      "training on iteration:2796    \t/3400, val loss:40.92873764038086\n",
      "training on iteration:2797    \t/3400, val loss:41.09320068359375\n",
      "training on iteration:2798    \t/3400, val loss:40.2696418762207\n",
      "training on iteration:2799    \t/3400, val loss:40.06206512451172\n",
      "training on iteration:2800    \t/3400, val loss:40.179874420166016\n",
      "training on iteration:2801    \t/3400, val loss:40.82810592651367\n",
      "training on iteration:2802    \t/3400, val loss:41.31887435913086\n",
      "training on iteration:2803    \t/3400, val loss:40.69563674926758\n",
      "training on iteration:2804    \t/3400, val loss:40.09589385986328\n",
      "training on iteration:2805    \t/3400, val loss:40.04075241088867\n",
      "training on iteration:2806    \t/3400, val loss:40.149879455566406\n",
      "training on iteration:2807    \t/3400, val loss:40.80166244506836\n",
      "training on iteration:2808    \t/3400, val loss:41.10084915161133\n",
      "training on iteration:2809    \t/3400, val loss:40.39707565307617\n",
      "training on iteration:2810    \t/3400, val loss:39.86151885986328\n",
      "training on iteration:2811    \t/3400, val loss:39.85258865356445\n",
      "training on iteration:2812    \t/3400, val loss:40.144588470458984\n",
      "training on iteration:2813    \t/3400, val loss:40.97218704223633\n",
      "training on iteration:2814    \t/3400, val loss:41.0874137878418\n",
      "training on iteration:2815    \t/3400, val loss:40.23566818237305\n",
      "training on iteration:2816    \t/3400, val loss:40.052528381347656\n",
      "training on iteration:2817    \t/3400, val loss:40.19112014770508\n",
      "training on iteration:2818    \t/3400, val loss:40.87648391723633\n",
      "training on iteration:2819    \t/3400, val loss:41.30879592895508\n",
      "training on iteration:2820    \t/3400, val loss:40.66923904418945\n",
      "training on iteration:2821    \t/3400, val loss:40.07909393310547\n",
      "training on iteration:2822    \t/3400, val loss:40.017822265625\n",
      "training on iteration:2823    \t/3400, val loss:40.14165115356445\n",
      "training on iteration:2824    \t/3400, val loss:40.76979446411133\n",
      "training on iteration:2825    \t/3400, val loss:41.03797149658203\n",
      "training on iteration:2826    \t/3400, val loss:40.34944534301758\n",
      "training on iteration:2827    \t/3400, val loss:39.84083938598633\n",
      "training on iteration:2828    \t/3400, val loss:39.84142303466797\n",
      "training on iteration:2829    \t/3400, val loss:40.16278839111328\n",
      "training on iteration:2830    \t/3400, val loss:41.02490997314453\n",
      "training on iteration:2831    \t/3400, val loss:41.10919189453125\n",
      "training on iteration:2832    \t/3400, val loss:40.23269271850586\n",
      "training on iteration:2833    \t/3400, val loss:40.05052947998047\n",
      "training on iteration:2834    \t/3400, val loss:40.165367126464844\n",
      "training on iteration:2835    \t/3400, val loss:40.911434173583984\n",
      "training on iteration:2836    \t/3400, val loss:41.46248245239258\n",
      "training on iteration:2837    \t/3400, val loss:40.72876739501953\n",
      "training on iteration:2838    \t/3400, val loss:40.069400787353516\n",
      "training on iteration:2839    \t/3400, val loss:40.011199951171875\n",
      "training on iteration:2840    \t/3400, val loss:40.11479949951172\n",
      "training on iteration:2841    \t/3400, val loss:40.84197235107422\n",
      "training on iteration:2842    \t/3400, val loss:41.19015884399414\n",
      "training on iteration:2843    \t/3400, val loss:40.400978088378906\n",
      "training on iteration:2844    \t/3400, val loss:39.837806701660156\n",
      "training on iteration:2845    \t/3400, val loss:39.820133209228516\n",
      "training on iteration:2846    \t/3400, val loss:40.11348342895508\n",
      "training on iteration:2847    \t/3400, val loss:41.05188751220703\n",
      "training on iteration:2848    \t/3400, val loss:41.17658233642578\n",
      "training on iteration:2849    \t/3400, val loss:40.23133087158203\n",
      "training on iteration:2850    \t/3400, val loss:40.04073715209961\n",
      "training on iteration:2851    \t/3400, val loss:40.13168716430664\n",
      "training on iteration:2852    \t/3400, val loss:40.91604232788086\n",
      "training on iteration:2853    \t/3400, val loss:41.583133697509766\n",
      "training on iteration:2854    \t/3400, val loss:40.75653076171875\n",
      "training on iteration:2855    \t/3400, val loss:40.0577507019043\n",
      "training on iteration:2856    \t/3400, val loss:40.00374984741211\n",
      "training on iteration:2857    \t/3400, val loss:40.09040451049805\n",
      "training on iteration:2858    \t/3400, val loss:40.83902359008789\n",
      "training on iteration:2859    \t/3400, val loss:41.19875717163086\n",
      "training on iteration:2860    \t/3400, val loss:40.403839111328125\n",
      "training on iteration:2861    \t/3400, val loss:39.800289154052734\n",
      "training on iteration:2862    \t/3400, val loss:39.79539108276367\n",
      "training on iteration:2863    \t/3400, val loss:40.091575622558594\n",
      "training on iteration:2864    \t/3400, val loss:41.06601333618164\n",
      "training on iteration:2865    \t/3400, val loss:41.16311264038086\n",
      "training on iteration:2866    \t/3400, val loss:40.21450424194336\n",
      "training on iteration:2867    \t/3400, val loss:40.0215950012207\n",
      "training on iteration:2868    \t/3400, val loss:40.13045120239258\n",
      "training on iteration:2869    \t/3400, val loss:40.88467025756836\n",
      "training on iteration:2870    \t/3400, val loss:41.5245361328125\n",
      "training on iteration:2871    \t/3400, val loss:40.7722282409668\n",
      "training on iteration:2872    \t/3400, val loss:40.08131790161133\n",
      "training on iteration:2873    \t/3400, val loss:40.009979248046875\n",
      "training on iteration:2874    \t/3400, val loss:40.078853607177734\n",
      "training on iteration:2875    \t/3400, val loss:40.78701400756836\n",
      "training on iteration:2876    \t/3400, val loss:41.171905517578125\n",
      "training on iteration:2877    \t/3400, val loss:40.38801956176758\n",
      "training on iteration:2878    \t/3400, val loss:39.79983139038086\n",
      "training on iteration:2879    \t/3400, val loss:39.77834701538086\n",
      "training on iteration:2880    \t/3400, val loss:40.06043243408203\n",
      "training on iteration:2881    \t/3400, val loss:40.97146224975586\n",
      "training on iteration:2882    \t/3400, val loss:41.10424041748047\n",
      "training on iteration:2883    \t/3400, val loss:40.19537353515625\n",
      "training on iteration:2884    \t/3400, val loss:40.00443649291992\n",
      "training on iteration:2885    \t/3400, val loss:40.11491012573242\n",
      "training on iteration:2886    \t/3400, val loss:40.87380599975586\n",
      "training on iteration:2887    \t/3400, val loss:41.548789978027344\n",
      "training on iteration:2888    \t/3400, val loss:40.77848434448242\n",
      "training on iteration:2889    \t/3400, val loss:40.06393814086914\n",
      "training on iteration:2890    \t/3400, val loss:39.99402618408203\n",
      "training on iteration:2891    \t/3400, val loss:40.062950134277344\n",
      "training on iteration:2892    \t/3400, val loss:40.72809600830078\n",
      "training on iteration:2893    \t/3400, val loss:41.071128845214844\n",
      "training on iteration:2894    \t/3400, val loss:40.380271911621094\n",
      "training on iteration:2895    \t/3400, val loss:39.79779815673828\n",
      "training on iteration:2896    \t/3400, val loss:39.784263610839844\n",
      "training on iteration:2897    \t/3400, val loss:40.08397674560547\n",
      "training on iteration:2898    \t/3400, val loss:40.967315673828125\n",
      "training on iteration:2899    \t/3400, val loss:41.09957504272461\n",
      "training on iteration:2900    \t/3400, val loss:40.212501525878906\n",
      "training on iteration:2901    \t/3400, val loss:40.0283088684082\n",
      "training on iteration:2902    \t/3400, val loss:40.144439697265625\n",
      "training on iteration:2903    \t/3400, val loss:40.89427947998047\n",
      "training on iteration:2904    \t/3400, val loss:41.4323844909668\n",
      "training on iteration:2905    \t/3400, val loss:40.71526336669922\n",
      "training on iteration:2906    \t/3400, val loss:40.05550765991211\n",
      "training on iteration:2907    \t/3400, val loss:39.991519927978516\n",
      "training on iteration:2908    \t/3400, val loss:40.07623291015625\n",
      "training on iteration:2909    \t/3400, val loss:40.77720260620117\n",
      "training on iteration:2910    \t/3400, val loss:41.08559799194336\n",
      "training on iteration:2911    \t/3400, val loss:40.33122634887695\n",
      "training on iteration:2912    \t/3400, val loss:39.78043746948242\n",
      "training on iteration:2913    \t/3400, val loss:39.779747009277344\n",
      "training on iteration:2914    \t/3400, val loss:40.083984375\n",
      "training on iteration:2915    \t/3400, val loss:40.978389739990234\n",
      "training on iteration:2916    \t/3400, val loss:41.107818603515625\n",
      "training on iteration:2917    \t/3400, val loss:40.19007110595703\n",
      "training on iteration:2918    \t/3400, val loss:40.00571823120117\n",
      "training on iteration:2919    \t/3400, val loss:40.109554290771484\n",
      "training on iteration:2920    \t/3400, val loss:40.86207962036133\n",
      "training on iteration:2921    \t/3400, val loss:41.50290298461914\n",
      "training on iteration:2922    \t/3400, val loss:40.75079345703125\n",
      "training on iteration:2923    \t/3400, val loss:40.05011749267578\n",
      "training on iteration:2924    \t/3400, val loss:39.981868743896484\n",
      "training on iteration:2925    \t/3400, val loss:40.03521728515625\n",
      "training on iteration:2926    \t/3400, val loss:40.74386978149414\n",
      "training on iteration:2927    \t/3400, val loss:41.14814376831055\n",
      "training on iteration:2928    \t/3400, val loss:40.38186264038086\n",
      "training on iteration:2929    \t/3400, val loss:39.7797966003418\n",
      "training on iteration:2930    \t/3400, val loss:39.761962890625\n",
      "training on iteration:2931    \t/3400, val loss:40.03795623779297\n",
      "training on iteration:2932    \t/3400, val loss:40.95970916748047\n",
      "training on iteration:2933    \t/3400, val loss:41.17719650268555\n",
      "training on iteration:2934    \t/3400, val loss:40.22269058227539\n",
      "training on iteration:2935    \t/3400, val loss:40.00062942504883\n",
      "training on iteration:2936    \t/3400, val loss:40.08847427368164\n",
      "training on iteration:2937    \t/3400, val loss:40.85197830200195\n",
      "training on iteration:2938    \t/3400, val loss:41.55530548095703\n",
      "training on iteration:2939    \t/3400, val loss:40.73657989501953\n",
      "training on iteration:2940    \t/3400, val loss:40.01045608520508\n",
      "training on iteration:2941    \t/3400, val loss:39.96463394165039\n",
      "training on iteration:2942    \t/3400, val loss:40.03533172607422\n",
      "training on iteration:2943    \t/3400, val loss:40.85313034057617\n",
      "training on iteration:2944    \t/3400, val loss:41.22810745239258\n",
      "training on iteration:2945    \t/3400, val loss:40.32517623901367\n",
      "training on iteration:2946    \t/3400, val loss:39.751129150390625\n",
      "training on iteration:2947    \t/3400, val loss:39.76270294189453\n",
      "training on iteration:2948    \t/3400, val loss:40.085968017578125\n",
      "training on iteration:2949    \t/3400, val loss:41.13410186767578\n",
      "training on iteration:2950    \t/3400, val loss:41.20344924926758\n",
      "training on iteration:2951    \t/3400, val loss:40.174224853515625\n",
      "training on iteration:2952    \t/3400, val loss:39.99247360229492\n",
      "training on iteration:2953    \t/3400, val loss:40.082862854003906\n",
      "training on iteration:2954    \t/3400, val loss:40.87160873413086\n",
      "training on iteration:2955    \t/3400, val loss:41.57429885864258\n",
      "training on iteration:2956    \t/3400, val loss:40.73152542114258\n",
      "training on iteration:2957    \t/3400, val loss:40.022193908691406\n",
      "training on iteration:2958    \t/3400, val loss:39.967525482177734\n",
      "training on iteration:2959    \t/3400, val loss:40.02484893798828\n",
      "training on iteration:2960    \t/3400, val loss:40.81385040283203\n",
      "training on iteration:2961    \t/3400, val loss:41.18751907348633\n",
      "training on iteration:2962    \t/3400, val loss:40.34884262084961\n",
      "training on iteration:2963    \t/3400, val loss:39.75798416137695\n",
      "training on iteration:2964    \t/3400, val loss:39.76137924194336\n",
      "training on iteration:2965    \t/3400, val loss:40.05150604248047\n",
      "training on iteration:2966    \t/3400, val loss:41.03963851928711\n",
      "training on iteration:2967    \t/3400, val loss:41.17110061645508\n",
      "training on iteration:2968    \t/3400, val loss:40.17308807373047\n",
      "training on iteration:2969    \t/3400, val loss:40.0016975402832\n",
      "training on iteration:2970    \t/3400, val loss:40.08000946044922\n",
      "training on iteration:2971    \t/3400, val loss:40.90224075317383\n",
      "training on iteration:2972    \t/3400, val loss:41.66245651245117\n",
      "training on iteration:2973    \t/3400, val loss:40.78406524658203\n",
      "training on iteration:2974    \t/3400, val loss:39.99971389770508\n",
      "training on iteration:2975    \t/3400, val loss:39.95935821533203\n",
      "training on iteration:2976    \t/3400, val loss:39.96660232543945\n",
      "training on iteration:2977    \t/3400, val loss:40.86342239379883\n",
      "training on iteration:2978    \t/3400, val loss:41.34617233276367\n",
      "training on iteration:2979    \t/3400, val loss:40.40805435180664\n",
      "training on iteration:2980    \t/3400, val loss:39.74924850463867\n",
      "training on iteration:2981    \t/3400, val loss:39.76093292236328\n",
      "training on iteration:2982    \t/3400, val loss:40.03419876098633\n",
      "training on iteration:2983    \t/3400, val loss:41.09697341918945\n",
      "training on iteration:2984    \t/3400, val loss:41.25666427612305\n",
      "training on iteration:2985    \t/3400, val loss:40.20236587524414\n",
      "training on iteration:2986    \t/3400, val loss:40.02230453491211\n",
      "training on iteration:2987    \t/3400, val loss:40.08072280883789\n",
      "training on iteration:2988    \t/3400, val loss:40.86138916015625\n",
      "training on iteration:2989    \t/3400, val loss:41.67921829223633\n",
      "training on iteration:2990    \t/3400, val loss:40.81304931640625\n",
      "training on iteration:2991    \t/3400, val loss:39.98204803466797\n",
      "training on iteration:2992    \t/3400, val loss:39.92866897583008\n",
      "training on iteration:2993    \t/3400, val loss:39.93466567993164\n",
      "training on iteration:2994    \t/3400, val loss:40.80170822143555\n",
      "training on iteration:2995    \t/3400, val loss:41.33747100830078\n",
      "training on iteration:2996    \t/3400, val loss:40.41094970703125\n",
      "training on iteration:2997    \t/3400, val loss:39.750030517578125\n",
      "training on iteration:2998    \t/3400, val loss:39.75535202026367\n",
      "training on iteration:2999    \t/3400, val loss:40.03147506713867\n",
      "training on iteration:3000    \t/3400, val loss:41.03709030151367\n",
      "training on iteration:3001    \t/3400, val loss:41.20708084106445\n",
      "training on iteration:3002    \t/3400, val loss:40.216339111328125\n",
      "training on iteration:3003    \t/3400, val loss:40.035064697265625\n",
      "training on iteration:3004    \t/3400, val loss:40.09096908569336\n",
      "training on iteration:3005    \t/3400, val loss:40.88019561767578\n",
      "training on iteration:3006    \t/3400, val loss:41.660762786865234\n",
      "training on iteration:3007    \t/3400, val loss:40.76340866088867\n",
      "training on iteration:3008    \t/3400, val loss:39.96284866333008\n",
      "training on iteration:3009    \t/3400, val loss:39.9012451171875\n",
      "training on iteration:3010    \t/3400, val loss:39.93511199951172\n",
      "training on iteration:3011    \t/3400, val loss:40.80960464477539\n",
      "training on iteration:3012    \t/3400, val loss:41.24538803100586\n",
      "training on iteration:3013    \t/3400, val loss:40.317325592041016\n",
      "training on iteration:3014    \t/3400, val loss:39.73035430908203\n",
      "training on iteration:3015    \t/3400, val loss:39.74473190307617\n",
      "training on iteration:3016    \t/3400, val loss:40.07509231567383\n",
      "training on iteration:3017    \t/3400, val loss:41.11301040649414\n",
      "training on iteration:3018    \t/3400, val loss:41.180049896240234\n",
      "training on iteration:3019    \t/3400, val loss:40.19429397583008\n",
      "training on iteration:3020    \t/3400, val loss:40.02489471435547\n",
      "training on iteration:3021    \t/3400, val loss:40.10375213623047\n",
      "training on iteration:3022    \t/3400, val loss:40.885093688964844\n",
      "training on iteration:3023    \t/3400, val loss:41.52535629272461\n",
      "training on iteration:3024    \t/3400, val loss:40.66936492919922\n",
      "training on iteration:3025    \t/3400, val loss:39.95803451538086\n",
      "training on iteration:3026    \t/3400, val loss:39.887325286865234\n",
      "training on iteration:3027    \t/3400, val loss:39.94194793701172\n",
      "training on iteration:3028    \t/3400, val loss:40.69176483154297\n",
      "training on iteration:3029    \t/3400, val loss:41.10791015625\n",
      "training on iteration:3030    \t/3400, val loss:40.30061721801758\n",
      "training on iteration:3031    \t/3400, val loss:39.72821044921875\n",
      "training on iteration:3032    \t/3400, val loss:39.74585723876953\n",
      "training on iteration:3033    \t/3400, val loss:40.057945251464844\n",
      "training on iteration:3034    \t/3400, val loss:41.0643424987793\n",
      "training on iteration:3035    \t/3400, val loss:41.20753860473633\n",
      "training on iteration:3036    \t/3400, val loss:40.221893310546875\n",
      "training on iteration:3037    \t/3400, val loss:40.01723098754883\n",
      "training on iteration:3038    \t/3400, val loss:40.0733642578125\n",
      "training on iteration:3039    \t/3400, val loss:40.83225631713867\n",
      "training on iteration:3040    \t/3400, val loss:41.53618621826172\n",
      "training on iteration:3041    \t/3400, val loss:40.70111846923828\n",
      "training on iteration:3042    \t/3400, val loss:39.94461441040039\n",
      "training on iteration:3043    \t/3400, val loss:39.88137435913086\n",
      "training on iteration:3044    \t/3400, val loss:39.93510055541992\n",
      "training on iteration:3045    \t/3400, val loss:40.73135757446289\n",
      "training on iteration:3046    \t/3400, val loss:41.15547561645508\n",
      "training on iteration:3047    \t/3400, val loss:40.30746841430664\n",
      "training on iteration:3048    \t/3400, val loss:39.73154067993164\n",
      "training on iteration:3049    \t/3400, val loss:39.75495529174805\n",
      "training on iteration:3050    \t/3400, val loss:40.07952880859375\n",
      "training on iteration:3051    \t/3400, val loss:41.10512161254883\n",
      "training on iteration:3052    \t/3400, val loss:41.18038558959961\n",
      "training on iteration:3053    \t/3400, val loss:40.18927764892578\n",
      "training on iteration:3054    \t/3400, val loss:40.012569427490234\n",
      "training on iteration:3055    \t/3400, val loss:40.07797622680664\n",
      "training on iteration:3056    \t/3400, val loss:40.75693130493164\n",
      "training on iteration:3057    \t/3400, val loss:41.35023880004883\n",
      "training on iteration:3058    \t/3400, val loss:40.60940933227539\n",
      "training on iteration:3059    \t/3400, val loss:39.932029724121094\n",
      "training on iteration:3060    \t/3400, val loss:39.84983444213867\n",
      "training on iteration:3061    \t/3400, val loss:39.91233444213867\n",
      "training on iteration:3062    \t/3400, val loss:40.56319046020508\n",
      "training on iteration:3063    \t/3400, val loss:40.931427001953125\n",
      "training on iteration:3064    \t/3400, val loss:40.2496452331543\n",
      "training on iteration:3065    \t/3400, val loss:39.74501419067383\n",
      "training on iteration:3066    \t/3400, val loss:39.763694763183594\n",
      "training on iteration:3067    \t/3400, val loss:40.095218658447266\n",
      "training on iteration:3068    \t/3400, val loss:40.946929931640625\n",
      "training on iteration:3069    \t/3400, val loss:40.9742431640625\n",
      "training on iteration:3070    \t/3400, val loss:40.1292610168457\n",
      "training on iteration:3071    \t/3400, val loss:40.005859375\n",
      "training on iteration:3072    \t/3400, val loss:40.116485595703125\n",
      "training on iteration:3073    \t/3400, val loss:40.81184005737305\n",
      "training on iteration:3074    \t/3400, val loss:41.16344451904297\n",
      "training on iteration:3075    \t/3400, val loss:40.42695999145508\n",
      "training on iteration:3076    \t/3400, val loss:39.89738082885742\n",
      "training on iteration:3077    \t/3400, val loss:39.83224868774414\n",
      "training on iteration:3078    \t/3400, val loss:39.98991012573242\n",
      "training on iteration:3079    \t/3400, val loss:40.71297073364258\n",
      "training on iteration:3080    \t/3400, val loss:40.9287223815918\n",
      "training on iteration:3081    \t/3400, val loss:40.13717269897461\n",
      "training on iteration:3082    \t/3400, val loss:39.7241325378418\n",
      "training on iteration:3083    \t/3400, val loss:39.77001953125\n",
      "training on iteration:3084    \t/3400, val loss:40.19004440307617\n",
      "training on iteration:3085    \t/3400, val loss:41.07862091064453\n",
      "training on iteration:3086    \t/3400, val loss:40.9366340637207\n",
      "training on iteration:3087    \t/3400, val loss:40.09497833251953\n",
      "training on iteration:3088    \t/3400, val loss:40.016361236572266\n",
      "training on iteration:3089    \t/3400, val loss:40.08928298950195\n",
      "training on iteration:3090    \t/3400, val loss:40.92683792114258\n",
      "training on iteration:3091    \t/3400, val loss:41.384361267089844\n",
      "training on iteration:3092    \t/3400, val loss:40.46114730834961\n",
      "training on iteration:3093    \t/3400, val loss:39.885948181152344\n",
      "training on iteration:3094    \t/3400, val loss:39.82459259033203\n",
      "training on iteration:3095    \t/3400, val loss:39.970359802246094\n",
      "training on iteration:3096    \t/3400, val loss:40.83878707885742\n",
      "training on iteration:3097    \t/3400, val loss:40.98847198486328\n",
      "training on iteration:3098    \t/3400, val loss:40.08613967895508\n",
      "training on iteration:3099    \t/3400, val loss:39.72589874267578\n",
      "training on iteration:3100    \t/3400, val loss:39.76726150512695\n",
      "training on iteration:3101    \t/3400, val loss:40.24797821044922\n",
      "training on iteration:3102    \t/3400, val loss:41.202877044677734\n",
      "training on iteration:3103    \t/3400, val loss:40.99404525756836\n",
      "training on iteration:3104    \t/3400, val loss:40.08946990966797\n",
      "training on iteration:3105    \t/3400, val loss:40.034786224365234\n",
      "training on iteration:3106    \t/3400, val loss:40.051795959472656\n",
      "training on iteration:3107    \t/3400, val loss:40.96763229370117\n",
      "training on iteration:3108    \t/3400, val loss:41.55643844604492\n",
      "training on iteration:3109    \t/3400, val loss:40.52519989013672\n",
      "training on iteration:3110    \t/3400, val loss:39.8746452331543\n",
      "training on iteration:3111    \t/3400, val loss:39.85333251953125\n",
      "training on iteration:3112    \t/3400, val loss:39.94129180908203\n",
      "training on iteration:3113    \t/3400, val loss:40.912315368652344\n",
      "training on iteration:3114    \t/3400, val loss:41.08617401123047\n",
      "training on iteration:3115    \t/3400, val loss:40.140132904052734\n",
      "training on iteration:3116    \t/3400, val loss:39.71683883666992\n",
      "training on iteration:3117    \t/3400, val loss:39.757171630859375\n",
      "training on iteration:3118    \t/3400, val loss:40.18729782104492\n",
      "training on iteration:3119    \t/3400, val loss:41.23645782470703\n",
      "training on iteration:3120    \t/3400, val loss:41.06161117553711\n",
      "training on iteration:3121    \t/3400, val loss:40.09398651123047\n",
      "training on iteration:3122    \t/3400, val loss:40.036319732666016\n",
      "training on iteration:3123    \t/3400, val loss:40.05487823486328\n",
      "training on iteration:3124    \t/3400, val loss:40.94069290161133\n",
      "training on iteration:3125    \t/3400, val loss:41.56951141357422\n",
      "training on iteration:3126    \t/3400, val loss:40.59329605102539\n",
      "training on iteration:3127    \t/3400, val loss:39.85878372192383\n",
      "training on iteration:3128    \t/3400, val loss:39.80930709838867\n",
      "training on iteration:3129    \t/3400, val loss:39.87997817993164\n",
      "training on iteration:3130    \t/3400, val loss:40.80308151245117\n",
      "training on iteration:3131    \t/3400, val loss:41.068504333496094\n",
      "training on iteration:3132    \t/3400, val loss:40.14701461791992\n",
      "training on iteration:3133    \t/3400, val loss:39.7098388671875\n",
      "training on iteration:3134    \t/3400, val loss:39.7480583190918\n",
      "training on iteration:3135    \t/3400, val loss:40.13637924194336\n",
      "training on iteration:3136    \t/3400, val loss:41.19572067260742\n",
      "training on iteration:3137    \t/3400, val loss:41.16508865356445\n",
      "training on iteration:3138    \t/3400, val loss:40.1358642578125\n",
      "training on iteration:3139    \t/3400, val loss:40.00712203979492\n",
      "training on iteration:3140    \t/3400, val loss:40.02020263671875\n",
      "training on iteration:3141    \t/3400, val loss:40.849246978759766\n",
      "training on iteration:3142    \t/3400, val loss:41.551475524902344\n",
      "training on iteration:3143    \t/3400, val loss:40.55805206298828\n",
      "training on iteration:3144    \t/3400, val loss:39.848907470703125\n",
      "training on iteration:3145    \t/3400, val loss:39.82204818725586\n",
      "training on iteration:3146    \t/3400, val loss:39.87560272216797\n",
      "training on iteration:3147    \t/3400, val loss:40.81371307373047\n",
      "training on iteration:3148    \t/3400, val loss:41.10159683227539\n",
      "training on iteration:3149    \t/3400, val loss:40.17007064819336\n",
      "training on iteration:3150    \t/3400, val loss:39.71297836303711\n",
      "training on iteration:3151    \t/3400, val loss:39.75099563598633\n",
      "training on iteration:3152    \t/3400, val loss:40.13414764404297\n",
      "training on iteration:3153    \t/3400, val loss:41.24503707885742\n",
      "training on iteration:3154    \t/3400, val loss:41.132591247558594\n",
      "training on iteration:3155    \t/3400, val loss:40.10040283203125\n",
      "training on iteration:3156    \t/3400, val loss:39.99671173095703\n",
      "training on iteration:3157    \t/3400, val loss:40.03740692138672\n",
      "training on iteration:3158    \t/3400, val loss:40.85570526123047\n",
      "training on iteration:3159    \t/3400, val loss:41.42910385131836\n",
      "training on iteration:3160    \t/3400, val loss:40.48113250732422\n",
      "training on iteration:3161    \t/3400, val loss:39.848323822021484\n",
      "training on iteration:3162    \t/3400, val loss:39.789337158203125\n",
      "training on iteration:3163    \t/3400, val loss:39.903076171875\n",
      "training on iteration:3164    \t/3400, val loss:40.8330192565918\n",
      "training on iteration:3165    \t/3400, val loss:40.97539520263672\n",
      "training on iteration:3166    \t/3400, val loss:40.05729293823242\n",
      "training on iteration:3167    \t/3400, val loss:39.69713592529297\n",
      "training on iteration:3168    \t/3400, val loss:39.74202346801758\n",
      "training on iteration:3169    \t/3400, val loss:40.182682037353516\n",
      "training on iteration:3170    \t/3400, val loss:41.19973373413086\n",
      "training on iteration:3171    \t/3400, val loss:41.0325813293457\n",
      "training on iteration:3172    \t/3400, val loss:40.073219299316406\n",
      "training on iteration:3173    \t/3400, val loss:39.98882293701172\n",
      "training on iteration:3174    \t/3400, val loss:40.02790069580078\n",
      "training on iteration:3175    \t/3400, val loss:40.84204864501953\n",
      "training on iteration:3176    \t/3400, val loss:41.34904098510742\n",
      "training on iteration:3177    \t/3400, val loss:40.4233283996582\n",
      "training on iteration:3178    \t/3400, val loss:39.8360481262207\n",
      "training on iteration:3179    \t/3400, val loss:39.77704620361328\n",
      "training on iteration:3180    \t/3400, val loss:39.873146057128906\n",
      "training on iteration:3181    \t/3400, val loss:40.65483856201172\n",
      "training on iteration:3182    \t/3400, val loss:40.93093490600586\n",
      "training on iteration:3183    \t/3400, val loss:40.118534088134766\n",
      "training on iteration:3184    \t/3400, val loss:39.69549560546875\n",
      "training on iteration:3185    \t/3400, val loss:39.74213409423828\n",
      "training on iteration:3186    \t/3400, val loss:40.13576126098633\n",
      "training on iteration:3187    \t/3400, val loss:41.1192626953125\n",
      "training on iteration:3188    \t/3400, val loss:41.00527572631836\n",
      "training on iteration:3189    \t/3400, val loss:40.06584930419922\n",
      "training on iteration:3190    \t/3400, val loss:39.97916793823242\n",
      "training on iteration:3191    \t/3400, val loss:40.01630783081055\n",
      "training on iteration:3192    \t/3400, val loss:40.813045501708984\n",
      "training on iteration:3193    \t/3400, val loss:41.39302444458008\n",
      "training on iteration:3194    \t/3400, val loss:40.44845962524414\n",
      "training on iteration:3195    \t/3400, val loss:39.81772994995117\n",
      "training on iteration:3196    \t/3400, val loss:39.769493103027344\n",
      "training on iteration:3197    \t/3400, val loss:39.87201690673828\n",
      "training on iteration:3198    \t/3400, val loss:40.68836212158203\n",
      "training on iteration:3199    \t/3400, val loss:40.954833984375\n",
      "training on iteration:3200    \t/3400, val loss:40.07363510131836\n",
      "training on iteration:3201    \t/3400, val loss:39.696083068847656\n",
      "training on iteration:3202    \t/3400, val loss:39.73773193359375\n",
      "training on iteration:3203    \t/3400, val loss:40.157291412353516\n",
      "training on iteration:3204    \t/3400, val loss:41.15324020385742\n",
      "training on iteration:3205    \t/3400, val loss:40.993125915527344\n",
      "training on iteration:3206    \t/3400, val loss:40.05709457397461\n",
      "training on iteration:3207    \t/3400, val loss:39.97632598876953\n",
      "training on iteration:3208    \t/3400, val loss:40.006866455078125\n",
      "training on iteration:3209    \t/3400, val loss:40.837928771972656\n",
      "training on iteration:3210    \t/3400, val loss:41.31521987915039\n",
      "training on iteration:3211    \t/3400, val loss:40.37249755859375\n",
      "training on iteration:3212    \t/3400, val loss:39.80970001220703\n",
      "training on iteration:3213    \t/3400, val loss:39.764404296875\n",
      "training on iteration:3214    \t/3400, val loss:39.88211441040039\n",
      "training on iteration:3215    \t/3400, val loss:40.70685577392578\n",
      "training on iteration:3216    \t/3400, val loss:40.88540267944336\n",
      "training on iteration:3217    \t/3400, val loss:40.0344123840332\n",
      "training on iteration:3218    \t/3400, val loss:39.69446563720703\n",
      "training on iteration:3219    \t/3400, val loss:39.74039077758789\n",
      "training on iteration:3220    \t/3400, val loss:40.18553924560547\n",
      "training on iteration:3221    \t/3400, val loss:41.1285400390625\n",
      "training on iteration:3222    \t/3400, val loss:40.908477783203125\n",
      "training on iteration:3223    \t/3400, val loss:40.0155143737793\n",
      "training on iteration:3224    \t/3400, val loss:39.97068405151367\n",
      "training on iteration:3225    \t/3400, val loss:40.006103515625\n",
      "training on iteration:3226    \t/3400, val loss:40.860511779785156\n",
      "training on iteration:3227    \t/3400, val loss:41.31741714477539\n",
      "training on iteration:3228    \t/3400, val loss:40.365455627441406\n",
      "training on iteration:3229    \t/3400, val loss:39.80006790161133\n",
      "training on iteration:3230    \t/3400, val loss:39.75265884399414\n",
      "training on iteration:3231    \t/3400, val loss:39.87462615966797\n",
      "training on iteration:3232    \t/3400, val loss:40.7862663269043\n",
      "training on iteration:3233    \t/3400, val loss:40.91515350341797\n",
      "training on iteration:3234    \t/3400, val loss:40.0198974609375\n",
      "training on iteration:3235    \t/3400, val loss:39.68682098388672\n",
      "training on iteration:3236    \t/3400, val loss:39.73650360107422\n",
      "training on iteration:3237    \t/3400, val loss:40.190086364746094\n",
      "training on iteration:3238    \t/3400, val loss:41.16799545288086\n",
      "training on iteration:3239    \t/3400, val loss:40.937660217285156\n",
      "training on iteration:3240    \t/3400, val loss:40.02134323120117\n",
      "training on iteration:3241    \t/3400, val loss:39.95720291137695\n",
      "training on iteration:3242    \t/3400, val loss:39.98389434814453\n",
      "training on iteration:3243    \t/3400, val loss:40.79827880859375\n",
      "training on iteration:3244    \t/3400, val loss:41.30431365966797\n",
      "training on iteration:3245    \t/3400, val loss:40.355430603027344\n",
      "training on iteration:3246    \t/3400, val loss:39.78994369506836\n",
      "training on iteration:3247    \t/3400, val loss:39.74938201904297\n",
      "training on iteration:3248    \t/3400, val loss:39.89093780517578\n",
      "training on iteration:3249    \t/3400, val loss:40.847599029541016\n",
      "training on iteration:3250    \t/3400, val loss:40.98368453979492\n",
      "training on iteration:3251    \t/3400, val loss:40.004886627197266\n",
      "training on iteration:3252    \t/3400, val loss:39.696144104003906\n",
      "training on iteration:3253    \t/3400, val loss:39.73782730102539\n",
      "training on iteration:3254    \t/3400, val loss:40.24375534057617\n",
      "training on iteration:3255    \t/3400, val loss:41.17850112915039\n",
      "training on iteration:3256    \t/3400, val loss:40.85300064086914\n",
      "training on iteration:3257    \t/3400, val loss:39.997314453125\n",
      "training on iteration:3258    \t/3400, val loss:39.96554183959961\n",
      "training on iteration:3259    \t/3400, val loss:39.98051071166992\n",
      "training on iteration:3260    \t/3400, val loss:40.86458969116211\n",
      "training on iteration:3261    \t/3400, val loss:41.29179000854492\n",
      "training on iteration:3262    \t/3400, val loss:40.26630401611328\n",
      "training on iteration:3263    \t/3400, val loss:39.7794189453125\n",
      "training on iteration:3264    \t/3400, val loss:39.7374382019043\n",
      "training on iteration:3265    \t/3400, val loss:39.92469024658203\n",
      "training on iteration:3266    \t/3400, val loss:40.88870620727539\n",
      "training on iteration:3267    \t/3400, val loss:40.86680221557617\n",
      "training on iteration:3268    \t/3400, val loss:39.93598556518555\n",
      "training on iteration:3269    \t/3400, val loss:39.70770263671875\n",
      "training on iteration:3270    \t/3400, val loss:39.76915740966797\n",
      "training on iteration:3271    \t/3400, val loss:40.28910446166992\n",
      "training on iteration:3272    \t/3400, val loss:40.979366302490234\n",
      "training on iteration:3273    \t/3400, val loss:40.61822509765625\n",
      "training on iteration:3274    \t/3400, val loss:39.932437896728516\n",
      "training on iteration:3275    \t/3400, val loss:39.90336990356445\n",
      "training on iteration:3276    \t/3400, val loss:40.01708221435547\n",
      "training on iteration:3277    \t/3400, val loss:40.8939094543457\n",
      "training on iteration:3278    \t/3400, val loss:41.04772186279297\n",
      "training on iteration:3279    \t/3400, val loss:40.15945816040039\n",
      "training on iteration:3280    \t/3400, val loss:39.79340744018555\n",
      "training on iteration:3281    \t/3400, val loss:39.744773864746094\n",
      "training on iteration:3282    \t/3400, val loss:39.98945999145508\n",
      "training on iteration:3283    \t/3400, val loss:40.70927047729492\n",
      "training on iteration:3284    \t/3400, val loss:40.66177749633789\n",
      "training on iteration:3285    \t/3400, val loss:39.919193267822266\n",
      "training on iteration:3286    \t/3400, val loss:39.70205307006836\n",
      "training on iteration:3287    \t/3400, val loss:39.78154373168945\n",
      "training on iteration:3288    \t/3400, val loss:40.29013442993164\n",
      "training on iteration:3289    \t/3400, val loss:40.82575225830078\n",
      "training on iteration:3290    \t/3400, val loss:40.49321365356445\n",
      "training on iteration:3291    \t/3400, val loss:39.91526412963867\n",
      "training on iteration:3292    \t/3400, val loss:39.88500213623047\n",
      "training on iteration:3293    \t/3400, val loss:40.059085845947266\n",
      "training on iteration:3294    \t/3400, val loss:40.766395568847656\n",
      "training on iteration:3295    \t/3400, val loss:40.8558349609375\n",
      "training on iteration:3296    \t/3400, val loss:40.13344955444336\n",
      "training on iteration:3297    \t/3400, val loss:39.81113052368164\n",
      "training on iteration:3298    \t/3400, val loss:39.76534652709961\n",
      "training on iteration:3299    \t/3400, val loss:40.01473617553711\n",
      "training on iteration:3300    \t/3400, val loss:40.65185546875\n",
      "training on iteration:3301    \t/3400, val loss:40.548133850097656\n",
      "training on iteration:3302    \t/3400, val loss:39.867149353027344\n",
      "training on iteration:3303    \t/3400, val loss:39.69339370727539\n",
      "training on iteration:3304    \t/3400, val loss:39.798030853271484\n",
      "training on iteration:3305    \t/3400, val loss:40.29854965209961\n",
      "training on iteration:3306    \t/3400, val loss:40.63815689086914\n",
      "training on iteration:3307    \t/3400, val loss:40.3253173828125\n",
      "training on iteration:3308    \t/3400, val loss:39.88801956176758\n",
      "training on iteration:3309    \t/3400, val loss:39.868377685546875\n",
      "training on iteration:3310    \t/3400, val loss:40.1485481262207\n",
      "training on iteration:3311    \t/3400, val loss:40.83319091796875\n",
      "training on iteration:3312    \t/3400, val loss:40.63035202026367\n",
      "training on iteration:3313    \t/3400, val loss:40.068721771240234\n",
      "training on iteration:3314    \t/3400, val loss:39.79810333251953\n",
      "training on iteration:3315    \t/3400, val loss:39.76576614379883\n",
      "training on iteration:3316    \t/3400, val loss:40.10415267944336\n",
      "training on iteration:3317    \t/3400, val loss:40.44829559326172\n",
      "training on iteration:3318    \t/3400, val loss:40.213844299316406\n",
      "training on iteration:3319    \t/3400, val loss:39.76276779174805\n",
      "training on iteration:3320    \t/3400, val loss:39.69870376586914\n",
      "training on iteration:3321    \t/3400, val loss:39.92686080932617\n",
      "training on iteration:3322    \t/3400, val loss:40.27749252319336\n",
      "training on iteration:3323    \t/3400, val loss:40.376808166503906\n",
      "training on iteration:3324    \t/3400, val loss:40.18880081176758\n",
      "training on iteration:3325    \t/3400, val loss:39.89826202392578\n",
      "training on iteration:3326    \t/3400, val loss:39.88016891479492\n",
      "training on iteration:3327    \t/3400, val loss:40.208839416503906\n",
      "training on iteration:3328    \t/3400, val loss:40.60719680786133\n",
      "training on iteration:3329    \t/3400, val loss:40.43544387817383\n",
      "training on iteration:3330    \t/3400, val loss:40.08199691772461\n",
      "training on iteration:3331    \t/3400, val loss:39.808963775634766\n",
      "training on iteration:3332    \t/3400, val loss:39.80409622192383\n",
      "training on iteration:3333    \t/3400, val loss:40.10511016845703\n",
      "training on iteration:3334    \t/3400, val loss:40.253055572509766\n",
      "training on iteration:3335    \t/3400, val loss:40.112037658691406\n",
      "training on iteration:3336    \t/3400, val loss:39.768985748291016\n",
      "training on iteration:3337    \t/3400, val loss:39.717247009277344\n",
      "training on iteration:3338    \t/3400, val loss:39.931026458740234\n",
      "training on iteration:3339    \t/3400, val loss:40.174861907958984\n",
      "training on iteration:3340    \t/3400, val loss:40.2949104309082\n",
      "training on iteration:3341    \t/3400, val loss:40.17097854614258\n",
      "training on iteration:3342    \t/3400, val loss:39.89873504638672\n",
      "training on iteration:3343    \t/3400, val loss:39.8706169128418\n",
      "training on iteration:3344    \t/3400, val loss:40.21689224243164\n",
      "training on iteration:3345    \t/3400, val loss:40.60678482055664\n",
      "training on iteration:3346    \t/3400, val loss:40.3869514465332\n",
      "training on iteration:3347    \t/3400, val loss:40.03553009033203\n",
      "training on iteration:3348    \t/3400, val loss:39.80557632446289\n",
      "training on iteration:3349    \t/3400, val loss:39.81951904296875\n",
      "training on iteration:3350    \t/3400, val loss:40.13075637817383\n",
      "training on iteration:3351    \t/3400, val loss:40.254825592041016\n",
      "training on iteration:3352    \t/3400, val loss:40.05352783203125\n",
      "training on iteration:3353    \t/3400, val loss:39.73637771606445\n",
      "training on iteration:3354    \t/3400, val loss:39.6976318359375\n",
      "training on iteration:3355    \t/3400, val loss:39.94430923461914\n",
      "training on iteration:3356    \t/3400, val loss:40.13292694091797\n",
      "training on iteration:3357    \t/3400, val loss:40.21968460083008\n",
      "training on iteration:3358    \t/3400, val loss:40.1147346496582\n",
      "training on iteration:3359    \t/3400, val loss:39.88908004760742\n",
      "training on iteration:3360    \t/3400, val loss:39.880706787109375\n",
      "training on iteration:3361    \t/3400, val loss:40.23512649536133\n",
      "training on iteration:3362    \t/3400, val loss:40.481719970703125\n",
      "training on iteration:3363    \t/3400, val loss:40.27265167236328\n",
      "training on iteration:3364    \t/3400, val loss:40.06659698486328\n",
      "training on iteration:3365    \t/3400, val loss:39.846160888671875\n",
      "training on iteration:3366    \t/3400, val loss:39.90167999267578\n",
      "training on iteration:3367    \t/3400, val loss:40.19845962524414\n",
      "training on iteration:3368    \t/3400, val loss:40.117652893066406\n",
      "training on iteration:3369    \t/3400, val loss:39.9024658203125\n",
      "training on iteration:3370    \t/3400, val loss:39.73499298095703\n",
      "training on iteration:3371    \t/3400, val loss:39.763858795166016\n",
      "training on iteration:3372    \t/3400, val loss:40.0621452331543\n",
      "training on iteration:3373    \t/3400, val loss:40.00728988647461\n",
      "training on iteration:3374    \t/3400, val loss:40.002384185791016\n",
      "training on iteration:3375    \t/3400, val loss:40.04839324951172\n",
      "training on iteration:3376    \t/3400, val loss:39.939552307128906\n",
      "training on iteration:3377    \t/3400, val loss:39.95835494995117\n",
      "training on iteration:3378    \t/3400, val loss:40.18072509765625\n",
      "training on iteration:3379    \t/3400, val loss:40.260704040527344\n",
      "training on iteration:3380    \t/3400, val loss:40.21235275268555\n",
      "training on iteration:3381    \t/3400, val loss:40.18125534057617\n",
      "training on iteration:3382    \t/3400, val loss:39.91643524169922\n",
      "training on iteration:3383    \t/3400, val loss:39.89981460571289\n",
      "training on iteration:3384    \t/3400, val loss:40.02979278564453\n",
      "training on iteration:3385    \t/3400, val loss:40.04525375366211\n",
      "training on iteration:3386    \t/3400, val loss:39.989723205566406\n",
      "training on iteration:3387    \t/3400, val loss:39.76284408569336\n",
      "training on iteration:3388    \t/3400, val loss:39.709556579589844\n",
      "training on iteration:3389    \t/3400, val loss:39.8811149597168\n",
      "training on iteration:3390    \t/3400, val loss:39.980987548828125\n",
      "training on iteration:3391    \t/3400, val loss:40.149253845214844\n",
      "training on iteration:3392    \t/3400, val loss:40.17436981201172\n",
      "training on iteration:3393    \t/3400, val loss:39.90674591064453\n",
      "training on iteration:3394    \t/3400, val loss:39.881927490234375\n",
      "training on iteration:3395    \t/3400, val loss:40.19060516357422\n",
      "training on iteration:3396    \t/3400, val loss:40.426841735839844\n",
      "training on iteration:3397    \t/3400, val loss:40.23363494873047\n",
      "training on iteration:3398    \t/3400, val loss:40.061126708984375\n",
      "training on iteration:3399    \t/3400, val loss:39.841468811035156\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_weight, train_loss_hist, val_loss_hist = train(model, optimizer, loss_fn, train_data_loader, [X_val, y_val], epoch=200)\n",
    "modelSaveDirectory = './state_dict/basic'\n",
    "torch.save(model_weight, f\"{modelSaveDirectory}.pth\")\n",
    "show_loss_stats(train_loss_hist, val_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_loss_stats(train_loss_hist, val_loss_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
