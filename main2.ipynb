{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Open        High         Low       Close   Adj Close  \\\n",
      "Date                                                                     \n",
      "2024-01-03  184.220001  185.880005  183.429993  184.250000  184.250000   \n",
      "2024-01-04  182.149994  183.089996  180.880005  181.910004  181.910004   \n",
      "2024-01-05  181.990005  182.759995  180.169998  181.179993  181.179993   \n",
      "2024-01-08  182.089996  185.600006  181.500000  185.559998  185.559998   \n",
      "2024-01-09  183.919998  185.149994  182.729996  185.139999  185.139999   \n",
      "2024-01-10  184.350006  186.399994  183.919998  186.190002  186.190002   \n",
      "2024-01-11  186.539993  187.050003  183.619995  185.589996  185.589996   \n",
      "2024-01-12  186.059998  186.740005  185.190002  185.919998  185.919998   \n",
      "2024-01-16  182.160004  184.259995  180.929993  183.630005  183.630005   \n",
      "2024-01-17  181.270004  182.929993  180.300003  182.679993  182.679993   \n",
      "\n",
      "              Volume  \n",
      "Date                  \n",
      "2024-01-03  58414500  \n",
      "2024-01-04  71983600  \n",
      "2024-01-05  62303300  \n",
      "2024-01-08  59144500  \n",
      "2024-01-09  42841800  \n",
      "2024-01-10  46792900  \n",
      "2024-01-11  49128400  \n",
      "2024-01-12  40444700  \n",
      "2024-01-16  65603000  \n",
      "2024-01-17  47317400  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ticker = \"AAPL\"\n",
    "start_date = \"2014-01-01\"\n",
    "end_date = \"2024-01-18\"\n",
    "\n",
    "# Download the historical data\n",
    "data = yf.download(ticker, start=start_date, end=end_date, interval='1d')\n",
    "#print(data.shape)\n",
    "print(data.iloc[-10:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.basic\n",
    "import utils.MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_size = 200\n",
    "forecast_size = 2\n",
    "val_percent = 0.05 #percent of data reserved as validation data\n",
    "batch_size = 128\n",
    "\n",
    "model = models.basic.Basic2(history_size, forecast_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2227, 8, 202])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor(data.iloc[:, 0:4].values).to(torch.float32)\n",
    "close = tensor[:, 3]\n",
    "\n",
    "MA100 = utils.MA.MA(close, 100)[:, None] # N - days + 1\n",
    "valid_days = MA100.shape[0]\n",
    "MA60 = utils.MA.MA(close, 60)[-valid_days:, None]\n",
    "MA20 = utils.MA.MA(close, 20)[-valid_days:, None]\n",
    "MA5 = utils.MA.MA(close, 5)[-valid_days:, None]\n",
    "tensor = tensor[-valid_days:, :]\n",
    "\n",
    "tensor = torch.cat((tensor, MA5, MA20, MA60, MA100), dim=1)\n",
    "#print(tensor.shape)\n",
    "\n",
    "tensor = tensor.unfold(0, history_size + forecast_size, 1) #(all_data, 8, hist+fore)\n",
    "print(tensor.shape)\n",
    "shape = tensor.shape\n",
    "\n",
    "size = tensor.shape[0]\n",
    "train_size = int(size * (1 - val_percent))\n",
    "shuffle_idx1 = torch.randperm(train_size)\n",
    "shuffle_idx2 = torch.arange(size - train_size) + train_size\n",
    "shuffle_idx = torch.cat((shuffle_idx1, shuffle_idx2))\n",
    "#shuffle = (tensor[shuffle_idx]).flatten(start_dim=-2) #shuffle data (all_data, hist+fore * 8)\n",
    "shuffle = tensor[shuffle_idx] #(N, 8, hist + fore)\n",
    "\n",
    "X_train = shuffle[:train_size, :, :history_size].flatten(start_dim=-2)\n",
    "y_train = shuffle[:train_size, 3, history_size:]\n",
    "\n",
    "X_val = shuffle[train_size:, :, :history_size].flatten(start_dim=-2)\n",
    "y_val = shuffle[train_size:, 3, history_size:]\n",
    "'''\n",
    "X_train = shuffle[:train_size, :history_size*8]\n",
    "y_train = shuffle[:train_size, history_size*8:].reshape((train_size, forecast_size, 8))[:, :, :4]\n",
    "X_val = shuffle[train_size:, :history_size*8]\n",
    "y_val = shuffle[train_size:, history_size*8:].reshape((size - train_size, forecast_size, 8))[:, :, :4]\n",
    "'''\n",
    "\n",
    "data_train = TensorDataset(X_train, y_train)\n",
    "train_data_loader = DataLoader(data_train, batch_size=batch_size)#, shuffle=True) #shuffle is intended before splitting train and val\n",
    "\n",
    "def MAPE_loss_fn(t, p): #truth and prediction\n",
    "    return torch.mean(100 * torch.abs(t-p)/torch.clamp(torch.abs(t), min=1e-8))\n",
    "loss_fn = MAPE_loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_data_loader, val_data, epoch = 1):\n",
    "    iteration_per_epoch = len(train_data_loader)\n",
    "    total_iteration = iteration_per_epoch * epoch\n",
    "    train_loss_hist = []\n",
    "    val_loss_hist = []\n",
    "    model.train()\n",
    "    X_val, y_val = val_data\n",
    "    for _e in range(epoch):\n",
    "        for t, (X, y) in enumerate(train_data_loader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            train_loss_hist.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iteration = _e*iteration_per_epoch + t\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(X_val)\n",
    "                val_loss = loss_fn(val_pred, y_val)\n",
    "                val_loss_hist.append(val_loss.item())\n",
    "            print(f'training on iteration:{iteration}    \\t/{total_iteration}, val loss:{val_loss.item()}')\n",
    "    model_weight = model.state_dict()\n",
    "    return model_weight, train_loss_hist, val_loss_hist\n",
    "\n",
    "def show_loss_stats(train_loss, val_loss):\n",
    "    x = range(len(train_loss))\n",
    "    plt.plot(x, train_loss, label='train_loss')\n",
    "    plt.plot(x, val_loss, label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on iteration:0    \t/3400, val loss:58.693241119384766\n",
      "training on iteration:1    \t/3400, val loss:34.58342361450195\n",
      "training on iteration:2    \t/3400, val loss:51.93033981323242\n",
      "training on iteration:3    \t/3400, val loss:60.07009506225586\n",
      "training on iteration:4    \t/3400, val loss:64.44955444335938\n",
      "training on iteration:5    \t/3400, val loss:67.01636505126953\n",
      "training on iteration:6    \t/3400, val loss:68.46025085449219\n",
      "training on iteration:7    \t/3400, val loss:69.03370666503906\n",
      "training on iteration:8    \t/3400, val loss:68.87689971923828\n",
      "training on iteration:9    \t/3400, val loss:67.93932342529297\n",
      "training on iteration:10    \t/3400, val loss:65.99433135986328\n",
      "training on iteration:11    \t/3400, val loss:62.333412170410156\n",
      "training on iteration:12    \t/3400, val loss:55.08646774291992\n",
      "training on iteration:13    \t/3400, val loss:49.457279205322266\n",
      "training on iteration:14    \t/3400, val loss:63.869972229003906\n",
      "training on iteration:15    \t/3400, val loss:45.80219650268555\n",
      "training on iteration:16    \t/3400, val loss:54.33946990966797\n",
      "training on iteration:17    \t/3400, val loss:58.98722457885742\n",
      "training on iteration:18    \t/3400, val loss:60.47327423095703\n",
      "training on iteration:19    \t/3400, val loss:59.77574920654297\n",
      "training on iteration:20    \t/3400, val loss:56.58718490600586\n",
      "training on iteration:21    \t/3400, val loss:48.65808868408203\n",
      "training on iteration:22    \t/3400, val loss:54.11887741088867\n",
      "training on iteration:23    \t/3400, val loss:53.71773910522461\n",
      "training on iteration:24    \t/3400, val loss:47.51049041748047\n",
      "training on iteration:25    \t/3400, val loss:53.17536163330078\n",
      "training on iteration:26    \t/3400, val loss:53.802494049072266\n",
      "training on iteration:27    \t/3400, val loss:50.212100982666016\n",
      "training on iteration:28    \t/3400, val loss:47.89181900024414\n",
      "training on iteration:29    \t/3400, val loss:48.17119598388672\n",
      "training on iteration:30    \t/3400, val loss:46.08333206176758\n",
      "training on iteration:31    \t/3400, val loss:48.8148078918457\n",
      "training on iteration:32    \t/3400, val loss:47.77260971069336\n",
      "training on iteration:33    \t/3400, val loss:45.93056106567383\n",
      "training on iteration:34    \t/3400, val loss:45.965576171875\n",
      "training on iteration:35    \t/3400, val loss:46.411338806152344\n",
      "training on iteration:36    \t/3400, val loss:48.84606170654297\n",
      "training on iteration:37    \t/3400, val loss:48.63865280151367\n",
      "training on iteration:38    \t/3400, val loss:45.882720947265625\n",
      "training on iteration:39    \t/3400, val loss:45.79187774658203\n",
      "training on iteration:40    \t/3400, val loss:46.068504333496094\n",
      "training on iteration:41    \t/3400, val loss:46.29655838012695\n",
      "training on iteration:42    \t/3400, val loss:45.71118927001953\n",
      "training on iteration:43    \t/3400, val loss:46.26427459716797\n",
      "training on iteration:44    \t/3400, val loss:46.796783447265625\n",
      "training on iteration:45    \t/3400, val loss:46.11138916015625\n",
      "training on iteration:46    \t/3400, val loss:46.02321243286133\n",
      "training on iteration:47    \t/3400, val loss:45.59738540649414\n",
      "training on iteration:48    \t/3400, val loss:45.595306396484375\n",
      "training on iteration:49    \t/3400, val loss:47.530521392822266\n",
      "training on iteration:50    \t/3400, val loss:46.805946350097656\n",
      "training on iteration:51    \t/3400, val loss:45.62179183959961\n",
      "training on iteration:52    \t/3400, val loss:45.48262405395508\n",
      "training on iteration:53    \t/3400, val loss:48.73931121826172\n",
      "training on iteration:54    \t/3400, val loss:48.6649284362793\n",
      "training on iteration:55    \t/3400, val loss:45.38037109375\n",
      "training on iteration:56    \t/3400, val loss:45.43633270263672\n",
      "training on iteration:57    \t/3400, val loss:47.1550178527832\n",
      "training on iteration:58    \t/3400, val loss:45.29832077026367\n",
      "training on iteration:59    \t/3400, val loss:45.38441848754883\n",
      "training on iteration:60    \t/3400, val loss:47.75481414794922\n",
      "training on iteration:61    \t/3400, val loss:45.454254150390625\n",
      "training on iteration:62    \t/3400, val loss:45.906673431396484\n",
      "training on iteration:63    \t/3400, val loss:45.500572204589844\n",
      "training on iteration:64    \t/3400, val loss:44.965431213378906\n",
      "training on iteration:65    \t/3400, val loss:45.10641098022461\n",
      "training on iteration:66    \t/3400, val loss:47.44020462036133\n",
      "training on iteration:67    \t/3400, val loss:44.7982063293457\n",
      "training on iteration:68    \t/3400, val loss:45.21216583251953\n",
      "training on iteration:69    \t/3400, val loss:47.791927337646484\n",
      "training on iteration:70    \t/3400, val loss:44.81660079956055\n",
      "training on iteration:71    \t/3400, val loss:46.33503341674805\n",
      "training on iteration:72    \t/3400, val loss:46.30727767944336\n",
      "training on iteration:73    \t/3400, val loss:44.427757263183594\n",
      "training on iteration:74    \t/3400, val loss:45.73020553588867\n",
      "training on iteration:75    \t/3400, val loss:44.36498260498047\n",
      "training on iteration:76    \t/3400, val loss:46.36062240600586\n",
      "training on iteration:77    \t/3400, val loss:44.2923469543457\n",
      "training on iteration:78    \t/3400, val loss:45.992919921875\n",
      "training on iteration:79    \t/3400, val loss:44.36672592163086\n",
      "training on iteration:80    \t/3400, val loss:44.53055953979492\n",
      "training on iteration:81    \t/3400, val loss:43.6712760925293\n",
      "training on iteration:82    \t/3400, val loss:44.958580017089844\n",
      "training on iteration:83    \t/3400, val loss:43.77477264404297\n",
      "training on iteration:84    \t/3400, val loss:43.82322311401367\n",
      "training on iteration:85    \t/3400, val loss:44.36758804321289\n",
      "training on iteration:86    \t/3400, val loss:42.974063873291016\n",
      "training on iteration:87    \t/3400, val loss:46.383506774902344\n",
      "training on iteration:88    \t/3400, val loss:42.556488037109375\n",
      "training on iteration:89    \t/3400, val loss:43.77285385131836\n",
      "training on iteration:90    \t/3400, val loss:42.04878616333008\n",
      "training on iteration:91    \t/3400, val loss:42.9622917175293\n",
      "training on iteration:92    \t/3400, val loss:41.42965316772461\n",
      "training on iteration:93    \t/3400, val loss:47.88361358642578\n",
      "training on iteration:94    \t/3400, val loss:44.697845458984375\n",
      "training on iteration:95    \t/3400, val loss:50.141414642333984\n",
      "training on iteration:96    \t/3400, val loss:43.8446159362793\n",
      "training on iteration:97    \t/3400, val loss:45.93071746826172\n",
      "training on iteration:98    \t/3400, val loss:39.010215759277344\n",
      "training on iteration:99    \t/3400, val loss:46.19388961791992\n",
      "training on iteration:100    \t/3400, val loss:37.946556091308594\n",
      "training on iteration:101    \t/3400, val loss:38.00925064086914\n",
      "training on iteration:102    \t/3400, val loss:27.188058853149414\n",
      "training on iteration:103    \t/3400, val loss:19.710891723632812\n",
      "training on iteration:104    \t/3400, val loss:42.54401779174805\n",
      "training on iteration:105    \t/3400, val loss:28.859399795532227\n",
      "training on iteration:106    \t/3400, val loss:32.731807708740234\n",
      "training on iteration:107    \t/3400, val loss:41.09207534790039\n",
      "training on iteration:108    \t/3400, val loss:35.89185333251953\n",
      "training on iteration:109    \t/3400, val loss:44.770503997802734\n",
      "training on iteration:110    \t/3400, val loss:46.091976165771484\n",
      "training on iteration:111    \t/3400, val loss:40.30474090576172\n",
      "training on iteration:112    \t/3400, val loss:47.84143829345703\n",
      "training on iteration:113    \t/3400, val loss:39.90789794921875\n",
      "training on iteration:114    \t/3400, val loss:48.28023910522461\n",
      "training on iteration:115    \t/3400, val loss:49.90848159790039\n",
      "training on iteration:116    \t/3400, val loss:46.6058349609375\n",
      "training on iteration:117    \t/3400, val loss:41.71549606323242\n",
      "training on iteration:118    \t/3400, val loss:42.45307540893555\n",
      "training on iteration:119    \t/3400, val loss:44.53081130981445\n",
      "training on iteration:120    \t/3400, val loss:45.303863525390625\n",
      "training on iteration:121    \t/3400, val loss:40.050514221191406\n",
      "training on iteration:122    \t/3400, val loss:39.45460891723633\n",
      "training on iteration:123    \t/3400, val loss:40.943599700927734\n",
      "training on iteration:124    \t/3400, val loss:41.012725830078125\n",
      "training on iteration:125    \t/3400, val loss:38.69986343383789\n",
      "training on iteration:126    \t/3400, val loss:38.34139633178711\n",
      "training on iteration:127    \t/3400, val loss:41.16038131713867\n",
      "training on iteration:128    \t/3400, val loss:38.451255798339844\n",
      "training on iteration:129    \t/3400, val loss:37.78873825073242\n",
      "training on iteration:130    \t/3400, val loss:37.819400787353516\n",
      "training on iteration:131    \t/3400, val loss:38.23827362060547\n",
      "training on iteration:132    \t/3400, val loss:32.46784973144531\n",
      "training on iteration:133    \t/3400, val loss:32.73226547241211\n",
      "training on iteration:134    \t/3400, val loss:29.726411819458008\n",
      "training on iteration:135    \t/3400, val loss:23.666898727416992\n",
      "training on iteration:136    \t/3400, val loss:7.7337775230407715\n",
      "training on iteration:137    \t/3400, val loss:64.67357635498047\n",
      "training on iteration:138    \t/3400, val loss:31.007171630859375\n",
      "training on iteration:139    \t/3400, val loss:44.318016052246094\n",
      "training on iteration:140    \t/3400, val loss:47.290924072265625\n",
      "training on iteration:141    \t/3400, val loss:44.07870864868164\n",
      "training on iteration:142    \t/3400, val loss:47.657718658447266\n",
      "training on iteration:143    \t/3400, val loss:45.271114349365234\n",
      "training on iteration:144    \t/3400, val loss:50.3300666809082\n",
      "training on iteration:145    \t/3400, val loss:54.4720573425293\n",
      "training on iteration:146    \t/3400, val loss:53.98920822143555\n",
      "training on iteration:147    \t/3400, val loss:48.550567626953125\n",
      "training on iteration:148    \t/3400, val loss:53.01732635498047\n",
      "training on iteration:149    \t/3400, val loss:47.50858688354492\n",
      "training on iteration:150    \t/3400, val loss:52.823280334472656\n",
      "training on iteration:151    \t/3400, val loss:56.78049087524414\n",
      "training on iteration:152    \t/3400, val loss:56.710411071777344\n",
      "training on iteration:153    \t/3400, val loss:53.1002082824707\n",
      "training on iteration:154    \t/3400, val loss:45.52469253540039\n",
      "training on iteration:155    \t/3400, val loss:49.616058349609375\n",
      "training on iteration:156    \t/3400, val loss:46.451141357421875\n",
      "training on iteration:157    \t/3400, val loss:52.592308044433594\n",
      "training on iteration:158    \t/3400, val loss:53.008087158203125\n",
      "training on iteration:159    \t/3400, val loss:48.523353576660156\n",
      "training on iteration:160    \t/3400, val loss:51.100643157958984\n",
      "training on iteration:161    \t/3400, val loss:47.862579345703125\n",
      "training on iteration:162    \t/3400, val loss:51.4476432800293\n",
      "training on iteration:163    \t/3400, val loss:54.61355209350586\n",
      "training on iteration:164    \t/3400, val loss:53.438865661621094\n",
      "training on iteration:165    \t/3400, val loss:47.66181182861328\n",
      "training on iteration:166    \t/3400, val loss:51.87797164916992\n",
      "training on iteration:167    \t/3400, val loss:48.1622314453125\n",
      "training on iteration:168    \t/3400, val loss:51.14641571044922\n",
      "training on iteration:169    \t/3400, val loss:54.14934539794922\n",
      "training on iteration:170    \t/3400, val loss:52.91764450073242\n",
      "training on iteration:171    \t/3400, val loss:46.925960540771484\n",
      "training on iteration:172    \t/3400, val loss:49.06882858276367\n",
      "training on iteration:173    \t/3400, val loss:45.799556732177734\n",
      "training on iteration:174    \t/3400, val loss:50.57207107543945\n",
      "training on iteration:175    \t/3400, val loss:51.899715423583984\n",
      "training on iteration:176    \t/3400, val loss:48.171634674072266\n",
      "training on iteration:177    \t/3400, val loss:49.327518463134766\n",
      "training on iteration:178    \t/3400, val loss:46.542564392089844\n",
      "training on iteration:179    \t/3400, val loss:50.9776611328125\n",
      "training on iteration:180    \t/3400, val loss:53.45718765258789\n",
      "training on iteration:181    \t/3400, val loss:51.53630828857422\n",
      "training on iteration:182    \t/3400, val loss:45.66485595703125\n",
      "training on iteration:183    \t/3400, val loss:49.627967834472656\n",
      "training on iteration:184    \t/3400, val loss:45.41376876831055\n",
      "training on iteration:185    \t/3400, val loss:50.9476432800293\n",
      "training on iteration:186    \t/3400, val loss:52.06422805786133\n",
      "training on iteration:187    \t/3400, val loss:48.60143280029297\n",
      "training on iteration:188    \t/3400, val loss:48.185142517089844\n",
      "training on iteration:189    \t/3400, val loss:47.576316833496094\n",
      "training on iteration:190    \t/3400, val loss:48.88112258911133\n",
      "training on iteration:191    \t/3400, val loss:51.477149963378906\n",
      "training on iteration:192    \t/3400, val loss:49.4216194152832\n",
      "training on iteration:193    \t/3400, val loss:45.36517333984375\n",
      "training on iteration:194    \t/3400, val loss:46.97124481201172\n",
      "training on iteration:195    \t/3400, val loss:47.087520599365234\n",
      "training on iteration:196    \t/3400, val loss:49.58675003051758\n",
      "training on iteration:197    \t/3400, val loss:46.87590408325195\n",
      "training on iteration:198    \t/3400, val loss:45.325740814208984\n",
      "training on iteration:199    \t/3400, val loss:45.23828125\n",
      "training on iteration:200    \t/3400, val loss:47.11211013793945\n",
      "training on iteration:201    \t/3400, val loss:46.241844177246094\n",
      "training on iteration:202    \t/3400, val loss:44.90080642700195\n",
      "training on iteration:203    \t/3400, val loss:44.95714569091797\n",
      "training on iteration:204    \t/3400, val loss:46.179927825927734\n",
      "training on iteration:205    \t/3400, val loss:46.340057373046875\n",
      "training on iteration:206    \t/3400, val loss:45.245460510253906\n",
      "training on iteration:207    \t/3400, val loss:45.3441276550293\n",
      "training on iteration:208    \t/3400, val loss:46.70197677612305\n",
      "training on iteration:209    \t/3400, val loss:45.31524658203125\n",
      "training on iteration:210    \t/3400, val loss:44.643367767333984\n",
      "training on iteration:211    \t/3400, val loss:44.91707992553711\n",
      "training on iteration:212    \t/3400, val loss:47.33055877685547\n",
      "training on iteration:213    \t/3400, val loss:46.32136917114258\n",
      "training on iteration:214    \t/3400, val loss:44.48580551147461\n",
      "training on iteration:215    \t/3400, val loss:45.38489532470703\n",
      "training on iteration:216    \t/3400, val loss:46.654441833496094\n",
      "training on iteration:217    \t/3400, val loss:44.854774475097656\n",
      "training on iteration:218    \t/3400, val loss:44.35683059692383\n",
      "training on iteration:219    \t/3400, val loss:45.18336868286133\n",
      "training on iteration:220    \t/3400, val loss:46.32034683227539\n",
      "training on iteration:221    \t/3400, val loss:44.5530891418457\n",
      "training on iteration:222    \t/3400, val loss:44.215511322021484\n",
      "training on iteration:223    \t/3400, val loss:45.12650680541992\n",
      "training on iteration:224    \t/3400, val loss:46.88158416748047\n",
      "training on iteration:225    \t/3400, val loss:44.5738410949707\n",
      "training on iteration:226    \t/3400, val loss:43.95669937133789\n",
      "training on iteration:227    \t/3400, val loss:44.328033447265625\n",
      "training on iteration:228    \t/3400, val loss:45.6795539855957\n",
      "training on iteration:229    \t/3400, val loss:44.18285369873047\n",
      "training on iteration:230    \t/3400, val loss:43.909690856933594\n",
      "training on iteration:231    \t/3400, val loss:45.709800720214844\n",
      "training on iteration:232    \t/3400, val loss:45.51675033569336\n",
      "training on iteration:233    \t/3400, val loss:43.5601692199707\n",
      "training on iteration:234    \t/3400, val loss:43.62430953979492\n",
      "training on iteration:235    \t/3400, val loss:44.2320671081543\n",
      "training on iteration:236    \t/3400, val loss:43.8629035949707\n",
      "training on iteration:237    \t/3400, val loss:43.7051887512207\n",
      "training on iteration:238    \t/3400, val loss:43.56846237182617\n",
      "training on iteration:239    \t/3400, val loss:43.74509811401367\n",
      "training on iteration:240    \t/3400, val loss:44.06171417236328\n",
      "training on iteration:241    \t/3400, val loss:43.89630126953125\n",
      "training on iteration:242    \t/3400, val loss:43.49790573120117\n",
      "training on iteration:243    \t/3400, val loss:42.99628829956055\n",
      "training on iteration:244    \t/3400, val loss:42.90839385986328\n",
      "training on iteration:245    \t/3400, val loss:43.47597885131836\n",
      "training on iteration:246    \t/3400, val loss:43.03544998168945\n",
      "training on iteration:247    \t/3400, val loss:42.40264892578125\n",
      "training on iteration:248    \t/3400, val loss:43.47107696533203\n",
      "training on iteration:249    \t/3400, val loss:44.11159896850586\n",
      "training on iteration:250    \t/3400, val loss:41.90283203125\n",
      "training on iteration:251    \t/3400, val loss:42.00408172607422\n",
      "training on iteration:252    \t/3400, val loss:42.36532211303711\n",
      "training on iteration:253    \t/3400, val loss:41.554054260253906\n",
      "training on iteration:254    \t/3400, val loss:41.729888916015625\n",
      "training on iteration:255    \t/3400, val loss:41.68927001953125\n",
      "training on iteration:256    \t/3400, val loss:40.85662078857422\n",
      "training on iteration:257    \t/3400, val loss:41.26735305786133\n",
      "training on iteration:258    \t/3400, val loss:42.27984619140625\n",
      "training on iteration:259    \t/3400, val loss:40.286869049072266\n",
      "training on iteration:260    \t/3400, val loss:39.59784698486328\n",
      "training on iteration:261    \t/3400, val loss:40.18390655517578\n",
      "training on iteration:262    \t/3400, val loss:39.79081344604492\n",
      "training on iteration:263    \t/3400, val loss:38.621788024902344\n",
      "training on iteration:264    \t/3400, val loss:39.73154067993164\n",
      "training on iteration:265    \t/3400, val loss:37.916839599609375\n",
      "training on iteration:266    \t/3400, val loss:37.943363189697266\n",
      "training on iteration:267    \t/3400, val loss:36.618473052978516\n",
      "training on iteration:268    \t/3400, val loss:35.22233200073242\n",
      "training on iteration:269    \t/3400, val loss:34.58970260620117\n",
      "training on iteration:270    \t/3400, val loss:34.128334045410156\n",
      "training on iteration:271    \t/3400, val loss:31.229656219482422\n",
      "training on iteration:272    \t/3400, val loss:29.1030330657959\n",
      "training on iteration:273    \t/3400, val loss:27.476333618164062\n",
      "training on iteration:274    \t/3400, val loss:19.903839111328125\n",
      "training on iteration:275    \t/3400, val loss:7.725335121154785\n",
      "training on iteration:276    \t/3400, val loss:41.863006591796875\n",
      "training on iteration:277    \t/3400, val loss:7.521328926086426\n",
      "training on iteration:278    \t/3400, val loss:15.395601272583008\n",
      "training on iteration:279    \t/3400, val loss:18.97312355041504\n",
      "training on iteration:280    \t/3400, val loss:21.664575576782227\n",
      "training on iteration:281    \t/3400, val loss:16.407039642333984\n",
      "training on iteration:282    \t/3400, val loss:9.176945686340332\n",
      "training on iteration:283    \t/3400, val loss:18.90839195251465\n",
      "training on iteration:284    \t/3400, val loss:11.628308296203613\n",
      "training on iteration:285    \t/3400, val loss:12.355268478393555\n",
      "training on iteration:286    \t/3400, val loss:17.635663986206055\n",
      "training on iteration:287    \t/3400, val loss:23.822078704833984\n",
      "training on iteration:288    \t/3400, val loss:21.192794799804688\n",
      "training on iteration:289    \t/3400, val loss:16.204389572143555\n",
      "training on iteration:290    \t/3400, val loss:4.770223140716553\n",
      "training on iteration:291    \t/3400, val loss:16.042673110961914\n",
      "training on iteration:292    \t/3400, val loss:17.720687866210938\n",
      "training on iteration:293    \t/3400, val loss:19.889698028564453\n",
      "training on iteration:294    \t/3400, val loss:9.554100036621094\n",
      "training on iteration:295    \t/3400, val loss:15.152460098266602\n",
      "training on iteration:296    \t/3400, val loss:8.007025718688965\n",
      "training on iteration:297    \t/3400, val loss:17.659826278686523\n",
      "training on iteration:298    \t/3400, val loss:15.928106307983398\n",
      "training on iteration:299    \t/3400, val loss:18.047809600830078\n",
      "training on iteration:300    \t/3400, val loss:3.9296209812164307\n",
      "training on iteration:301    \t/3400, val loss:14.636148452758789\n",
      "training on iteration:302    \t/3400, val loss:18.004528045654297\n",
      "training on iteration:303    \t/3400, val loss:19.462570190429688\n",
      "training on iteration:304    \t/3400, val loss:14.260255813598633\n",
      "training on iteration:305    \t/3400, val loss:6.5108513832092285\n",
      "training on iteration:306    \t/3400, val loss:20.565427780151367\n",
      "training on iteration:307    \t/3400, val loss:10.46307373046875\n",
      "training on iteration:308    \t/3400, val loss:13.366265296936035\n",
      "training on iteration:309    \t/3400, val loss:18.603899002075195\n",
      "training on iteration:310    \t/3400, val loss:21.4064884185791\n",
      "training on iteration:311    \t/3400, val loss:20.724788665771484\n",
      "training on iteration:312    \t/3400, val loss:17.75857162475586\n",
      "training on iteration:313    \t/3400, val loss:12.973479270935059\n",
      "training on iteration:314    \t/3400, val loss:6.103362083435059\n",
      "training on iteration:315    \t/3400, val loss:9.121211051940918\n",
      "training on iteration:316    \t/3400, val loss:7.475815296173096\n",
      "training on iteration:317    \t/3400, val loss:9.535988807678223\n",
      "training on iteration:318    \t/3400, val loss:4.311739921569824\n",
      "training on iteration:319    \t/3400, val loss:8.164823532104492\n",
      "training on iteration:320    \t/3400, val loss:7.016417980194092\n",
      "training on iteration:321    \t/3400, val loss:6.576550483703613\n",
      "training on iteration:322    \t/3400, val loss:3.740438461303711\n",
      "training on iteration:323    \t/3400, val loss:5.026034832000732\n",
      "training on iteration:324    \t/3400, val loss:5.77489709854126\n",
      "training on iteration:325    \t/3400, val loss:5.268196105957031\n",
      "training on iteration:326    \t/3400, val loss:5.189755916595459\n",
      "training on iteration:327    \t/3400, val loss:7.254183769226074\n",
      "training on iteration:328    \t/3400, val loss:3.5718629360198975\n",
      "training on iteration:329    \t/3400, val loss:4.432406425476074\n",
      "training on iteration:330    \t/3400, val loss:6.491171836853027\n",
      "training on iteration:331    \t/3400, val loss:3.948169708251953\n",
      "training on iteration:332    \t/3400, val loss:7.621776580810547\n",
      "training on iteration:333    \t/3400, val loss:4.8545684814453125\n",
      "training on iteration:334    \t/3400, val loss:6.481183052062988\n",
      "training on iteration:335    \t/3400, val loss:6.1434454917907715\n",
      "training on iteration:336    \t/3400, val loss:5.1417741775512695\n",
      "training on iteration:337    \t/3400, val loss:4.430957317352295\n",
      "training on iteration:338    \t/3400, val loss:6.706137657165527\n",
      "training on iteration:339    \t/3400, val loss:4.262333393096924\n",
      "training on iteration:340    \t/3400, val loss:5.177531719207764\n",
      "training on iteration:341    \t/3400, val loss:5.816955089569092\n",
      "training on iteration:342    \t/3400, val loss:5.170748233795166\n",
      "training on iteration:343    \t/3400, val loss:6.582770824432373\n",
      "training on iteration:344    \t/3400, val loss:4.726108074188232\n",
      "training on iteration:345    \t/3400, val loss:4.0541229248046875\n",
      "training on iteration:346    \t/3400, val loss:6.634984493255615\n",
      "training on iteration:347    \t/3400, val loss:3.8651695251464844\n",
      "training on iteration:348    \t/3400, val loss:7.743825435638428\n",
      "training on iteration:349    \t/3400, val loss:4.279627323150635\n",
      "training on iteration:350    \t/3400, val loss:5.449681758880615\n",
      "training on iteration:351    \t/3400, val loss:7.4947099685668945\n",
      "training on iteration:352    \t/3400, val loss:3.9989333152770996\n",
      "training on iteration:353    \t/3400, val loss:8.462430953979492\n",
      "training on iteration:354    \t/3400, val loss:4.143718719482422\n",
      "training on iteration:355    \t/3400, val loss:4.173696994781494\n",
      "training on iteration:356    \t/3400, val loss:10.759994506835938\n",
      "training on iteration:357    \t/3400, val loss:4.1859660148620605\n",
      "training on iteration:358    \t/3400, val loss:3.8636605739593506\n",
      "training on iteration:359    \t/3400, val loss:11.607787132263184\n",
      "training on iteration:360    \t/3400, val loss:5.9596171379089355\n",
      "training on iteration:361    \t/3400, val loss:5.851420879364014\n",
      "training on iteration:362    \t/3400, val loss:8.293389320373535\n",
      "training on iteration:363    \t/3400, val loss:7.66658878326416\n",
      "training on iteration:364    \t/3400, val loss:10.371809959411621\n",
      "training on iteration:365    \t/3400, val loss:6.417230129241943\n",
      "training on iteration:366    \t/3400, val loss:8.676424026489258\n",
      "training on iteration:367    \t/3400, val loss:4.597725868225098\n",
      "training on iteration:368    \t/3400, val loss:8.58045768737793\n",
      "training on iteration:369    \t/3400, val loss:8.092979431152344\n",
      "training on iteration:370    \t/3400, val loss:3.8811397552490234\n",
      "training on iteration:371    \t/3400, val loss:6.163458347320557\n",
      "training on iteration:372    \t/3400, val loss:7.4810028076171875\n",
      "training on iteration:373    \t/3400, val loss:7.800305366516113\n",
      "training on iteration:374    \t/3400, val loss:5.859044551849365\n",
      "training on iteration:375    \t/3400, val loss:7.357627868652344\n",
      "training on iteration:376    \t/3400, val loss:6.792667865753174\n",
      "training on iteration:377    \t/3400, val loss:6.008788585662842\n",
      "training on iteration:378    \t/3400, val loss:6.579959869384766\n",
      "training on iteration:379    \t/3400, val loss:3.902371644973755\n",
      "training on iteration:380    \t/3400, val loss:8.239463806152344\n",
      "training on iteration:381    \t/3400, val loss:8.497184753417969\n",
      "training on iteration:382    \t/3400, val loss:4.6084394454956055\n",
      "training on iteration:383    \t/3400, val loss:8.382813453674316\n",
      "training on iteration:384    \t/3400, val loss:7.949587345123291\n",
      "training on iteration:385    \t/3400, val loss:11.532021522521973\n",
      "training on iteration:386    \t/3400, val loss:6.969898700714111\n",
      "training on iteration:387    \t/3400, val loss:11.7381010055542\n",
      "training on iteration:388    \t/3400, val loss:11.66673755645752\n",
      "training on iteration:389    \t/3400, val loss:5.876372814178467\n",
      "training on iteration:390    \t/3400, val loss:14.451047897338867\n",
      "training on iteration:391    \t/3400, val loss:16.113378524780273\n",
      "training on iteration:392    \t/3400, val loss:16.21152687072754\n",
      "training on iteration:393    \t/3400, val loss:16.25296974182129\n",
      "training on iteration:394    \t/3400, val loss:6.497692584991455\n",
      "training on iteration:395    \t/3400, val loss:17.45235252380371\n",
      "training on iteration:396    \t/3400, val loss:15.886616706848145\n",
      "training on iteration:397    \t/3400, val loss:16.561349868774414\n",
      "training on iteration:398    \t/3400, val loss:14.745203971862793\n",
      "training on iteration:399    \t/3400, val loss:19.703622817993164\n",
      "training on iteration:400    \t/3400, val loss:16.537961959838867\n",
      "training on iteration:401    \t/3400, val loss:9.822097778320312\n",
      "training on iteration:402    \t/3400, val loss:17.200448989868164\n",
      "training on iteration:403    \t/3400, val loss:12.692848205566406\n",
      "training on iteration:404    \t/3400, val loss:16.4436092376709\n",
      "training on iteration:405    \t/3400, val loss:14.933297157287598\n",
      "training on iteration:406    \t/3400, val loss:23.21938705444336\n",
      "training on iteration:407    \t/3400, val loss:8.436372756958008\n",
      "training on iteration:408    \t/3400, val loss:17.617280960083008\n",
      "training on iteration:409    \t/3400, val loss:21.175312042236328\n",
      "training on iteration:410    \t/3400, val loss:26.091066360473633\n",
      "training on iteration:411    \t/3400, val loss:28.00742530822754\n",
      "training on iteration:412    \t/3400, val loss:21.838247299194336\n",
      "training on iteration:413    \t/3400, val loss:21.193893432617188\n",
      "training on iteration:414    \t/3400, val loss:8.820051193237305\n",
      "training on iteration:415    \t/3400, val loss:24.284053802490234\n",
      "training on iteration:416    \t/3400, val loss:18.10426902770996\n",
      "training on iteration:417    \t/3400, val loss:26.792211532592773\n",
      "training on iteration:418    \t/3400, val loss:31.417360305786133\n",
      "training on iteration:419    \t/3400, val loss:28.747650146484375\n",
      "training on iteration:420    \t/3400, val loss:20.39609718322754\n",
      "training on iteration:421    \t/3400, val loss:25.460086822509766\n",
      "training on iteration:422    \t/3400, val loss:14.615547180175781\n",
      "training on iteration:423    \t/3400, val loss:13.667694091796875\n",
      "training on iteration:424    \t/3400, val loss:31.917301177978516\n",
      "training on iteration:425    \t/3400, val loss:20.766660690307617\n",
      "training on iteration:426    \t/3400, val loss:19.09017562866211\n",
      "training on iteration:427    \t/3400, val loss:18.490447998046875\n",
      "training on iteration:428    \t/3400, val loss:25.080184936523438\n",
      "training on iteration:429    \t/3400, val loss:22.117280960083008\n",
      "training on iteration:430    \t/3400, val loss:26.109622955322266\n",
      "training on iteration:431    \t/3400, val loss:23.803462982177734\n",
      "training on iteration:432    \t/3400, val loss:15.601323127746582\n",
      "training on iteration:433    \t/3400, val loss:7.093573570251465\n",
      "training on iteration:434    \t/3400, val loss:22.899168014526367\n",
      "training on iteration:435    \t/3400, val loss:16.93319320678711\n",
      "training on iteration:436    \t/3400, val loss:16.702035903930664\n",
      "training on iteration:437    \t/3400, val loss:17.291318893432617\n",
      "training on iteration:438    \t/3400, val loss:23.680126190185547\n",
      "training on iteration:439    \t/3400, val loss:19.788869857788086\n",
      "training on iteration:440    \t/3400, val loss:24.310415267944336\n",
      "training on iteration:441    \t/3400, val loss:21.923189163208008\n",
      "training on iteration:442    \t/3400, val loss:8.24526309967041\n",
      "training on iteration:443    \t/3400, val loss:36.73139572143555\n",
      "training on iteration:444    \t/3400, val loss:8.030547142028809\n",
      "training on iteration:445    \t/3400, val loss:25.240453720092773\n",
      "training on iteration:446    \t/3400, val loss:35.616539001464844\n",
      "training on iteration:447    \t/3400, val loss:39.235313415527344\n",
      "training on iteration:448    \t/3400, val loss:38.92726516723633\n",
      "training on iteration:449    \t/3400, val loss:34.68175506591797\n",
      "training on iteration:450    \t/3400, val loss:26.439828872680664\n",
      "training on iteration:451    \t/3400, val loss:32.28300094604492\n",
      "training on iteration:452    \t/3400, val loss:24.897502899169922\n",
      "training on iteration:453    \t/3400, val loss:29.577442169189453\n",
      "training on iteration:454    \t/3400, val loss:29.57187271118164\n",
      "training on iteration:455    \t/3400, val loss:22.46207618713379\n",
      "training on iteration:456    \t/3400, val loss:9.357762336730957\n",
      "training on iteration:457    \t/3400, val loss:19.390422821044922\n",
      "training on iteration:458    \t/3400, val loss:4.841403484344482\n",
      "training on iteration:459    \t/3400, val loss:13.250638008117676\n",
      "training on iteration:460    \t/3400, val loss:10.767305374145508\n",
      "training on iteration:461    \t/3400, val loss:5.715365409851074\n",
      "training on iteration:462    \t/3400, val loss:3.454432964324951\n",
      "training on iteration:463    \t/3400, val loss:12.903433799743652\n",
      "training on iteration:464    \t/3400, val loss:11.737611770629883\n",
      "training on iteration:465    \t/3400, val loss:6.329486846923828\n",
      "training on iteration:466    \t/3400, val loss:3.9485416412353516\n",
      "training on iteration:467    \t/3400, val loss:12.52843189239502\n",
      "training on iteration:468    \t/3400, val loss:12.559136390686035\n",
      "training on iteration:469    \t/3400, val loss:7.381350517272949\n",
      "training on iteration:470    \t/3400, val loss:11.310589790344238\n",
      "training on iteration:471    \t/3400, val loss:9.268908500671387\n",
      "training on iteration:472    \t/3400, val loss:18.643280029296875\n",
      "training on iteration:473    \t/3400, val loss:18.812347412109375\n",
      "training on iteration:474    \t/3400, val loss:8.412596702575684\n",
      "training on iteration:475    \t/3400, val loss:21.467655181884766\n",
      "training on iteration:476    \t/3400, val loss:9.495993614196777\n",
      "training on iteration:477    \t/3400, val loss:18.00588035583496\n",
      "training on iteration:478    \t/3400, val loss:26.091461181640625\n",
      "training on iteration:479    \t/3400, val loss:26.373157501220703\n",
      "training on iteration:480    \t/3400, val loss:19.871015548706055\n",
      "training on iteration:481    \t/3400, val loss:17.459074020385742\n",
      "training on iteration:482    \t/3400, val loss:11.581281661987305\n",
      "training on iteration:483    \t/3400, val loss:8.095072746276855\n",
      "training on iteration:484    \t/3400, val loss:11.360400199890137\n",
      "training on iteration:485    \t/3400, val loss:15.374262809753418\n",
      "training on iteration:486    \t/3400, val loss:9.3360595703125\n",
      "training on iteration:487    \t/3400, val loss:3.432060718536377\n",
      "training on iteration:488    \t/3400, val loss:9.00766372680664\n",
      "training on iteration:489    \t/3400, val loss:6.11639404296875\n",
      "training on iteration:490    \t/3400, val loss:5.962947845458984\n",
      "training on iteration:491    \t/3400, val loss:6.899785041809082\n",
      "training on iteration:492    \t/3400, val loss:7.4399189949035645\n",
      "training on iteration:493    \t/3400, val loss:6.754897117614746\n",
      "training on iteration:494    \t/3400, val loss:4.966624736785889\n",
      "training on iteration:495    \t/3400, val loss:8.533047676086426\n",
      "training on iteration:496    \t/3400, val loss:8.826008796691895\n",
      "training on iteration:497    \t/3400, val loss:6.301310062408447\n",
      "training on iteration:498    \t/3400, val loss:4.359535217285156\n",
      "training on iteration:499    \t/3400, val loss:7.3425750732421875\n",
      "training on iteration:500    \t/3400, val loss:5.163142681121826\n",
      "training on iteration:501    \t/3400, val loss:7.872833728790283\n",
      "training on iteration:502    \t/3400, val loss:4.631937503814697\n",
      "training on iteration:503    \t/3400, val loss:8.237275123596191\n",
      "training on iteration:504    \t/3400, val loss:8.427528381347656\n",
      "training on iteration:505    \t/3400, val loss:4.894261360168457\n",
      "training on iteration:506    \t/3400, val loss:8.241253852844238\n",
      "training on iteration:507    \t/3400, val loss:4.148366451263428\n",
      "training on iteration:508    \t/3400, val loss:8.314728736877441\n",
      "training on iteration:509    \t/3400, val loss:7.541863918304443\n",
      "training on iteration:510    \t/3400, val loss:3.723938465118408\n",
      "training on iteration:511    \t/3400, val loss:5.816486835479736\n",
      "training on iteration:512    \t/3400, val loss:8.83798599243164\n",
      "training on iteration:513    \t/3400, val loss:5.282063007354736\n",
      "training on iteration:514    \t/3400, val loss:3.855668783187866\n",
      "training on iteration:515    \t/3400, val loss:5.161282062530518\n",
      "training on iteration:516    \t/3400, val loss:5.31710958480835\n",
      "training on iteration:517    \t/3400, val loss:3.603627920150757\n",
      "training on iteration:518    \t/3400, val loss:4.632735252380371\n",
      "training on iteration:519    \t/3400, val loss:5.311948299407959\n",
      "training on iteration:520    \t/3400, val loss:4.58126163482666\n",
      "training on iteration:521    \t/3400, val loss:8.423561096191406\n",
      "training on iteration:522    \t/3400, val loss:4.836400032043457\n",
      "training on iteration:523    \t/3400, val loss:4.852244853973389\n",
      "training on iteration:524    \t/3400, val loss:6.674565315246582\n",
      "training on iteration:525    \t/3400, val loss:5.39462947845459\n",
      "training on iteration:526    \t/3400, val loss:4.565957546234131\n",
      "training on iteration:527    \t/3400, val loss:5.6584553718566895\n",
      "training on iteration:528    \t/3400, val loss:7.071690082550049\n",
      "training on iteration:529    \t/3400, val loss:5.121796607971191\n",
      "training on iteration:530    \t/3400, val loss:4.630581378936768\n",
      "training on iteration:531    \t/3400, val loss:7.7984466552734375\n",
      "training on iteration:532    \t/3400, val loss:3.614366054534912\n",
      "training on iteration:533    \t/3400, val loss:3.5727486610412598\n",
      "training on iteration:534    \t/3400, val loss:9.642558097839355\n",
      "training on iteration:535    \t/3400, val loss:4.550364971160889\n",
      "training on iteration:536    \t/3400, val loss:4.579058647155762\n",
      "training on iteration:537    \t/3400, val loss:5.984827518463135\n",
      "training on iteration:538    \t/3400, val loss:6.76520299911499\n",
      "training on iteration:539    \t/3400, val loss:4.635875225067139\n",
      "training on iteration:540    \t/3400, val loss:4.488686561584473\n",
      "training on iteration:541    \t/3400, val loss:4.8541178703308105\n",
      "training on iteration:542    \t/3400, val loss:4.199324607849121\n",
      "training on iteration:543    \t/3400, val loss:6.578351020812988\n",
      "training on iteration:544    \t/3400, val loss:3.5174756050109863\n",
      "training on iteration:545    \t/3400, val loss:3.7273828983306885\n",
      "training on iteration:546    \t/3400, val loss:7.569735050201416\n",
      "training on iteration:547    \t/3400, val loss:4.622496604919434\n",
      "training on iteration:548    \t/3400, val loss:3.621094226837158\n",
      "training on iteration:549    \t/3400, val loss:5.4269700050354\n",
      "training on iteration:550    \t/3400, val loss:3.7086634635925293\n",
      "training on iteration:551    \t/3400, val loss:4.284928321838379\n",
      "training on iteration:552    \t/3400, val loss:5.531539440155029\n",
      "training on iteration:553    \t/3400, val loss:3.607344388961792\n",
      "training on iteration:554    \t/3400, val loss:3.826127529144287\n",
      "training on iteration:555    \t/3400, val loss:8.799312591552734\n",
      "training on iteration:556    \t/3400, val loss:5.516925811767578\n",
      "training on iteration:557    \t/3400, val loss:3.4871304035186768\n",
      "training on iteration:558    \t/3400, val loss:7.077424049377441\n",
      "training on iteration:559    \t/3400, val loss:6.368442058563232\n",
      "training on iteration:560    \t/3400, val loss:8.709305763244629\n",
      "training on iteration:561    \t/3400, val loss:4.910480976104736\n",
      "training on iteration:562    \t/3400, val loss:10.327120780944824\n",
      "training on iteration:563    \t/3400, val loss:10.498931884765625\n",
      "training on iteration:564    \t/3400, val loss:7.1364240646362305\n",
      "training on iteration:565    \t/3400, val loss:8.482217788696289\n",
      "training on iteration:566    \t/3400, val loss:8.693414688110352\n",
      "training on iteration:567    \t/3400, val loss:8.24050521850586\n",
      "training on iteration:568    \t/3400, val loss:10.261940002441406\n",
      "training on iteration:569    \t/3400, val loss:4.566451072692871\n",
      "training on iteration:570    \t/3400, val loss:8.03918743133545\n",
      "training on iteration:571    \t/3400, val loss:10.035097122192383\n",
      "training on iteration:572    \t/3400, val loss:10.386914253234863\n",
      "training on iteration:573    \t/3400, val loss:4.442803859710693\n",
      "training on iteration:574    \t/3400, val loss:4.816634654998779\n",
      "training on iteration:575    \t/3400, val loss:6.964314937591553\n",
      "training on iteration:576    \t/3400, val loss:6.278449058532715\n",
      "training on iteration:577    \t/3400, val loss:3.5008583068847656\n",
      "training on iteration:578    \t/3400, val loss:6.495231628417969\n",
      "training on iteration:579    \t/3400, val loss:4.003596782684326\n",
      "training on iteration:580    \t/3400, val loss:5.486540794372559\n",
      "training on iteration:581    \t/3400, val loss:7.111667633056641\n",
      "training on iteration:582    \t/3400, val loss:3.990114212036133\n",
      "training on iteration:583    \t/3400, val loss:4.79098653793335\n",
      "training on iteration:584    \t/3400, val loss:7.059067726135254\n",
      "training on iteration:585    \t/3400, val loss:5.027091979980469\n",
      "training on iteration:586    \t/3400, val loss:4.205514430999756\n",
      "training on iteration:587    \t/3400, val loss:7.541823387145996\n",
      "training on iteration:588    \t/3400, val loss:7.101343631744385\n",
      "training on iteration:589    \t/3400, val loss:5.1174235343933105\n",
      "training on iteration:590    \t/3400, val loss:4.636589527130127\n",
      "training on iteration:591    \t/3400, val loss:6.2891387939453125\n",
      "training on iteration:592    \t/3400, val loss:3.9542288780212402\n",
      "training on iteration:593    \t/3400, val loss:3.8773844242095947\n",
      "training on iteration:594    \t/3400, val loss:8.242878913879395\n",
      "training on iteration:595    \t/3400, val loss:4.920159339904785\n",
      "training on iteration:596    \t/3400, val loss:4.013530731201172\n",
      "training on iteration:597    \t/3400, val loss:6.7384867668151855\n",
      "training on iteration:598    \t/3400, val loss:4.984699726104736\n",
      "training on iteration:599    \t/3400, val loss:5.454073905944824\n",
      "training on iteration:600    \t/3400, val loss:4.413820266723633\n",
      "training on iteration:601    \t/3400, val loss:4.230810165405273\n",
      "training on iteration:602    \t/3400, val loss:3.6979758739471436\n",
      "training on iteration:603    \t/3400, val loss:6.5372185707092285\n",
      "training on iteration:604    \t/3400, val loss:3.858065128326416\n",
      "training on iteration:605    \t/3400, val loss:3.883693218231201\n",
      "training on iteration:606    \t/3400, val loss:9.693325996398926\n",
      "training on iteration:607    \t/3400, val loss:6.968580722808838\n",
      "training on iteration:608    \t/3400, val loss:10.01871395111084\n",
      "training on iteration:609    \t/3400, val loss:4.137856483459473\n",
      "training on iteration:610    \t/3400, val loss:13.59649658203125\n",
      "training on iteration:611    \t/3400, val loss:10.918770790100098\n",
      "training on iteration:612    \t/3400, val loss:8.12437915802002\n",
      "training on iteration:613    \t/3400, val loss:8.722095489501953\n",
      "training on iteration:614    \t/3400, val loss:9.264548301696777\n",
      "training on iteration:615    \t/3400, val loss:18.460317611694336\n",
      "training on iteration:616    \t/3400, val loss:17.82335090637207\n",
      "training on iteration:617    \t/3400, val loss:6.016323089599609\n",
      "training on iteration:618    \t/3400, val loss:25.405378341674805\n",
      "training on iteration:619    \t/3400, val loss:7.787184715270996\n",
      "training on iteration:620    \t/3400, val loss:21.598424911499023\n",
      "training on iteration:621    \t/3400, val loss:31.145366668701172\n",
      "training on iteration:622    \t/3400, val loss:33.890525817871094\n",
      "training on iteration:623    \t/3400, val loss:32.22257614135742\n",
      "training on iteration:624    \t/3400, val loss:25.725915908813477\n",
      "training on iteration:625    \t/3400, val loss:13.95200252532959\n",
      "training on iteration:626    \t/3400, val loss:12.43455696105957\n",
      "training on iteration:627    \t/3400, val loss:18.14065170288086\n",
      "training on iteration:628    \t/3400, val loss:13.005620002746582\n",
      "training on iteration:629    \t/3400, val loss:25.590423583984375\n",
      "training on iteration:630    \t/3400, val loss:31.584211349487305\n",
      "training on iteration:631    \t/3400, val loss:31.876462936401367\n",
      "training on iteration:632    \t/3400, val loss:27.205455780029297\n",
      "training on iteration:633    \t/3400, val loss:16.331714630126953\n",
      "training on iteration:634    \t/3400, val loss:19.911836624145508\n",
      "training on iteration:635    \t/3400, val loss:4.587512969970703\n",
      "training on iteration:636    \t/3400, val loss:17.75087547302246\n",
      "training on iteration:637    \t/3400, val loss:21.539325714111328\n",
      "training on iteration:638    \t/3400, val loss:20.229598999023438\n",
      "training on iteration:639    \t/3400, val loss:22.73621940612793\n",
      "training on iteration:640    \t/3400, val loss:17.484485626220703\n",
      "training on iteration:641    \t/3400, val loss:13.367410659790039\n",
      "training on iteration:642    \t/3400, val loss:6.978194236755371\n",
      "training on iteration:643    \t/3400, val loss:10.502609252929688\n",
      "training on iteration:644    \t/3400, val loss:11.655001640319824\n",
      "training on iteration:645    \t/3400, val loss:11.012455940246582\n",
      "training on iteration:646    \t/3400, val loss:5.770783424377441\n",
      "training on iteration:647    \t/3400, val loss:5.452452659606934\n",
      "training on iteration:648    \t/3400, val loss:8.856833457946777\n",
      "training on iteration:649    \t/3400, val loss:13.651698112487793\n",
      "training on iteration:650    \t/3400, val loss:8.733892440795898\n",
      "training on iteration:651    \t/3400, val loss:11.858932495117188\n",
      "training on iteration:652    \t/3400, val loss:6.788003444671631\n",
      "training on iteration:653    \t/3400, val loss:14.085861206054688\n",
      "training on iteration:654    \t/3400, val loss:17.88233757019043\n",
      "training on iteration:655    \t/3400, val loss:12.79051399230957\n",
      "training on iteration:656    \t/3400, val loss:11.48735523223877\n",
      "training on iteration:657    \t/3400, val loss:4.868834495544434\n",
      "training on iteration:658    \t/3400, val loss:8.790459632873535\n",
      "training on iteration:659    \t/3400, val loss:13.766350746154785\n",
      "training on iteration:660    \t/3400, val loss:11.38898754119873\n",
      "training on iteration:661    \t/3400, val loss:9.593415260314941\n",
      "training on iteration:662    \t/3400, val loss:4.823258876800537\n",
      "training on iteration:663    \t/3400, val loss:10.845649719238281\n",
      "training on iteration:664    \t/3400, val loss:10.24708080291748\n",
      "training on iteration:665    \t/3400, val loss:10.886540412902832\n",
      "training on iteration:666    \t/3400, val loss:8.864598274230957\n",
      "training on iteration:667    \t/3400, val loss:7.653310298919678\n",
      "training on iteration:668    \t/3400, val loss:6.322081565856934\n",
      "training on iteration:669    \t/3400, val loss:8.281353950500488\n",
      "training on iteration:670    \t/3400, val loss:4.868765830993652\n",
      "training on iteration:671    \t/3400, val loss:8.749524116516113\n",
      "training on iteration:672    \t/3400, val loss:6.834709167480469\n",
      "training on iteration:673    \t/3400, val loss:10.07611083984375\n",
      "training on iteration:674    \t/3400, val loss:9.186270713806152\n",
      "training on iteration:675    \t/3400, val loss:6.6682634353637695\n",
      "training on iteration:676    \t/3400, val loss:6.69263219833374\n",
      "training on iteration:677    \t/3400, val loss:8.189631462097168\n",
      "training on iteration:678    \t/3400, val loss:12.535247802734375\n",
      "training on iteration:679    \t/3400, val loss:7.73224401473999\n",
      "training on iteration:680    \t/3400, val loss:9.490585327148438\n",
      "training on iteration:681    \t/3400, val loss:5.140305519104004\n",
      "training on iteration:682    \t/3400, val loss:11.545161247253418\n",
      "training on iteration:683    \t/3400, val loss:11.607657432556152\n",
      "training on iteration:684    \t/3400, val loss:10.31266975402832\n",
      "training on iteration:685    \t/3400, val loss:8.004301071166992\n",
      "training on iteration:686    \t/3400, val loss:7.946300029754639\n",
      "training on iteration:687    \t/3400, val loss:12.542470932006836\n",
      "training on iteration:688    \t/3400, val loss:11.323980331420898\n",
      "training on iteration:689    \t/3400, val loss:4.889737129211426\n",
      "training on iteration:690    \t/3400, val loss:12.23426628112793\n",
      "training on iteration:691    \t/3400, val loss:11.315116882324219\n",
      "training on iteration:692    \t/3400, val loss:15.416021347045898\n",
      "training on iteration:693    \t/3400, val loss:16.656978607177734\n",
      "training on iteration:694    \t/3400, val loss:9.252856254577637\n",
      "training on iteration:695    \t/3400, val loss:8.833819389343262\n",
      "training on iteration:696    \t/3400, val loss:6.24587345123291\n",
      "training on iteration:697    \t/3400, val loss:10.53627872467041\n",
      "training on iteration:698    \t/3400, val loss:12.8882474899292\n",
      "training on iteration:699    \t/3400, val loss:12.464826583862305\n",
      "training on iteration:700    \t/3400, val loss:7.375397682189941\n",
      "training on iteration:701    \t/3400, val loss:7.240048408508301\n",
      "training on iteration:702    \t/3400, val loss:12.444137573242188\n",
      "training on iteration:703    \t/3400, val loss:6.040393829345703\n",
      "training on iteration:704    \t/3400, val loss:9.03736686706543\n",
      "training on iteration:705    \t/3400, val loss:9.800651550292969\n",
      "training on iteration:706    \t/3400, val loss:13.048049926757812\n",
      "training on iteration:707    \t/3400, val loss:8.388301849365234\n",
      "training on iteration:708    \t/3400, val loss:7.308762550354004\n",
      "training on iteration:709    \t/3400, val loss:5.141723155975342\n",
      "training on iteration:710    \t/3400, val loss:8.459199905395508\n",
      "training on iteration:711    \t/3400, val loss:7.207805156707764\n",
      "training on iteration:712    \t/3400, val loss:5.707233905792236\n",
      "training on iteration:713    \t/3400, val loss:5.070345401763916\n",
      "training on iteration:714    \t/3400, val loss:6.590035438537598\n",
      "training on iteration:715    \t/3400, val loss:3.5073962211608887\n",
      "training on iteration:716    \t/3400, val loss:7.470033168792725\n",
      "training on iteration:717    \t/3400, val loss:8.688536643981934\n",
      "training on iteration:718    \t/3400, val loss:4.386229515075684\n",
      "training on iteration:719    \t/3400, val loss:5.759613990783691\n",
      "training on iteration:720    \t/3400, val loss:6.086158275604248\n",
      "training on iteration:721    \t/3400, val loss:6.079415798187256\n",
      "training on iteration:722    \t/3400, val loss:7.038660526275635\n",
      "training on iteration:723    \t/3400, val loss:3.9676764011383057\n",
      "training on iteration:724    \t/3400, val loss:6.815299034118652\n",
      "training on iteration:725    \t/3400, val loss:7.177159309387207\n",
      "training on iteration:726    \t/3400, val loss:3.5491607189178467\n",
      "training on iteration:727    \t/3400, val loss:5.387324810028076\n",
      "training on iteration:728    \t/3400, val loss:5.289443016052246\n",
      "training on iteration:729    \t/3400, val loss:4.568696022033691\n",
      "training on iteration:730    \t/3400, val loss:3.6145126819610596\n",
      "training on iteration:731    \t/3400, val loss:5.172397136688232\n",
      "training on iteration:732    \t/3400, val loss:4.800563812255859\n",
      "training on iteration:733    \t/3400, val loss:3.255368947982788\n",
      "training on iteration:734    \t/3400, val loss:5.554360389709473\n",
      "training on iteration:735    \t/3400, val loss:4.769927024841309\n",
      "training on iteration:736    \t/3400, val loss:4.942180156707764\n",
      "training on iteration:737    \t/3400, val loss:3.7791366577148438\n",
      "training on iteration:738    \t/3400, val loss:6.245126247406006\n",
      "training on iteration:739    \t/3400, val loss:3.270521879196167\n",
      "training on iteration:740    \t/3400, val loss:3.6567153930664062\n",
      "training on iteration:741    \t/3400, val loss:5.445598125457764\n",
      "training on iteration:742    \t/3400, val loss:3.6619746685028076\n",
      "training on iteration:743    \t/3400, val loss:3.695596218109131\n",
      "training on iteration:744    \t/3400, val loss:3.6438066959381104\n",
      "training on iteration:745    \t/3400, val loss:3.5964109897613525\n",
      "training on iteration:746    \t/3400, val loss:3.6915011405944824\n",
      "training on iteration:747    \t/3400, val loss:4.31974458694458\n",
      "training on iteration:748    \t/3400, val loss:3.541944742202759\n",
      "training on iteration:749    \t/3400, val loss:3.68654465675354\n",
      "training on iteration:750    \t/3400, val loss:6.067199230194092\n",
      "training on iteration:751    \t/3400, val loss:3.6215384006500244\n",
      "training on iteration:752    \t/3400, val loss:4.018732070922852\n",
      "training on iteration:753    \t/3400, val loss:4.214820384979248\n",
      "training on iteration:754    \t/3400, val loss:3.5100173950195312\n",
      "training on iteration:755    \t/3400, val loss:6.051595211029053\n",
      "training on iteration:756    \t/3400, val loss:3.9500579833984375\n",
      "training on iteration:757    \t/3400, val loss:3.5451183319091797\n",
      "training on iteration:758    \t/3400, val loss:4.813033103942871\n",
      "training on iteration:759    \t/3400, val loss:5.091026306152344\n",
      "training on iteration:760    \t/3400, val loss:4.0114521980285645\n",
      "training on iteration:761    \t/3400, val loss:4.664534091949463\n",
      "training on iteration:762    \t/3400, val loss:4.416507244110107\n",
      "training on iteration:763    \t/3400, val loss:4.063644886016846\n",
      "training on iteration:764    \t/3400, val loss:6.566054821014404\n",
      "training on iteration:765    \t/3400, val loss:3.439638137817383\n",
      "training on iteration:766    \t/3400, val loss:3.40309739112854\n",
      "training on iteration:767    \t/3400, val loss:7.65111780166626\n",
      "training on iteration:768    \t/3400, val loss:4.927159786224365\n",
      "training on iteration:769    \t/3400, val loss:5.649521827697754\n",
      "training on iteration:770    \t/3400, val loss:3.75536847114563\n",
      "training on iteration:771    \t/3400, val loss:5.796725273132324\n",
      "training on iteration:772    \t/3400, val loss:3.464120626449585\n",
      "training on iteration:773    \t/3400, val loss:3.8867204189300537\n",
      "training on iteration:774    \t/3400, val loss:6.655589580535889\n",
      "training on iteration:775    \t/3400, val loss:5.316494941711426\n",
      "training on iteration:776    \t/3400, val loss:4.68887186050415\n",
      "training on iteration:777    \t/3400, val loss:5.220748424530029\n",
      "training on iteration:778    \t/3400, val loss:4.159430980682373\n",
      "training on iteration:779    \t/3400, val loss:3.898677349090576\n",
      "training on iteration:780    \t/3400, val loss:5.89473819732666\n",
      "training on iteration:781    \t/3400, val loss:4.453367710113525\n",
      "training on iteration:782    \t/3400, val loss:3.7002851963043213\n",
      "training on iteration:783    \t/3400, val loss:5.884388446807861\n",
      "training on iteration:784    \t/3400, val loss:3.306493043899536\n",
      "training on iteration:785    \t/3400, val loss:4.517849922180176\n",
      "training on iteration:786    \t/3400, val loss:4.346227169036865\n",
      "training on iteration:787    \t/3400, val loss:4.019442558288574\n",
      "training on iteration:788    \t/3400, val loss:4.705222129821777\n",
      "training on iteration:789    \t/3400, val loss:3.927093505859375\n",
      "training on iteration:790    \t/3400, val loss:3.6810240745544434\n",
      "training on iteration:791    \t/3400, val loss:5.649632453918457\n",
      "training on iteration:792    \t/3400, val loss:3.264868974685669\n",
      "training on iteration:793    \t/3400, val loss:6.08416223526001\n",
      "training on iteration:794    \t/3400, val loss:3.6816744804382324\n",
      "training on iteration:795    \t/3400, val loss:3.2373039722442627\n",
      "training on iteration:796    \t/3400, val loss:4.9079694747924805\n",
      "training on iteration:797    \t/3400, val loss:3.289116144180298\n",
      "training on iteration:798    \t/3400, val loss:4.7904510498046875\n",
      "training on iteration:799    \t/3400, val loss:3.478602886199951\n",
      "training on iteration:800    \t/3400, val loss:3.67631196975708\n",
      "training on iteration:801    \t/3400, val loss:4.449438571929932\n",
      "training on iteration:802    \t/3400, val loss:4.081613063812256\n",
      "training on iteration:803    \t/3400, val loss:4.020648956298828\n",
      "training on iteration:804    \t/3400, val loss:3.2761282920837402\n",
      "training on iteration:805    \t/3400, val loss:3.3601927757263184\n",
      "training on iteration:806    \t/3400, val loss:4.937602996826172\n",
      "training on iteration:807    \t/3400, val loss:3.9368412494659424\n",
      "training on iteration:808    \t/3400, val loss:4.052404403686523\n",
      "training on iteration:809    \t/3400, val loss:3.247041940689087\n",
      "training on iteration:810    \t/3400, val loss:8.702454566955566\n",
      "training on iteration:811    \t/3400, val loss:4.685798645019531\n",
      "training on iteration:812    \t/3400, val loss:6.40151309967041\n",
      "training on iteration:813    \t/3400, val loss:5.016668796539307\n",
      "training on iteration:814    \t/3400, val loss:5.9275970458984375\n",
      "training on iteration:815    \t/3400, val loss:6.256265163421631\n",
      "training on iteration:816    \t/3400, val loss:4.052019119262695\n",
      "training on iteration:817    \t/3400, val loss:7.3469414710998535\n",
      "training on iteration:818    \t/3400, val loss:3.92948317527771\n",
      "training on iteration:819    \t/3400, val loss:3.6613857746124268\n",
      "training on iteration:820    \t/3400, val loss:8.319016456604004\n",
      "training on iteration:821    \t/3400, val loss:6.326220989227295\n",
      "training on iteration:822    \t/3400, val loss:3.8560128211975098\n",
      "training on iteration:823    \t/3400, val loss:7.121303558349609\n",
      "training on iteration:824    \t/3400, val loss:8.182186126708984\n",
      "training on iteration:825    \t/3400, val loss:3.5536341667175293\n",
      "training on iteration:826    \t/3400, val loss:8.013659477233887\n",
      "training on iteration:827    \t/3400, val loss:6.425829887390137\n",
      "training on iteration:828    \t/3400, val loss:10.238423347473145\n",
      "training on iteration:829    \t/3400, val loss:5.3124308586120605\n",
      "training on iteration:830    \t/3400, val loss:8.336450576782227\n",
      "training on iteration:831    \t/3400, val loss:5.73891544342041\n",
      "training on iteration:832    \t/3400, val loss:7.195869445800781\n",
      "training on iteration:833    \t/3400, val loss:6.603875160217285\n",
      "training on iteration:834    \t/3400, val loss:3.223116636276245\n",
      "training on iteration:835    \t/3400, val loss:10.315633773803711\n",
      "training on iteration:836    \t/3400, val loss:9.63646411895752\n",
      "training on iteration:837    \t/3400, val loss:3.91109299659729\n",
      "training on iteration:838    \t/3400, val loss:9.80777645111084\n",
      "training on iteration:839    \t/3400, val loss:7.193228244781494\n",
      "training on iteration:840    \t/3400, val loss:15.330706596374512\n",
      "training on iteration:841    \t/3400, val loss:13.988303184509277\n",
      "training on iteration:842    \t/3400, val loss:12.085320472717285\n",
      "training on iteration:843    \t/3400, val loss:10.699640274047852\n",
      "training on iteration:844    \t/3400, val loss:3.9140429496765137\n",
      "training on iteration:845    \t/3400, val loss:9.673828125\n",
      "training on iteration:846    \t/3400, val loss:9.638326644897461\n",
      "training on iteration:847    \t/3400, val loss:5.321078777313232\n",
      "training on iteration:848    \t/3400, val loss:7.0824503898620605\n",
      "training on iteration:849    \t/3400, val loss:6.836666584014893\n",
      "training on iteration:850    \t/3400, val loss:4.392523288726807\n",
      "training on iteration:851    \t/3400, val loss:7.6243181228637695\n",
      "training on iteration:852    \t/3400, val loss:10.076611518859863\n",
      "training on iteration:853    \t/3400, val loss:3.4630801677703857\n",
      "training on iteration:854    \t/3400, val loss:4.319202423095703\n",
      "training on iteration:855    \t/3400, val loss:4.932258605957031\n",
      "training on iteration:856    \t/3400, val loss:3.7912745475769043\n",
      "training on iteration:857    \t/3400, val loss:4.410534381866455\n",
      "training on iteration:858    \t/3400, val loss:4.153931140899658\n",
      "training on iteration:859    \t/3400, val loss:3.3905184268951416\n",
      "training on iteration:860    \t/3400, val loss:3.4447433948516846\n",
      "training on iteration:861    \t/3400, val loss:9.762319564819336\n",
      "training on iteration:862    \t/3400, val loss:6.057544231414795\n",
      "training on iteration:863    \t/3400, val loss:9.603279113769531\n",
      "training on iteration:864    \t/3400, val loss:3.1765403747558594\n",
      "training on iteration:865    \t/3400, val loss:12.652406692504883\n",
      "training on iteration:866    \t/3400, val loss:11.967584609985352\n",
      "training on iteration:867    \t/3400, val loss:3.606642484664917\n",
      "training on iteration:868    \t/3400, val loss:7.1581878662109375\n",
      "training on iteration:869    \t/3400, val loss:8.716287612915039\n",
      "training on iteration:870    \t/3400, val loss:9.366667747497559\n",
      "training on iteration:871    \t/3400, val loss:6.097869396209717\n",
      "training on iteration:872    \t/3400, val loss:3.918243885040283\n",
      "training on iteration:873    \t/3400, val loss:9.510309219360352\n",
      "training on iteration:874    \t/3400, val loss:9.759851455688477\n",
      "training on iteration:875    \t/3400, val loss:5.944808006286621\n",
      "training on iteration:876    \t/3400, val loss:8.012645721435547\n",
      "training on iteration:877    \t/3400, val loss:8.156723976135254\n",
      "training on iteration:878    \t/3400, val loss:13.455636024475098\n",
      "training on iteration:879    \t/3400, val loss:11.884115219116211\n",
      "training on iteration:880    \t/3400, val loss:8.877795219421387\n",
      "training on iteration:881    \t/3400, val loss:13.59696102142334\n",
      "training on iteration:882    \t/3400, val loss:4.334227085113525\n",
      "training on iteration:883    \t/3400, val loss:13.70285701751709\n",
      "training on iteration:884    \t/3400, val loss:12.627481460571289\n",
      "training on iteration:885    \t/3400, val loss:3.3893487453460693\n",
      "training on iteration:886    \t/3400, val loss:4.189656734466553\n",
      "training on iteration:887    \t/3400, val loss:8.158686637878418\n",
      "training on iteration:888    \t/3400, val loss:9.184927940368652\n",
      "training on iteration:889    \t/3400, val loss:3.1706466674804688\n",
      "training on iteration:890    \t/3400, val loss:4.510036945343018\n",
      "training on iteration:891    \t/3400, val loss:9.194631576538086\n",
      "training on iteration:892    \t/3400, val loss:7.437106609344482\n",
      "training on iteration:893    \t/3400, val loss:7.450937747955322\n",
      "training on iteration:894    \t/3400, val loss:3.3498387336730957\n",
      "training on iteration:895    \t/3400, val loss:15.112065315246582\n",
      "training on iteration:896    \t/3400, val loss:18.672840118408203\n",
      "training on iteration:897    \t/3400, val loss:13.826336860656738\n",
      "training on iteration:898    \t/3400, val loss:5.216721057891846\n",
      "training on iteration:899    \t/3400, val loss:11.967814445495605\n",
      "training on iteration:900    \t/3400, val loss:5.884007453918457\n",
      "training on iteration:901    \t/3400, val loss:9.04719066619873\n",
      "training on iteration:902    \t/3400, val loss:5.267539978027344\n",
      "training on iteration:903    \t/3400, val loss:3.5781095027923584\n",
      "training on iteration:904    \t/3400, val loss:7.711953163146973\n",
      "training on iteration:905    \t/3400, val loss:9.52247142791748\n",
      "training on iteration:906    \t/3400, val loss:3.7831761837005615\n",
      "training on iteration:907    \t/3400, val loss:10.704748153686523\n",
      "training on iteration:908    \t/3400, val loss:5.09749174118042\n",
      "training on iteration:909    \t/3400, val loss:12.652931213378906\n",
      "training on iteration:910    \t/3400, val loss:12.207794189453125\n",
      "training on iteration:911    \t/3400, val loss:12.994032859802246\n",
      "training on iteration:912    \t/3400, val loss:7.435709476470947\n",
      "training on iteration:913    \t/3400, val loss:7.771088600158691\n",
      "training on iteration:914    \t/3400, val loss:10.144189834594727\n",
      "training on iteration:915    \t/3400, val loss:9.261398315429688\n",
      "training on iteration:916    \t/3400, val loss:5.291090965270996\n",
      "training on iteration:917    \t/3400, val loss:7.102315425872803\n",
      "training on iteration:918    \t/3400, val loss:3.091440439224243\n",
      "training on iteration:919    \t/3400, val loss:8.975302696228027\n",
      "training on iteration:920    \t/3400, val loss:6.242852687835693\n",
      "training on iteration:921    \t/3400, val loss:3.4239563941955566\n",
      "training on iteration:922    \t/3400, val loss:5.132619380950928\n",
      "training on iteration:923    \t/3400, val loss:4.573597431182861\n",
      "training on iteration:924    \t/3400, val loss:4.5146870613098145\n",
      "training on iteration:925    \t/3400, val loss:5.378116130828857\n",
      "training on iteration:926    \t/3400, val loss:6.4062957763671875\n",
      "training on iteration:927    \t/3400, val loss:4.105134010314941\n",
      "training on iteration:928    \t/3400, val loss:3.35868239402771\n",
      "training on iteration:929    \t/3400, val loss:9.214421272277832\n",
      "training on iteration:930    \t/3400, val loss:6.9779205322265625\n",
      "training on iteration:931    \t/3400, val loss:6.23967981338501\n",
      "training on iteration:932    \t/3400, val loss:3.367436647415161\n",
      "training on iteration:933    \t/3400, val loss:7.439939975738525\n",
      "training on iteration:934    \t/3400, val loss:8.812729835510254\n",
      "training on iteration:935    \t/3400, val loss:3.1712892055511475\n",
      "training on iteration:936    \t/3400, val loss:4.0168585777282715\n",
      "training on iteration:937    \t/3400, val loss:8.022835731506348\n",
      "training on iteration:938    \t/3400, val loss:6.349250316619873\n",
      "training on iteration:939    \t/3400, val loss:5.264190673828125\n",
      "training on iteration:940    \t/3400, val loss:3.2479307651519775\n",
      "training on iteration:941    \t/3400, val loss:8.831686019897461\n",
      "training on iteration:942    \t/3400, val loss:5.983523845672607\n",
      "training on iteration:943    \t/3400, val loss:5.693126678466797\n",
      "training on iteration:944    \t/3400, val loss:3.408641815185547\n",
      "training on iteration:945    \t/3400, val loss:8.520708084106445\n",
      "training on iteration:946    \t/3400, val loss:3.9003968238830566\n",
      "training on iteration:947    \t/3400, val loss:4.651370525360107\n",
      "training on iteration:948    \t/3400, val loss:5.876491069793701\n",
      "training on iteration:949    \t/3400, val loss:5.347199440002441\n",
      "training on iteration:950    \t/3400, val loss:5.822963714599609\n",
      "training on iteration:951    \t/3400, val loss:3.6242105960845947\n",
      "training on iteration:952    \t/3400, val loss:7.5078043937683105\n",
      "training on iteration:953    \t/3400, val loss:3.250408411026001\n",
      "training on iteration:954    \t/3400, val loss:3.173593044281006\n",
      "training on iteration:955    \t/3400, val loss:7.055415153503418\n",
      "training on iteration:956    \t/3400, val loss:3.328744649887085\n",
      "training on iteration:957    \t/3400, val loss:5.22655725479126\n",
      "training on iteration:958    \t/3400, val loss:4.93149471282959\n",
      "training on iteration:959    \t/3400, val loss:5.2992472648620605\n",
      "training on iteration:960    \t/3400, val loss:4.365713596343994\n",
      "training on iteration:961    \t/3400, val loss:3.8595597743988037\n",
      "training on iteration:962    \t/3400, val loss:3.861234426498413\n",
      "training on iteration:963    \t/3400, val loss:3.911846876144409\n",
      "training on iteration:964    \t/3400, val loss:3.754183530807495\n",
      "training on iteration:965    \t/3400, val loss:3.1938412189483643\n",
      "training on iteration:966    \t/3400, val loss:3.144979476928711\n",
      "training on iteration:967    \t/3400, val loss:4.723138332366943\n",
      "training on iteration:968    \t/3400, val loss:3.4988174438476562\n",
      "training on iteration:969    \t/3400, val loss:3.3565313816070557\n",
      "training on iteration:970    \t/3400, val loss:3.950340509414673\n",
      "training on iteration:971    \t/3400, val loss:3.685190439224243\n",
      "training on iteration:972    \t/3400, val loss:4.6724395751953125\n",
      "training on iteration:973    \t/3400, val loss:3.273115873336792\n",
      "training on iteration:974    \t/3400, val loss:3.4969124794006348\n",
      "training on iteration:975    \t/3400, val loss:3.838488817214966\n",
      "training on iteration:976    \t/3400, val loss:4.352763652801514\n",
      "training on iteration:977    \t/3400, val loss:3.4137661457061768\n",
      "training on iteration:978    \t/3400, val loss:4.051294803619385\n",
      "training on iteration:979    \t/3400, val loss:3.158456563949585\n",
      "training on iteration:980    \t/3400, val loss:7.492835998535156\n",
      "training on iteration:981    \t/3400, val loss:4.157446384429932\n",
      "training on iteration:982    \t/3400, val loss:4.10072660446167\n",
      "training on iteration:983    \t/3400, val loss:5.930821418762207\n",
      "training on iteration:984    \t/3400, val loss:5.718461513519287\n",
      "training on iteration:985    \t/3400, val loss:6.34921932220459\n",
      "training on iteration:986    \t/3400, val loss:4.3688530921936035\n",
      "training on iteration:987    \t/3400, val loss:9.13034725189209\n",
      "training on iteration:988    \t/3400, val loss:7.512080669403076\n",
      "training on iteration:989    \t/3400, val loss:3.9628922939300537\n",
      "training on iteration:990    \t/3400, val loss:5.3093037605285645\n",
      "training on iteration:991    \t/3400, val loss:6.528934955596924\n",
      "training on iteration:992    \t/3400, val loss:4.247908592224121\n",
      "training on iteration:993    \t/3400, val loss:5.10629940032959\n",
      "training on iteration:994    \t/3400, val loss:5.52146053314209\n",
      "training on iteration:995    \t/3400, val loss:6.50575590133667\n",
      "training on iteration:996    \t/3400, val loss:7.993502616882324\n",
      "training on iteration:997    \t/3400, val loss:4.907259941101074\n",
      "training on iteration:998    \t/3400, val loss:8.699972152709961\n",
      "training on iteration:999    \t/3400, val loss:10.274287223815918\n",
      "training on iteration:1000    \t/3400, val loss:8.920626640319824\n",
      "training on iteration:1001    \t/3400, val loss:3.7544238567352295\n",
      "training on iteration:1002    \t/3400, val loss:11.266480445861816\n",
      "training on iteration:1003    \t/3400, val loss:7.892042636871338\n",
      "training on iteration:1004    \t/3400, val loss:7.237843036651611\n",
      "training on iteration:1005    \t/3400, val loss:10.089142799377441\n",
      "training on iteration:1006    \t/3400, val loss:9.53347110748291\n",
      "training on iteration:1007    \t/3400, val loss:5.347300052642822\n",
      "training on iteration:1008    \t/3400, val loss:11.720076560974121\n",
      "training on iteration:1009    \t/3400, val loss:9.939872741699219\n",
      "training on iteration:1010    \t/3400, val loss:9.227684020996094\n",
      "training on iteration:1011    \t/3400, val loss:8.844202041625977\n",
      "training on iteration:1012    \t/3400, val loss:8.616555213928223\n",
      "training on iteration:1013    \t/3400, val loss:3.191929340362549\n",
      "training on iteration:1014    \t/3400, val loss:8.14411735534668\n",
      "training on iteration:1015    \t/3400, val loss:11.448578834533691\n",
      "training on iteration:1016    \t/3400, val loss:10.106436729431152\n",
      "training on iteration:1017    \t/3400, val loss:5.568297386169434\n",
      "training on iteration:1018    \t/3400, val loss:7.367591381072998\n",
      "training on iteration:1019    \t/3400, val loss:8.451655387878418\n",
      "training on iteration:1020    \t/3400, val loss:3.1231846809387207\n",
      "training on iteration:1021    \t/3400, val loss:7.90631103515625\n",
      "training on iteration:1022    \t/3400, val loss:9.766901016235352\n",
      "training on iteration:1023    \t/3400, val loss:7.02469539642334\n",
      "training on iteration:1024    \t/3400, val loss:3.044384479522705\n",
      "training on iteration:1025    \t/3400, val loss:6.598161220550537\n",
      "training on iteration:1026    \t/3400, val loss:3.5090444087982178\n",
      "training on iteration:1027    \t/3400, val loss:6.085387706756592\n",
      "training on iteration:1028    \t/3400, val loss:7.1444854736328125\n",
      "training on iteration:1029    \t/3400, val loss:5.121315956115723\n",
      "training on iteration:1030    \t/3400, val loss:8.399518013000488\n",
      "training on iteration:1031    \t/3400, val loss:6.689869403839111\n",
      "training on iteration:1032    \t/3400, val loss:9.123991966247559\n",
      "training on iteration:1033    \t/3400, val loss:5.630979061126709\n",
      "training on iteration:1034    \t/3400, val loss:7.122278690338135\n",
      "training on iteration:1035    \t/3400, val loss:2.9953701496124268\n",
      "training on iteration:1036    \t/3400, val loss:9.34415340423584\n",
      "training on iteration:1037    \t/3400, val loss:8.877928733825684\n",
      "training on iteration:1038    \t/3400, val loss:5.3853631019592285\n",
      "training on iteration:1039    \t/3400, val loss:7.1606950759887695\n",
      "training on iteration:1040    \t/3400, val loss:8.674936294555664\n",
      "training on iteration:1041    \t/3400, val loss:13.145262718200684\n",
      "training on iteration:1042    \t/3400, val loss:11.19865608215332\n",
      "training on iteration:1043    \t/3400, val loss:4.106558799743652\n",
      "training on iteration:1044    \t/3400, val loss:4.740682125091553\n",
      "training on iteration:1045    \t/3400, val loss:7.5382490158081055\n",
      "training on iteration:1046    \t/3400, val loss:5.946728229522705\n",
      "training on iteration:1047    \t/3400, val loss:9.180476188659668\n",
      "training on iteration:1048    \t/3400, val loss:4.7005767822265625\n",
      "training on iteration:1049    \t/3400, val loss:12.262919425964355\n",
      "training on iteration:1050    \t/3400, val loss:12.254603385925293\n",
      "training on iteration:1051    \t/3400, val loss:10.111898422241211\n",
      "training on iteration:1052    \t/3400, val loss:6.3211517333984375\n",
      "training on iteration:1053    \t/3400, val loss:4.438339710235596\n",
      "training on iteration:1054    \t/3400, val loss:7.731913089752197\n",
      "training on iteration:1055    \t/3400, val loss:7.227726459503174\n",
      "training on iteration:1056    \t/3400, val loss:3.26810622215271\n",
      "training on iteration:1057    \t/3400, val loss:4.356243133544922\n",
      "training on iteration:1058    \t/3400, val loss:4.51380729675293\n",
      "training on iteration:1059    \t/3400, val loss:4.089858531951904\n",
      "training on iteration:1060    \t/3400, val loss:4.05232572555542\n",
      "training on iteration:1061    \t/3400, val loss:5.8596415519714355\n",
      "training on iteration:1062    \t/3400, val loss:4.417222023010254\n",
      "training on iteration:1063    \t/3400, val loss:4.091240406036377\n",
      "training on iteration:1064    \t/3400, val loss:5.9523539543151855\n",
      "training on iteration:1065    \t/3400, val loss:7.325989246368408\n",
      "training on iteration:1066    \t/3400, val loss:3.4868197441101074\n",
      "training on iteration:1067    \t/3400, val loss:3.4424595832824707\n",
      "training on iteration:1068    \t/3400, val loss:5.473080635070801\n",
      "training on iteration:1069    \t/3400, val loss:3.63582444190979\n",
      "training on iteration:1070    \t/3400, val loss:4.489343166351318\n",
      "training on iteration:1071    \t/3400, val loss:4.491661548614502\n",
      "training on iteration:1072    \t/3400, val loss:3.7467236518859863\n",
      "training on iteration:1073    \t/3400, val loss:2.9788014888763428\n",
      "training on iteration:1074    \t/3400, val loss:5.426488399505615\n",
      "training on iteration:1075    \t/3400, val loss:3.3655028343200684\n",
      "training on iteration:1076    \t/3400, val loss:3.053337574005127\n",
      "training on iteration:1077    \t/3400, val loss:4.1473612785339355\n",
      "training on iteration:1078    \t/3400, val loss:3.225543975830078\n",
      "training on iteration:1079    \t/3400, val loss:3.8327558040618896\n",
      "training on iteration:1080    \t/3400, val loss:3.7001373767852783\n",
      "training on iteration:1081    \t/3400, val loss:3.435509204864502\n",
      "training on iteration:1082    \t/3400, val loss:7.018421649932861\n",
      "training on iteration:1083    \t/3400, val loss:5.4893317222595215\n",
      "training on iteration:1084    \t/3400, val loss:6.309124946594238\n",
      "training on iteration:1085    \t/3400, val loss:3.439842462539673\n",
      "training on iteration:1086    \t/3400, val loss:9.509430885314941\n",
      "training on iteration:1087    \t/3400, val loss:6.579028606414795\n",
      "training on iteration:1088    \t/3400, val loss:4.824687957763672\n",
      "training on iteration:1089    \t/3400, val loss:3.0954325199127197\n",
      "training on iteration:1090    \t/3400, val loss:10.3631591796875\n",
      "training on iteration:1091    \t/3400, val loss:7.2671356201171875\n",
      "training on iteration:1092    \t/3400, val loss:7.37155818939209\n",
      "training on iteration:1093    \t/3400, val loss:3.8157570362091064\n",
      "training on iteration:1094    \t/3400, val loss:10.256781578063965\n",
      "training on iteration:1095    \t/3400, val loss:10.843888282775879\n",
      "training on iteration:1096    \t/3400, val loss:8.299524307250977\n",
      "training on iteration:1097    \t/3400, val loss:3.500458002090454\n",
      "training on iteration:1098    \t/3400, val loss:6.20122766494751\n",
      "training on iteration:1099    \t/3400, val loss:8.372993469238281\n",
      "training on iteration:1100    \t/3400, val loss:4.936236381530762\n",
      "training on iteration:1101    \t/3400, val loss:6.149997234344482\n",
      "training on iteration:1102    \t/3400, val loss:5.603957653045654\n",
      "training on iteration:1103    \t/3400, val loss:7.404713153839111\n",
      "training on iteration:1104    \t/3400, val loss:4.635037899017334\n",
      "training on iteration:1105    \t/3400, val loss:4.2841267585754395\n",
      "training on iteration:1106    \t/3400, val loss:6.4078545570373535\n",
      "training on iteration:1107    \t/3400, val loss:6.927772521972656\n",
      "training on iteration:1108    \t/3400, val loss:3.09082293510437\n",
      "training on iteration:1109    \t/3400, val loss:4.6098313331604\n",
      "training on iteration:1110    \t/3400, val loss:4.391114234924316\n",
      "training on iteration:1111    \t/3400, val loss:3.541895627975464\n",
      "training on iteration:1112    \t/3400, val loss:3.0783114433288574\n",
      "training on iteration:1113    \t/3400, val loss:4.700741291046143\n",
      "training on iteration:1114    \t/3400, val loss:3.7471354007720947\n",
      "training on iteration:1115    \t/3400, val loss:3.0597805976867676\n",
      "training on iteration:1116    \t/3400, val loss:8.000858306884766\n",
      "training on iteration:1117    \t/3400, val loss:7.003086090087891\n",
      "training on iteration:1118    \t/3400, val loss:3.559410333633423\n",
      "training on iteration:1119    \t/3400, val loss:3.2465147972106934\n",
      "training on iteration:1120    \t/3400, val loss:7.318699359893799\n",
      "training on iteration:1121    \t/3400, val loss:3.268718957901001\n",
      "training on iteration:1122    \t/3400, val loss:3.206101179122925\n",
      "training on iteration:1123    \t/3400, val loss:5.765671253204346\n",
      "training on iteration:1124    \t/3400, val loss:5.610790252685547\n",
      "training on iteration:1125    \t/3400, val loss:5.4581193923950195\n",
      "training on iteration:1126    \t/3400, val loss:3.8479597568511963\n",
      "training on iteration:1127    \t/3400, val loss:6.982205390930176\n",
      "training on iteration:1128    \t/3400, val loss:6.139926433563232\n",
      "training on iteration:1129    \t/3400, val loss:3.6723506450653076\n",
      "training on iteration:1130    \t/3400, val loss:5.793241024017334\n",
      "training on iteration:1131    \t/3400, val loss:7.297709941864014\n",
      "training on iteration:1132    \t/3400, val loss:4.599797248840332\n",
      "training on iteration:1133    \t/3400, val loss:3.4380316734313965\n",
      "training on iteration:1134    \t/3400, val loss:5.8316874504089355\n",
      "training on iteration:1135    \t/3400, val loss:3.3443844318389893\n",
      "training on iteration:1136    \t/3400, val loss:5.080578804016113\n",
      "training on iteration:1137    \t/3400, val loss:4.6265716552734375\n",
      "training on iteration:1138    \t/3400, val loss:5.01106595993042\n",
      "training on iteration:1139    \t/3400, val loss:3.90254545211792\n",
      "training on iteration:1140    \t/3400, val loss:3.646101236343384\n",
      "training on iteration:1141    \t/3400, val loss:6.152802467346191\n",
      "training on iteration:1142    \t/3400, val loss:5.013615131378174\n",
      "training on iteration:1143    \t/3400, val loss:2.9790215492248535\n",
      "training on iteration:1144    \t/3400, val loss:5.687014579772949\n",
      "training on iteration:1145    \t/3400, val loss:4.304488182067871\n",
      "training on iteration:1146    \t/3400, val loss:5.223402976989746\n",
      "training on iteration:1147    \t/3400, val loss:5.579853057861328\n",
      "training on iteration:1148    \t/3400, val loss:7.5248918533325195\n",
      "training on iteration:1149    \t/3400, val loss:4.962172508239746\n",
      "training on iteration:1150    \t/3400, val loss:4.11328649520874\n",
      "training on iteration:1151    \t/3400, val loss:7.690760135650635\n",
      "training on iteration:1152    \t/3400, val loss:6.22713565826416\n",
      "training on iteration:1153    \t/3400, val loss:5.062004566192627\n",
      "training on iteration:1154    \t/3400, val loss:3.8475022315979004\n",
      "training on iteration:1155    \t/3400, val loss:5.25295877456665\n",
      "training on iteration:1156    \t/3400, val loss:3.138681650161743\n",
      "training on iteration:1157    \t/3400, val loss:3.9132046699523926\n",
      "training on iteration:1158    \t/3400, val loss:5.454245567321777\n",
      "training on iteration:1159    \t/3400, val loss:3.248680353164673\n",
      "training on iteration:1160    \t/3400, val loss:3.036102294921875\n",
      "training on iteration:1161    \t/3400, val loss:4.698614597320557\n",
      "training on iteration:1162    \t/3400, val loss:3.2620654106140137\n",
      "training on iteration:1163    \t/3400, val loss:3.5868265628814697\n",
      "training on iteration:1164    \t/3400, val loss:3.811095714569092\n",
      "training on iteration:1165    \t/3400, val loss:3.243683338165283\n",
      "training on iteration:1166    \t/3400, val loss:3.5411670207977295\n",
      "training on iteration:1167    \t/3400, val loss:6.559775352478027\n",
      "training on iteration:1168    \t/3400, val loss:3.343877077102661\n",
      "training on iteration:1169    \t/3400, val loss:3.0224153995513916\n",
      "training on iteration:1170    \t/3400, val loss:5.30450439453125\n",
      "training on iteration:1171    \t/3400, val loss:3.566279888153076\n",
      "training on iteration:1172    \t/3400, val loss:3.3733325004577637\n",
      "training on iteration:1173    \t/3400, val loss:3.10248064994812\n",
      "training on iteration:1174    \t/3400, val loss:3.218313217163086\n",
      "training on iteration:1175    \t/3400, val loss:5.168124675750732\n",
      "training on iteration:1176    \t/3400, val loss:3.4392333030700684\n",
      "training on iteration:1177    \t/3400, val loss:3.217883825302124\n",
      "training on iteration:1178    \t/3400, val loss:3.085784673690796\n",
      "training on iteration:1179    \t/3400, val loss:2.9666802883148193\n",
      "training on iteration:1180    \t/3400, val loss:5.480527400970459\n",
      "training on iteration:1181    \t/3400, val loss:3.066887378692627\n",
      "training on iteration:1182    \t/3400, val loss:3.2793123722076416\n",
      "training on iteration:1183    \t/3400, val loss:3.2133963108062744\n",
      "training on iteration:1184    \t/3400, val loss:5.090188026428223\n",
      "training on iteration:1185    \t/3400, val loss:3.6938958168029785\n",
      "training on iteration:1186    \t/3400, val loss:3.7414920330047607\n",
      "training on iteration:1187    \t/3400, val loss:3.8634250164031982\n",
      "training on iteration:1188    \t/3400, val loss:3.857773542404175\n",
      "training on iteration:1189    \t/3400, val loss:6.1889495849609375\n",
      "training on iteration:1190    \t/3400, val loss:3.611412763595581\n",
      "training on iteration:1191    \t/3400, val loss:3.3578085899353027\n",
      "training on iteration:1192    \t/3400, val loss:6.321362495422363\n",
      "training on iteration:1193    \t/3400, val loss:7.161224365234375\n",
      "training on iteration:1194    \t/3400, val loss:4.726762294769287\n",
      "training on iteration:1195    \t/3400, val loss:4.17703914642334\n",
      "training on iteration:1196    \t/3400, val loss:6.986358165740967\n",
      "training on iteration:1197    \t/3400, val loss:5.692808628082275\n",
      "training on iteration:1198    \t/3400, val loss:3.596963405609131\n",
      "training on iteration:1199    \t/3400, val loss:4.3778486251831055\n",
      "training on iteration:1200    \t/3400, val loss:4.592026710510254\n",
      "training on iteration:1201    \t/3400, val loss:3.207029104232788\n",
      "training on iteration:1202    \t/3400, val loss:3.615313768386841\n",
      "training on iteration:1203    \t/3400, val loss:3.422358989715576\n",
      "training on iteration:1204    \t/3400, val loss:4.293946743011475\n",
      "training on iteration:1205    \t/3400, val loss:3.2077889442443848\n",
      "training on iteration:1206    \t/3400, val loss:4.553887367248535\n",
      "training on iteration:1207    \t/3400, val loss:3.107971668243408\n",
      "training on iteration:1208    \t/3400, val loss:3.603365659713745\n",
      "training on iteration:1209    \t/3400, val loss:5.48018741607666\n",
      "training on iteration:1210    \t/3400, val loss:2.9295082092285156\n",
      "training on iteration:1211    \t/3400, val loss:3.2790048122406006\n",
      "training on iteration:1212    \t/3400, val loss:2.9651167392730713\n",
      "training on iteration:1213    \t/3400, val loss:3.308741807937622\n",
      "training on iteration:1214    \t/3400, val loss:4.740419864654541\n",
      "training on iteration:1215    \t/3400, val loss:2.9427027702331543\n",
      "training on iteration:1216    \t/3400, val loss:3.4939024448394775\n",
      "training on iteration:1217    \t/3400, val loss:3.1335813999176025\n",
      "training on iteration:1218    \t/3400, val loss:6.7230353355407715\n",
      "training on iteration:1219    \t/3400, val loss:2.9859519004821777\n",
      "training on iteration:1220    \t/3400, val loss:2.914058208465576\n",
      "training on iteration:1221    \t/3400, val loss:3.677229166030884\n",
      "training on iteration:1222    \t/3400, val loss:3.2797915935516357\n",
      "training on iteration:1223    \t/3400, val loss:3.3547327518463135\n",
      "training on iteration:1224    \t/3400, val loss:2.921856641769409\n",
      "training on iteration:1225    \t/3400, val loss:3.321798801422119\n",
      "training on iteration:1226    \t/3400, val loss:3.9554061889648438\n",
      "training on iteration:1227    \t/3400, val loss:3.452937602996826\n",
      "training on iteration:1228    \t/3400, val loss:3.1693637371063232\n",
      "training on iteration:1229    \t/3400, val loss:3.171538829803467\n",
      "training on iteration:1230    \t/3400, val loss:4.343689441680908\n",
      "training on iteration:1231    \t/3400, val loss:3.1350905895233154\n",
      "training on iteration:1232    \t/3400, val loss:3.140498638153076\n",
      "training on iteration:1233    \t/3400, val loss:3.8164279460906982\n",
      "training on iteration:1234    \t/3400, val loss:3.683438539505005\n",
      "training on iteration:1235    \t/3400, val loss:7.7744140625\n",
      "training on iteration:1236    \t/3400, val loss:4.793541431427002\n",
      "training on iteration:1237    \t/3400, val loss:7.985602378845215\n",
      "training on iteration:1238    \t/3400, val loss:3.249835968017578\n",
      "training on iteration:1239    \t/3400, val loss:7.941592216491699\n",
      "training on iteration:1240    \t/3400, val loss:3.13136887550354\n",
      "training on iteration:1241    \t/3400, val loss:3.651043176651001\n",
      "training on iteration:1242    \t/3400, val loss:6.743325710296631\n",
      "training on iteration:1243    \t/3400, val loss:5.461085319519043\n",
      "training on iteration:1244    \t/3400, val loss:5.182394504547119\n",
      "training on iteration:1245    \t/3400, val loss:4.2699809074401855\n",
      "training on iteration:1246    \t/3400, val loss:7.479501247406006\n",
      "training on iteration:1247    \t/3400, val loss:3.390695571899414\n",
      "training on iteration:1248    \t/3400, val loss:3.195603132247925\n",
      "training on iteration:1249    \t/3400, val loss:7.0856451988220215\n",
      "training on iteration:1250    \t/3400, val loss:4.793204307556152\n",
      "training on iteration:1251    \t/3400, val loss:4.015257358551025\n",
      "training on iteration:1252    \t/3400, val loss:8.799813270568848\n",
      "training on iteration:1253    \t/3400, val loss:10.114503860473633\n",
      "training on iteration:1254    \t/3400, val loss:7.263210773468018\n",
      "training on iteration:1255    \t/3400, val loss:4.025483131408691\n",
      "training on iteration:1256    \t/3400, val loss:5.8572998046875\n",
      "training on iteration:1257    \t/3400, val loss:9.789247512817383\n",
      "training on iteration:1258    \t/3400, val loss:4.143710136413574\n",
      "training on iteration:1259    \t/3400, val loss:9.002553939819336\n",
      "training on iteration:1260    \t/3400, val loss:5.313255310058594\n",
      "training on iteration:1261    \t/3400, val loss:9.672545433044434\n",
      "training on iteration:1262    \t/3400, val loss:5.539010524749756\n",
      "training on iteration:1263    \t/3400, val loss:9.723045349121094\n",
      "training on iteration:1264    \t/3400, val loss:4.511050224304199\n",
      "training on iteration:1265    \t/3400, val loss:12.689986228942871\n",
      "training on iteration:1266    \t/3400, val loss:14.486172676086426\n",
      "training on iteration:1267    \t/3400, val loss:13.365157127380371\n",
      "training on iteration:1268    \t/3400, val loss:11.998908042907715\n",
      "training on iteration:1269    \t/3400, val loss:3.200096845626831\n",
      "training on iteration:1270    \t/3400, val loss:11.73007869720459\n",
      "training on iteration:1271    \t/3400, val loss:11.788545608520508\n",
      "training on iteration:1272    \t/3400, val loss:11.667509078979492\n",
      "training on iteration:1273    \t/3400, val loss:8.136892318725586\n",
      "training on iteration:1274    \t/3400, val loss:9.707466125488281\n",
      "training on iteration:1275    \t/3400, val loss:9.698862075805664\n",
      "training on iteration:1276    \t/3400, val loss:6.620694637298584\n",
      "training on iteration:1277    \t/3400, val loss:11.050456047058105\n",
      "training on iteration:1278    \t/3400, val loss:9.059733390808105\n",
      "training on iteration:1279    \t/3400, val loss:8.987279891967773\n",
      "training on iteration:1280    \t/3400, val loss:2.9711551666259766\n",
      "training on iteration:1281    \t/3400, val loss:7.7809576988220215\n",
      "training on iteration:1282    \t/3400, val loss:5.764689922332764\n",
      "training on iteration:1283    \t/3400, val loss:4.198807716369629\n",
      "training on iteration:1284    \t/3400, val loss:6.687930583953857\n",
      "training on iteration:1285    \t/3400, val loss:6.443493366241455\n",
      "training on iteration:1286    \t/3400, val loss:2.800335645675659\n",
      "training on iteration:1287    \t/3400, val loss:4.690659523010254\n",
      "training on iteration:1288    \t/3400, val loss:3.551004409790039\n",
      "training on iteration:1289    \t/3400, val loss:3.887960195541382\n",
      "training on iteration:1290    \t/3400, val loss:3.940986394882202\n",
      "training on iteration:1291    \t/3400, val loss:4.32863187789917\n",
      "training on iteration:1292    \t/3400, val loss:3.6755595207214355\n",
      "training on iteration:1293    \t/3400, val loss:2.8969342708587646\n",
      "training on iteration:1294    \t/3400, val loss:6.436366081237793\n",
      "training on iteration:1295    \t/3400, val loss:4.48479700088501\n",
      "training on iteration:1296    \t/3400, val loss:2.8532087802886963\n",
      "training on iteration:1297    \t/3400, val loss:5.026961326599121\n",
      "training on iteration:1298    \t/3400, val loss:3.825784206390381\n",
      "training on iteration:1299    \t/3400, val loss:4.144937992095947\n",
      "training on iteration:1300    \t/3400, val loss:5.489681720733643\n",
      "training on iteration:1301    \t/3400, val loss:5.064820289611816\n",
      "training on iteration:1302    \t/3400, val loss:6.8604278564453125\n",
      "training on iteration:1303    \t/3400, val loss:4.886428356170654\n",
      "training on iteration:1304    \t/3400, val loss:8.792420387268066\n",
      "training on iteration:1305    \t/3400, val loss:4.858372688293457\n",
      "training on iteration:1306    \t/3400, val loss:5.289877891540527\n",
      "training on iteration:1307    \t/3400, val loss:3.7788586616516113\n",
      "training on iteration:1308    \t/3400, val loss:5.889466762542725\n",
      "training on iteration:1309    \t/3400, val loss:3.0724716186523438\n",
      "training on iteration:1310    \t/3400, val loss:3.207460880279541\n",
      "training on iteration:1311    \t/3400, val loss:6.899836540222168\n",
      "training on iteration:1312    \t/3400, val loss:2.9485607147216797\n",
      "training on iteration:1313    \t/3400, val loss:2.8853867053985596\n",
      "training on iteration:1314    \t/3400, val loss:4.664299488067627\n",
      "training on iteration:1315    \t/3400, val loss:2.875162363052368\n",
      "training on iteration:1316    \t/3400, val loss:4.327502727508545\n",
      "training on iteration:1317    \t/3400, val loss:3.136928081512451\n",
      "training on iteration:1318    \t/3400, val loss:3.085156202316284\n",
      "training on iteration:1319    \t/3400, val loss:3.0304713249206543\n",
      "training on iteration:1320    \t/3400, val loss:4.983237266540527\n",
      "training on iteration:1321    \t/3400, val loss:3.143188953399658\n",
      "training on iteration:1322    \t/3400, val loss:3.192509889602661\n",
      "training on iteration:1323    \t/3400, val loss:3.4140288829803467\n",
      "training on iteration:1324    \t/3400, val loss:2.856037139892578\n",
      "training on iteration:1325    \t/3400, val loss:5.384958744049072\n",
      "training on iteration:1326    \t/3400, val loss:3.2143311500549316\n",
      "training on iteration:1327    \t/3400, val loss:3.1524136066436768\n",
      "training on iteration:1328    \t/3400, val loss:5.431305408477783\n",
      "training on iteration:1329    \t/3400, val loss:3.039771556854248\n",
      "training on iteration:1330    \t/3400, val loss:3.0797438621520996\n",
      "training on iteration:1331    \t/3400, val loss:3.7867119312286377\n",
      "training on iteration:1332    \t/3400, val loss:3.3071720600128174\n",
      "training on iteration:1333    \t/3400, val loss:5.272036552429199\n",
      "training on iteration:1334    \t/3400, val loss:3.039041042327881\n",
      "training on iteration:1335    \t/3400, val loss:2.8897013664245605\n",
      "training on iteration:1336    \t/3400, val loss:4.83990478515625\n",
      "training on iteration:1337    \t/3400, val loss:2.955432176589966\n",
      "training on iteration:1338    \t/3400, val loss:3.1633574962615967\n",
      "training on iteration:1339    \t/3400, val loss:3.364586114883423\n",
      "training on iteration:1340    \t/3400, val loss:2.8333041667938232\n",
      "training on iteration:1341    \t/3400, val loss:3.770354747772217\n",
      "training on iteration:1342    \t/3400, val loss:3.1061313152313232\n",
      "training on iteration:1343    \t/3400, val loss:2.845165729522705\n",
      "training on iteration:1344    \t/3400, val loss:4.774068355560303\n",
      "training on iteration:1345    \t/3400, val loss:2.985729932785034\n",
      "training on iteration:1346    \t/3400, val loss:3.418191432952881\n",
      "training on iteration:1347    \t/3400, val loss:3.303758382797241\n",
      "training on iteration:1348    \t/3400, val loss:3.050269842147827\n",
      "training on iteration:1349    \t/3400, val loss:4.024453163146973\n",
      "training on iteration:1350    \t/3400, val loss:3.0825650691986084\n",
      "training on iteration:1351    \t/3400, val loss:3.3758418560028076\n",
      "training on iteration:1352    \t/3400, val loss:3.4126105308532715\n",
      "training on iteration:1353    \t/3400, val loss:3.7357571125030518\n",
      "training on iteration:1354    \t/3400, val loss:6.7242751121521\n",
      "training on iteration:1355    \t/3400, val loss:4.339599132537842\n",
      "training on iteration:1356    \t/3400, val loss:6.4736151695251465\n",
      "training on iteration:1357    \t/3400, val loss:3.392969846725464\n",
      "training on iteration:1358    \t/3400, val loss:8.433347702026367\n",
      "training on iteration:1359    \t/3400, val loss:3.8114893436431885\n",
      "training on iteration:1360    \t/3400, val loss:7.4452595710754395\n",
      "training on iteration:1361    \t/3400, val loss:4.7665252685546875\n",
      "training on iteration:1362    \t/3400, val loss:10.291475296020508\n",
      "training on iteration:1363    \t/3400, val loss:5.273833274841309\n",
      "training on iteration:1364    \t/3400, val loss:10.229249000549316\n",
      "training on iteration:1365    \t/3400, val loss:3.8976962566375732\n",
      "training on iteration:1366    \t/3400, val loss:12.748124122619629\n",
      "training on iteration:1367    \t/3400, val loss:15.090250968933105\n",
      "training on iteration:1368    \t/3400, val loss:8.002954483032227\n",
      "training on iteration:1369    \t/3400, val loss:15.404121398925781\n",
      "training on iteration:1370    \t/3400, val loss:12.064518928527832\n",
      "training on iteration:1371    \t/3400, val loss:11.035212516784668\n",
      "training on iteration:1372    \t/3400, val loss:17.359901428222656\n",
      "training on iteration:1373    \t/3400, val loss:15.137062072753906\n",
      "training on iteration:1374    \t/3400, val loss:4.876833915710449\n",
      "training on iteration:1375    \t/3400, val loss:15.790491104125977\n",
      "training on iteration:1376    \t/3400, val loss:7.626609802246094\n",
      "training on iteration:1377    \t/3400, val loss:13.290733337402344\n",
      "training on iteration:1378    \t/3400, val loss:19.15707778930664\n",
      "training on iteration:1379    \t/3400, val loss:17.563627243041992\n",
      "training on iteration:1380    \t/3400, val loss:8.288351058959961\n",
      "training on iteration:1381    \t/3400, val loss:13.153458595275879\n",
      "training on iteration:1382    \t/3400, val loss:13.565963745117188\n",
      "training on iteration:1383    \t/3400, val loss:6.620778560638428\n",
      "training on iteration:1384    \t/3400, val loss:11.887243270874023\n",
      "training on iteration:1385    \t/3400, val loss:8.494831085205078\n",
      "training on iteration:1386    \t/3400, val loss:5.225893974304199\n",
      "training on iteration:1387    \t/3400, val loss:9.66026496887207\n",
      "training on iteration:1388    \t/3400, val loss:7.337398052215576\n",
      "training on iteration:1389    \t/3400, val loss:12.782692909240723\n",
      "training on iteration:1390    \t/3400, val loss:13.901883125305176\n",
      "training on iteration:1391    \t/3400, val loss:6.713348865509033\n",
      "training on iteration:1392    \t/3400, val loss:11.774027824401855\n",
      "training on iteration:1393    \t/3400, val loss:8.998887062072754\n",
      "training on iteration:1394    \t/3400, val loss:8.578100204467773\n",
      "training on iteration:1395    \t/3400, val loss:13.281328201293945\n",
      "training on iteration:1396    \t/3400, val loss:10.010416984558105\n",
      "training on iteration:1397    \t/3400, val loss:3.6614158153533936\n",
      "training on iteration:1398    \t/3400, val loss:7.532157897949219\n",
      "training on iteration:1399    \t/3400, val loss:3.4809861183166504\n",
      "training on iteration:1400    \t/3400, val loss:8.291049003601074\n",
      "training on iteration:1401    \t/3400, val loss:5.719181537628174\n",
      "training on iteration:1402    \t/3400, val loss:5.4424967765808105\n",
      "training on iteration:1403    \t/3400, val loss:3.325286626815796\n",
      "training on iteration:1404    \t/3400, val loss:7.205662727355957\n",
      "training on iteration:1405    \t/3400, val loss:7.3920207023620605\n",
      "training on iteration:1406    \t/3400, val loss:3.1816647052764893\n",
      "training on iteration:1407    \t/3400, val loss:3.953894853591919\n",
      "training on iteration:1408    \t/3400, val loss:4.949253082275391\n",
      "training on iteration:1409    \t/3400, val loss:5.703517436981201\n",
      "training on iteration:1410    \t/3400, val loss:2.8748295307159424\n",
      "training on iteration:1411    \t/3400, val loss:3.059906244277954\n",
      "training on iteration:1412    \t/3400, val loss:5.381450653076172\n",
      "training on iteration:1413    \t/3400, val loss:4.086600303649902\n",
      "training on iteration:1414    \t/3400, val loss:2.897442102432251\n",
      "training on iteration:1415    \t/3400, val loss:2.983081340789795\n",
      "training on iteration:1416    \t/3400, val loss:3.9365429878234863\n",
      "training on iteration:1417    \t/3400, val loss:2.7340736389160156\n",
      "training on iteration:1418    \t/3400, val loss:3.2982218265533447\n",
      "training on iteration:1419    \t/3400, val loss:5.020415306091309\n",
      "training on iteration:1420    \t/3400, val loss:2.8386213779449463\n",
      "training on iteration:1421    \t/3400, val loss:2.8368141651153564\n",
      "training on iteration:1422    \t/3400, val loss:7.386105537414551\n",
      "training on iteration:1423    \t/3400, val loss:4.742026329040527\n",
      "training on iteration:1424    \t/3400, val loss:5.119479179382324\n",
      "training on iteration:1425    \t/3400, val loss:2.957225799560547\n",
      "training on iteration:1426    \t/3400, val loss:8.015467643737793\n",
      "training on iteration:1427    \t/3400, val loss:5.300143241882324\n",
      "training on iteration:1428    \t/3400, val loss:5.242796421051025\n",
      "training on iteration:1429    \t/3400, val loss:2.9426116943359375\n",
      "training on iteration:1430    \t/3400, val loss:10.128173828125\n",
      "training on iteration:1431    \t/3400, val loss:9.51720142364502\n",
      "training on iteration:1432    \t/3400, val loss:3.093378782272339\n",
      "training on iteration:1433    \t/3400, val loss:4.05914831161499\n",
      "training on iteration:1434    \t/3400, val loss:6.097476959228516\n",
      "training on iteration:1435    \t/3400, val loss:6.232947826385498\n",
      "training on iteration:1436    \t/3400, val loss:3.5816762447357178\n",
      "training on iteration:1437    \t/3400, val loss:2.955031633377075\n",
      "training on iteration:1438    \t/3400, val loss:6.807232856750488\n",
      "training on iteration:1439    \t/3400, val loss:3.8999381065368652\n",
      "training on iteration:1440    \t/3400, val loss:3.5716395378112793\n",
      "training on iteration:1441    \t/3400, val loss:3.639927387237549\n",
      "training on iteration:1442    \t/3400, val loss:5.077143669128418\n",
      "training on iteration:1443    \t/3400, val loss:2.9673056602478027\n",
      "training on iteration:1444    \t/3400, val loss:3.6812472343444824\n",
      "training on iteration:1445    \t/3400, val loss:4.13306188583374\n",
      "training on iteration:1446    \t/3400, val loss:2.899792194366455\n",
      "training on iteration:1447    \t/3400, val loss:3.401883840560913\n",
      "training on iteration:1448    \t/3400, val loss:5.165674686431885\n",
      "training on iteration:1449    \t/3400, val loss:2.905473232269287\n",
      "training on iteration:1450    \t/3400, val loss:2.9233200550079346\n",
      "training on iteration:1451    \t/3400, val loss:4.413252353668213\n",
      "training on iteration:1452    \t/3400, val loss:3.5047504901885986\n",
      "training on iteration:1453    \t/3400, val loss:2.965202569961548\n",
      "training on iteration:1454    \t/3400, val loss:3.8721446990966797\n",
      "training on iteration:1455    \t/3400, val loss:2.907325267791748\n",
      "training on iteration:1456    \t/3400, val loss:4.698355674743652\n",
      "training on iteration:1457    \t/3400, val loss:3.1502113342285156\n",
      "training on iteration:1458    \t/3400, val loss:2.8647007942199707\n",
      "training on iteration:1459    \t/3400, val loss:3.429222345352173\n",
      "training on iteration:1460    \t/3400, val loss:3.075395345687866\n",
      "training on iteration:1461    \t/3400, val loss:3.5758585929870605\n",
      "training on iteration:1462    \t/3400, val loss:3.0508759021759033\n",
      "training on iteration:1463    \t/3400, val loss:3.4171359539031982\n",
      "training on iteration:1464    \t/3400, val loss:3.5035207271575928\n",
      "training on iteration:1465    \t/3400, val loss:3.5068447589874268\n",
      "training on iteration:1466    \t/3400, val loss:3.4875848293304443\n",
      "training on iteration:1467    \t/3400, val loss:2.9482369422912598\n",
      "training on iteration:1468    \t/3400, val loss:3.0098159313201904\n",
      "training on iteration:1469    \t/3400, val loss:4.457509994506836\n",
      "training on iteration:1470    \t/3400, val loss:3.4400956630706787\n",
      "training on iteration:1471    \t/3400, val loss:2.9398136138916016\n",
      "training on iteration:1472    \t/3400, val loss:3.6681692600250244\n",
      "training on iteration:1473    \t/3400, val loss:4.0392022132873535\n",
      "training on iteration:1474    \t/3400, val loss:3.2021708488464355\n",
      "training on iteration:1475    \t/3400, val loss:3.1955676078796387\n",
      "training on iteration:1476    \t/3400, val loss:3.3038814067840576\n",
      "training on iteration:1477    \t/3400, val loss:2.9028353691101074\n",
      "training on iteration:1478    \t/3400, val loss:4.570223808288574\n",
      "training on iteration:1479    \t/3400, val loss:2.9665608406066895\n",
      "training on iteration:1480    \t/3400, val loss:3.2893691062927246\n",
      "training on iteration:1481    \t/3400, val loss:4.303347587585449\n",
      "training on iteration:1482    \t/3400, val loss:3.14585542678833\n",
      "training on iteration:1483    \t/3400, val loss:3.0020267963409424\n",
      "training on iteration:1484    \t/3400, val loss:3.5814719200134277\n",
      "training on iteration:1485    \t/3400, val loss:2.95507550239563\n",
      "training on iteration:1486    \t/3400, val loss:5.338344097137451\n",
      "training on iteration:1487    \t/3400, val loss:3.476646661758423\n",
      "training on iteration:1488    \t/3400, val loss:2.8985226154327393\n",
      "training on iteration:1489    \t/3400, val loss:4.212138652801514\n",
      "training on iteration:1490    \t/3400, val loss:4.135947227478027\n",
      "training on iteration:1491    \t/3400, val loss:3.8808577060699463\n",
      "training on iteration:1492    \t/3400, val loss:3.4781265258789062\n",
      "training on iteration:1493    \t/3400, val loss:3.635648488998413\n",
      "training on iteration:1494    \t/3400, val loss:2.948690414428711\n",
      "training on iteration:1495    \t/3400, val loss:4.844306468963623\n",
      "training on iteration:1496    \t/3400, val loss:3.318561315536499\n",
      "training on iteration:1497    \t/3400, val loss:2.9039852619171143\n",
      "training on iteration:1498    \t/3400, val loss:5.607926368713379\n",
      "training on iteration:1499    \t/3400, val loss:3.5691781044006348\n",
      "training on iteration:1500    \t/3400, val loss:4.73970365524292\n",
      "training on iteration:1501    \t/3400, val loss:3.7278409004211426\n",
      "training on iteration:1502    \t/3400, val loss:5.61214017868042\n",
      "training on iteration:1503    \t/3400, val loss:3.9891726970672607\n",
      "training on iteration:1504    \t/3400, val loss:3.621403932571411\n",
      "training on iteration:1505    \t/3400, val loss:5.538773536682129\n",
      "training on iteration:1506    \t/3400, val loss:4.779011249542236\n",
      "training on iteration:1507    \t/3400, val loss:3.1816132068634033\n",
      "training on iteration:1508    \t/3400, val loss:4.172610282897949\n",
      "training on iteration:1509    \t/3400, val loss:2.8970162868499756\n",
      "training on iteration:1510    \t/3400, val loss:4.0352606773376465\n",
      "training on iteration:1511    \t/3400, val loss:3.6873855590820312\n",
      "training on iteration:1512    \t/3400, val loss:3.2251296043395996\n",
      "training on iteration:1513    \t/3400, val loss:3.0409233570098877\n",
      "training on iteration:1514    \t/3400, val loss:3.970881938934326\n",
      "training on iteration:1515    \t/3400, val loss:3.7582876682281494\n",
      "training on iteration:1516    \t/3400, val loss:3.2426016330718994\n",
      "training on iteration:1517    \t/3400, val loss:3.1237950325012207\n",
      "training on iteration:1518    \t/3400, val loss:2.839468240737915\n",
      "training on iteration:1519    \t/3400, val loss:4.01658821105957\n",
      "training on iteration:1520    \t/3400, val loss:3.3238558769226074\n",
      "training on iteration:1521    \t/3400, val loss:2.8785247802734375\n",
      "training on iteration:1522    \t/3400, val loss:3.9237353801727295\n",
      "training on iteration:1523    \t/3400, val loss:3.0775539875030518\n",
      "training on iteration:1524    \t/3400, val loss:5.598598957061768\n",
      "training on iteration:1525    \t/3400, val loss:3.28230881690979\n",
      "training on iteration:1526    \t/3400, val loss:2.880983829498291\n",
      "training on iteration:1527    \t/3400, val loss:3.6597115993499756\n",
      "training on iteration:1528    \t/3400, val loss:3.159414529800415\n",
      "training on iteration:1529    \t/3400, val loss:3.634416103363037\n",
      "training on iteration:1530    \t/3400, val loss:3.49375319480896\n",
      "training on iteration:1531    \t/3400, val loss:3.2506184577941895\n",
      "training on iteration:1532    \t/3400, val loss:2.8681747913360596\n",
      "training on iteration:1533    \t/3400, val loss:5.489441394805908\n",
      "training on iteration:1534    \t/3400, val loss:3.639946222305298\n",
      "training on iteration:1535    \t/3400, val loss:3.020946502685547\n",
      "training on iteration:1536    \t/3400, val loss:4.647693634033203\n",
      "training on iteration:1537    \t/3400, val loss:4.094833850860596\n",
      "training on iteration:1538    \t/3400, val loss:4.980821132659912\n",
      "training on iteration:1539    \t/3400, val loss:3.4899139404296875\n",
      "training on iteration:1540    \t/3400, val loss:5.1844401359558105\n",
      "training on iteration:1541    \t/3400, val loss:4.751677513122559\n",
      "training on iteration:1542    \t/3400, val loss:3.2713890075683594\n",
      "training on iteration:1543    \t/3400, val loss:4.119226932525635\n",
      "training on iteration:1544    \t/3400, val loss:4.524722099304199\n",
      "training on iteration:1545    \t/3400, val loss:3.6827824115753174\n",
      "training on iteration:1546    \t/3400, val loss:4.256376266479492\n",
      "training on iteration:1547    \t/3400, val loss:3.922013759613037\n",
      "training on iteration:1548    \t/3400, val loss:4.543595790863037\n",
      "training on iteration:1549    \t/3400, val loss:3.5631823539733887\n",
      "training on iteration:1550    \t/3400, val loss:6.317912578582764\n",
      "training on iteration:1551    \t/3400, val loss:4.136866569519043\n",
      "training on iteration:1552    \t/3400, val loss:5.165966510772705\n",
      "training on iteration:1553    \t/3400, val loss:5.434047222137451\n",
      "training on iteration:1554    \t/3400, val loss:6.367572784423828\n",
      "training on iteration:1555    \t/3400, val loss:3.8623592853546143\n",
      "training on iteration:1556    \t/3400, val loss:2.9742722511291504\n",
      "training on iteration:1557    \t/3400, val loss:4.5638580322265625\n",
      "training on iteration:1558    \t/3400, val loss:3.4272477626800537\n",
      "training on iteration:1559    \t/3400, val loss:3.1259844303131104\n",
      "training on iteration:1560    \t/3400, val loss:3.2651207447052\n",
      "training on iteration:1561    \t/3400, val loss:3.1114883422851562\n",
      "training on iteration:1562    \t/3400, val loss:2.824702024459839\n",
      "training on iteration:1563    \t/3400, val loss:4.773560047149658\n",
      "training on iteration:1564    \t/3400, val loss:3.016226291656494\n",
      "training on iteration:1565    \t/3400, val loss:3.2027173042297363\n",
      "training on iteration:1566    \t/3400, val loss:3.7398416996002197\n",
      "training on iteration:1567    \t/3400, val loss:3.795499086380005\n",
      "training on iteration:1568    \t/3400, val loss:3.512239694595337\n",
      "training on iteration:1569    \t/3400, val loss:3.1124236583709717\n",
      "training on iteration:1570    \t/3400, val loss:3.2439162731170654\n",
      "training on iteration:1571    \t/3400, val loss:3.598734140396118\n",
      "training on iteration:1572    \t/3400, val loss:3.9526734352111816\n",
      "training on iteration:1573    \t/3400, val loss:2.7915802001953125\n",
      "training on iteration:1574    \t/3400, val loss:3.150808572769165\n",
      "training on iteration:1575    \t/3400, val loss:4.780020236968994\n",
      "training on iteration:1576    \t/3400, val loss:3.6726887226104736\n",
      "training on iteration:1577    \t/3400, val loss:3.2615437507629395\n",
      "training on iteration:1578    \t/3400, val loss:3.6705050468444824\n",
      "training on iteration:1579    \t/3400, val loss:2.8626673221588135\n",
      "training on iteration:1580    \t/3400, val loss:4.9344611167907715\n",
      "training on iteration:1581    \t/3400, val loss:3.482457399368286\n",
      "training on iteration:1582    \t/3400, val loss:2.819610357284546\n",
      "training on iteration:1583    \t/3400, val loss:5.970499515533447\n",
      "training on iteration:1584    \t/3400, val loss:3.865417957305908\n",
      "training on iteration:1585    \t/3400, val loss:5.233463287353516\n",
      "training on iteration:1586    \t/3400, val loss:3.818366527557373\n",
      "training on iteration:1587    \t/3400, val loss:6.520474910736084\n",
      "training on iteration:1588    \t/3400, val loss:4.0883378982543945\n",
      "training on iteration:1589    \t/3400, val loss:4.359210014343262\n",
      "training on iteration:1590    \t/3400, val loss:5.836759090423584\n",
      "training on iteration:1591    \t/3400, val loss:4.78490686416626\n",
      "training on iteration:1592    \t/3400, val loss:2.7735366821289062\n",
      "training on iteration:1593    \t/3400, val loss:4.804051876068115\n",
      "training on iteration:1594    \t/3400, val loss:3.705409526824951\n",
      "training on iteration:1595    \t/3400, val loss:3.9851813316345215\n",
      "training on iteration:1596    \t/3400, val loss:4.274176120758057\n",
      "training on iteration:1597    \t/3400, val loss:5.122124195098877\n",
      "training on iteration:1598    \t/3400, val loss:4.713953495025635\n",
      "training on iteration:1599    \t/3400, val loss:3.52974534034729\n",
      "training on iteration:1600    \t/3400, val loss:6.874753475189209\n",
      "training on iteration:1601    \t/3400, val loss:5.210988521575928\n",
      "training on iteration:1602    \t/3400, val loss:3.4798743724823\n",
      "training on iteration:1603    \t/3400, val loss:4.442019939422607\n",
      "training on iteration:1604    \t/3400, val loss:4.259610652923584\n",
      "training on iteration:1605    \t/3400, val loss:3.7999274730682373\n",
      "training on iteration:1606    \t/3400, val loss:4.4187726974487305\n",
      "training on iteration:1607    \t/3400, val loss:3.528024435043335\n",
      "training on iteration:1608    \t/3400, val loss:4.572622776031494\n",
      "training on iteration:1609    \t/3400, val loss:4.592364311218262\n",
      "training on iteration:1610    \t/3400, val loss:5.421833515167236\n",
      "training on iteration:1611    \t/3400, val loss:5.55230712890625\n",
      "training on iteration:1612    \t/3400, val loss:3.582412004470825\n",
      "training on iteration:1613    \t/3400, val loss:7.414463043212891\n",
      "training on iteration:1614    \t/3400, val loss:7.62106466293335\n",
      "training on iteration:1615    \t/3400, val loss:5.113050937652588\n",
      "training on iteration:1616    \t/3400, val loss:4.603472709655762\n",
      "training on iteration:1617    \t/3400, val loss:6.423938751220703\n",
      "training on iteration:1618    \t/3400, val loss:8.886663436889648\n",
      "training on iteration:1619    \t/3400, val loss:3.813511371612549\n",
      "training on iteration:1620    \t/3400, val loss:9.889068603515625\n",
      "training on iteration:1621    \t/3400, val loss:4.297999858856201\n",
      "training on iteration:1622    \t/3400, val loss:10.146041870117188\n",
      "training on iteration:1623    \t/3400, val loss:9.992325782775879\n",
      "training on iteration:1624    \t/3400, val loss:4.922853946685791\n",
      "training on iteration:1625    \t/3400, val loss:8.17187213897705\n",
      "training on iteration:1626    \t/3400, val loss:5.037961483001709\n",
      "training on iteration:1627    \t/3400, val loss:9.26561164855957\n",
      "training on iteration:1628    \t/3400, val loss:4.845559120178223\n",
      "training on iteration:1629    \t/3400, val loss:8.639531135559082\n",
      "training on iteration:1630    \t/3400, val loss:4.6046247482299805\n",
      "training on iteration:1631    \t/3400, val loss:11.323128700256348\n",
      "training on iteration:1632    \t/3400, val loss:12.886484146118164\n",
      "training on iteration:1633    \t/3400, val loss:5.7132039070129395\n",
      "training on iteration:1634    \t/3400, val loss:11.545663833618164\n",
      "training on iteration:1635    \t/3400, val loss:7.792891979217529\n",
      "training on iteration:1636    \t/3400, val loss:9.899843215942383\n",
      "training on iteration:1637    \t/3400, val loss:14.2261962890625\n",
      "training on iteration:1638    \t/3400, val loss:10.281621932983398\n",
      "training on iteration:1639    \t/3400, val loss:4.813930034637451\n",
      "training on iteration:1640    \t/3400, val loss:5.64639949798584\n",
      "training on iteration:1641    \t/3400, val loss:6.860785007476807\n",
      "training on iteration:1642    \t/3400, val loss:7.239107608795166\n",
      "training on iteration:1643    \t/3400, val loss:3.443528652191162\n",
      "training on iteration:1644    \t/3400, val loss:3.1190085411071777\n",
      "training on iteration:1645    \t/3400, val loss:4.982293128967285\n",
      "training on iteration:1646    \t/3400, val loss:4.667807579040527\n",
      "training on iteration:1647    \t/3400, val loss:3.32372784614563\n",
      "training on iteration:1648    \t/3400, val loss:3.2903456687927246\n",
      "training on iteration:1649    \t/3400, val loss:3.610105514526367\n",
      "training on iteration:1650    \t/3400, val loss:2.7649075984954834\n",
      "training on iteration:1651    \t/3400, val loss:3.803987979888916\n",
      "training on iteration:1652    \t/3400, val loss:4.420557498931885\n",
      "training on iteration:1653    \t/3400, val loss:2.7300896644592285\n",
      "training on iteration:1654    \t/3400, val loss:2.8413913249969482\n",
      "training on iteration:1655    \t/3400, val loss:3.1804988384246826\n",
      "training on iteration:1656    \t/3400, val loss:3.4795286655426025\n",
      "training on iteration:1657    \t/3400, val loss:2.91042160987854\n",
      "training on iteration:1658    \t/3400, val loss:3.1770803928375244\n",
      "training on iteration:1659    \t/3400, val loss:2.790811538696289\n",
      "training on iteration:1660    \t/3400, val loss:5.368434429168701\n",
      "training on iteration:1661    \t/3400, val loss:3.4463117122650146\n",
      "training on iteration:1662    \t/3400, val loss:2.8217434883117676\n",
      "training on iteration:1663    \t/3400, val loss:3.6743991374969482\n",
      "training on iteration:1664    \t/3400, val loss:3.4914374351501465\n",
      "training on iteration:1665    \t/3400, val loss:3.4232468605041504\n",
      "training on iteration:1666    \t/3400, val loss:3.0116894245147705\n",
      "training on iteration:1667    \t/3400, val loss:3.751838207244873\n",
      "training on iteration:1668    \t/3400, val loss:3.007227659225464\n",
      "training on iteration:1669    \t/3400, val loss:4.010955810546875\n",
      "training on iteration:1670    \t/3400, val loss:3.5372586250305176\n",
      "training on iteration:1671    \t/3400, val loss:2.846895456314087\n",
      "training on iteration:1672    \t/3400, val loss:3.1444942951202393\n",
      "training on iteration:1673    \t/3400, val loss:4.626992702484131\n",
      "training on iteration:1674    \t/3400, val loss:3.6028647422790527\n",
      "training on iteration:1675    \t/3400, val loss:2.8816421031951904\n",
      "training on iteration:1676    \t/3400, val loss:4.377295970916748\n",
      "training on iteration:1677    \t/3400, val loss:3.5392684936523438\n",
      "training on iteration:1678    \t/3400, val loss:3.448171854019165\n",
      "training on iteration:1679    \t/3400, val loss:3.9728262424468994\n",
      "training on iteration:1680    \t/3400, val loss:3.2920453548431396\n",
      "training on iteration:1681    \t/3400, val loss:2.9118168354034424\n",
      "training on iteration:1682    \t/3400, val loss:4.230960369110107\n",
      "training on iteration:1683    \t/3400, val loss:3.0435473918914795\n",
      "training on iteration:1684    \t/3400, val loss:3.039759874343872\n",
      "training on iteration:1685    \t/3400, val loss:4.831921577453613\n",
      "training on iteration:1686    \t/3400, val loss:2.886322021484375\n",
      "training on iteration:1687    \t/3400, val loss:2.9181771278381348\n",
      "training on iteration:1688    \t/3400, val loss:3.739320755004883\n",
      "training on iteration:1689    \t/3400, val loss:2.766237497329712\n",
      "training on iteration:1690    \t/3400, val loss:3.6218745708465576\n",
      "training on iteration:1691    \t/3400, val loss:3.095428466796875\n",
      "training on iteration:1692    \t/3400, val loss:3.024259090423584\n",
      "training on iteration:1693    \t/3400, val loss:2.8232645988464355\n",
      "training on iteration:1694    \t/3400, val loss:5.4629950523376465\n",
      "training on iteration:1695    \t/3400, val loss:2.8207952976226807\n",
      "training on iteration:1696    \t/3400, val loss:2.7489802837371826\n",
      "training on iteration:1697    \t/3400, val loss:4.226243019104004\n",
      "training on iteration:1698    \t/3400, val loss:2.8191869258880615\n",
      "training on iteration:1699    \t/3400, val loss:2.973602294921875\n",
      "training on iteration:1700    \t/3400, val loss:2.8498153686523438\n",
      "training on iteration:1701    \t/3400, val loss:3.0200746059417725\n",
      "training on iteration:1702    \t/3400, val loss:3.461345911026001\n",
      "training on iteration:1703    \t/3400, val loss:2.795849323272705\n",
      "training on iteration:1704    \t/3400, val loss:3.166858673095703\n",
      "training on iteration:1705    \t/3400, val loss:2.8863770961761475\n",
      "training on iteration:1706    \t/3400, val loss:3.117213010787964\n",
      "training on iteration:1707    \t/3400, val loss:3.6384615898132324\n",
      "training on iteration:1708    \t/3400, val loss:2.8071138858795166\n",
      "training on iteration:1709    \t/3400, val loss:2.9875614643096924\n",
      "training on iteration:1710    \t/3400, val loss:2.7397549152374268\n",
      "training on iteration:1711    \t/3400, val loss:5.424320220947266\n",
      "training on iteration:1712    \t/3400, val loss:2.8913328647613525\n",
      "training on iteration:1713    \t/3400, val loss:2.727945327758789\n",
      "training on iteration:1714    \t/3400, val loss:4.13811731338501\n",
      "training on iteration:1715    \t/3400, val loss:2.746724843978882\n",
      "training on iteration:1716    \t/3400, val loss:3.4280622005462646\n",
      "training on iteration:1717    \t/3400, val loss:2.8064465522766113\n",
      "training on iteration:1718    \t/3400, val loss:3.1563448905944824\n",
      "training on iteration:1719    \t/3400, val loss:3.030651807785034\n",
      "training on iteration:1720    \t/3400, val loss:2.991926431655884\n",
      "training on iteration:1721    \t/3400, val loss:2.888550281524658\n",
      "training on iteration:1722    \t/3400, val loss:2.867567777633667\n",
      "training on iteration:1723    \t/3400, val loss:3.0129072666168213\n",
      "training on iteration:1724    \t/3400, val loss:3.473905086517334\n",
      "training on iteration:1725    \t/3400, val loss:2.917868137359619\n",
      "training on iteration:1726    \t/3400, val loss:2.923860549926758\n",
      "training on iteration:1727    \t/3400, val loss:2.7411301136016846\n",
      "training on iteration:1728    \t/3400, val loss:5.8984808921813965\n",
      "training on iteration:1729    \t/3400, val loss:2.9486782550811768\n",
      "training on iteration:1730    \t/3400, val loss:2.767913341522217\n",
      "training on iteration:1731    \t/3400, val loss:4.429474353790283\n",
      "training on iteration:1732    \t/3400, val loss:2.9002346992492676\n",
      "training on iteration:1733    \t/3400, val loss:3.4105966091156006\n",
      "training on iteration:1734    \t/3400, val loss:4.031017780303955\n",
      "training on iteration:1735    \t/3400, val loss:2.8106024265289307\n",
      "training on iteration:1736    \t/3400, val loss:3.013957977294922\n",
      "training on iteration:1737    \t/3400, val loss:3.585891008377075\n",
      "training on iteration:1738    \t/3400, val loss:3.2769527435302734\n",
      "training on iteration:1739    \t/3400, val loss:2.873136281967163\n",
      "training on iteration:1740    \t/3400, val loss:3.117283344268799\n",
      "training on iteration:1741    \t/3400, val loss:3.2080955505371094\n",
      "training on iteration:1742    \t/3400, val loss:3.5845706462860107\n",
      "training on iteration:1743    \t/3400, val loss:2.950613498687744\n",
      "training on iteration:1744    \t/3400, val loss:3.486438512802124\n",
      "training on iteration:1745    \t/3400, val loss:5.346246242523193\n",
      "training on iteration:1746    \t/3400, val loss:3.3831787109375\n",
      "training on iteration:1747    \t/3400, val loss:5.091564655303955\n",
      "training on iteration:1748    \t/3400, val loss:3.3730967044830322\n",
      "training on iteration:1749    \t/3400, val loss:5.672579288482666\n",
      "training on iteration:1750    \t/3400, val loss:3.939373016357422\n",
      "training on iteration:1751    \t/3400, val loss:3.065800428390503\n",
      "training on iteration:1752    \t/3400, val loss:7.385812282562256\n",
      "training on iteration:1753    \t/3400, val loss:5.354843616485596\n",
      "training on iteration:1754    \t/3400, val loss:3.4209799766540527\n",
      "training on iteration:1755    \t/3400, val loss:3.400792360305786\n",
      "training on iteration:1756    \t/3400, val loss:3.8307058811187744\n",
      "training on iteration:1757    \t/3400, val loss:3.2401084899902344\n",
      "training on iteration:1758    \t/3400, val loss:4.649722576141357\n",
      "training on iteration:1759    \t/3400, val loss:4.1091694831848145\n",
      "training on iteration:1760    \t/3400, val loss:4.276508808135986\n",
      "training on iteration:1761    \t/3400, val loss:3.599876880645752\n",
      "training on iteration:1762    \t/3400, val loss:5.670176982879639\n",
      "training on iteration:1763    \t/3400, val loss:5.185549259185791\n",
      "training on iteration:1764    \t/3400, val loss:3.2387430667877197\n",
      "training on iteration:1765    \t/3400, val loss:6.496845722198486\n",
      "training on iteration:1766    \t/3400, val loss:7.64084005355835\n",
      "training on iteration:1767    \t/3400, val loss:4.874265193939209\n",
      "training on iteration:1768    \t/3400, val loss:5.417634010314941\n",
      "training on iteration:1769    \t/3400, val loss:6.599606990814209\n",
      "training on iteration:1770    \t/3400, val loss:7.266068458557129\n",
      "training on iteration:1771    \t/3400, val loss:3.8035855293273926\n",
      "training on iteration:1772    \t/3400, val loss:5.510077953338623\n",
      "training on iteration:1773    \t/3400, val loss:3.2977795600891113\n",
      "training on iteration:1774    \t/3400, val loss:4.694050312042236\n",
      "training on iteration:1775    \t/3400, val loss:4.366055965423584\n",
      "training on iteration:1776    \t/3400, val loss:2.7706196308135986\n",
      "training on iteration:1777    \t/3400, val loss:5.561753749847412\n",
      "training on iteration:1778    \t/3400, val loss:4.435258865356445\n",
      "training on iteration:1779    \t/3400, val loss:3.423170328140259\n",
      "training on iteration:1780    \t/3400, val loss:5.455291748046875\n",
      "training on iteration:1781    \t/3400, val loss:6.988165378570557\n",
      "training on iteration:1782    \t/3400, val loss:2.7595088481903076\n",
      "training on iteration:1783    \t/3400, val loss:3.6717724800109863\n",
      "training on iteration:1784    \t/3400, val loss:6.713509559631348\n",
      "training on iteration:1785    \t/3400, val loss:5.2348175048828125\n",
      "training on iteration:1786    \t/3400, val loss:5.423436641693115\n",
      "training on iteration:1787    \t/3400, val loss:3.0613441467285156\n",
      "training on iteration:1788    \t/3400, val loss:8.879383087158203\n",
      "training on iteration:1789    \t/3400, val loss:5.671600341796875\n",
      "training on iteration:1790    \t/3400, val loss:7.561546325683594\n",
      "training on iteration:1791    \t/3400, val loss:3.14604115486145\n",
      "training on iteration:1792    \t/3400, val loss:11.6302490234375\n",
      "training on iteration:1793    \t/3400, val loss:13.381543159484863\n",
      "training on iteration:1794    \t/3400, val loss:6.3416428565979\n",
      "training on iteration:1795    \t/3400, val loss:14.210126876831055\n",
      "training on iteration:1796    \t/3400, val loss:11.58370304107666\n",
      "training on iteration:1797    \t/3400, val loss:8.109018325805664\n",
      "training on iteration:1798    \t/3400, val loss:12.839834213256836\n",
      "training on iteration:1799    \t/3400, val loss:8.69566535949707\n",
      "training on iteration:1800    \t/3400, val loss:6.655707359313965\n",
      "training on iteration:1801    \t/3400, val loss:5.1520915031433105\n",
      "training on iteration:1802    \t/3400, val loss:8.687503814697266\n",
      "training on iteration:1803    \t/3400, val loss:10.624662399291992\n",
      "training on iteration:1804    \t/3400, val loss:3.901932954788208\n",
      "training on iteration:1805    \t/3400, val loss:9.771031379699707\n",
      "training on iteration:1806    \t/3400, val loss:5.215756416320801\n",
      "training on iteration:1807    \t/3400, val loss:9.270127296447754\n",
      "training on iteration:1808    \t/3400, val loss:11.299220085144043\n",
      "training on iteration:1809    \t/3400, val loss:5.402774333953857\n",
      "training on iteration:1810    \t/3400, val loss:7.147812366485596\n",
      "training on iteration:1811    \t/3400, val loss:3.6610300540924072\n",
      "training on iteration:1812    \t/3400, val loss:9.03941822052002\n",
      "training on iteration:1813    \t/3400, val loss:10.415926933288574\n",
      "training on iteration:1814    \t/3400, val loss:4.332127571105957\n",
      "training on iteration:1815    \t/3400, val loss:8.144538879394531\n",
      "training on iteration:1816    \t/3400, val loss:4.320776462554932\n",
      "training on iteration:1817    \t/3400, val loss:8.746021270751953\n",
      "training on iteration:1818    \t/3400, val loss:11.130486488342285\n",
      "training on iteration:1819    \t/3400, val loss:6.3771748542785645\n",
      "training on iteration:1820    \t/3400, val loss:5.111049175262451\n",
      "training on iteration:1821    \t/3400, val loss:3.852104663848877\n",
      "training on iteration:1822    \t/3400, val loss:7.698322772979736\n",
      "training on iteration:1823    \t/3400, val loss:8.609837532043457\n",
      "training on iteration:1824    \t/3400, val loss:4.771831035614014\n",
      "training on iteration:1825    \t/3400, val loss:4.961031436920166\n",
      "training on iteration:1826    \t/3400, val loss:4.593058109283447\n",
      "training on iteration:1827    \t/3400, val loss:6.6447434425354\n",
      "training on iteration:1828    \t/3400, val loss:4.737461566925049\n",
      "training on iteration:1829    \t/3400, val loss:6.563196659088135\n",
      "training on iteration:1830    \t/3400, val loss:5.204547882080078\n",
      "training on iteration:1831    \t/3400, val loss:7.121909141540527\n",
      "training on iteration:1832    \t/3400, val loss:7.252047538757324\n",
      "training on iteration:1833    \t/3400, val loss:4.096898555755615\n",
      "training on iteration:1834    \t/3400, val loss:5.420929908752441\n",
      "training on iteration:1835    \t/3400, val loss:3.9587366580963135\n",
      "training on iteration:1836    \t/3400, val loss:7.385041236877441\n",
      "training on iteration:1837    \t/3400, val loss:3.9559555053710938\n",
      "training on iteration:1838    \t/3400, val loss:3.676888942718506\n",
      "training on iteration:1839    \t/3400, val loss:3.520998477935791\n",
      "training on iteration:1840    \t/3400, val loss:5.928390026092529\n",
      "training on iteration:1841    \t/3400, val loss:4.025994300842285\n",
      "training on iteration:1842    \t/3400, val loss:4.641928195953369\n",
      "training on iteration:1843    \t/3400, val loss:2.7063472270965576\n",
      "training on iteration:1844    \t/3400, val loss:7.296284198760986\n",
      "training on iteration:1845    \t/3400, val loss:6.048526287078857\n",
      "training on iteration:1846    \t/3400, val loss:3.249741792678833\n",
      "training on iteration:1847    \t/3400, val loss:2.898247480392456\n",
      "training on iteration:1848    \t/3400, val loss:6.120277404785156\n",
      "training on iteration:1849    \t/3400, val loss:4.793554782867432\n",
      "training on iteration:1850    \t/3400, val loss:3.1682732105255127\n",
      "training on iteration:1851    \t/3400, val loss:3.2409915924072266\n",
      "training on iteration:1852    \t/3400, val loss:5.618455410003662\n",
      "training on iteration:1853    \t/3400, val loss:3.4233391284942627\n",
      "training on iteration:1854    \t/3400, val loss:3.6150310039520264\n",
      "training on iteration:1855    \t/3400, val loss:3.184225559234619\n",
      "training on iteration:1856    \t/3400, val loss:6.243685722351074\n",
      "training on iteration:1857    \t/3400, val loss:3.4214882850646973\n",
      "training on iteration:1858    \t/3400, val loss:4.069731712341309\n",
      "training on iteration:1859    \t/3400, val loss:3.0760483741760254\n",
      "training on iteration:1860    \t/3400, val loss:7.139119625091553\n",
      "training on iteration:1861    \t/3400, val loss:3.7517025470733643\n",
      "training on iteration:1862    \t/3400, val loss:4.5979390144348145\n",
      "training on iteration:1863    \t/3400, val loss:2.720020294189453\n",
      "training on iteration:1864    \t/3400, val loss:10.312363624572754\n",
      "training on iteration:1865    \t/3400, val loss:11.124105453491211\n",
      "training on iteration:1866    \t/3400, val loss:4.073975086212158\n",
      "training on iteration:1867    \t/3400, val loss:9.395262718200684\n",
      "training on iteration:1868    \t/3400, val loss:5.911850452423096\n",
      "training on iteration:1869    \t/3400, val loss:9.37006950378418\n",
      "training on iteration:1870    \t/3400, val loss:12.714034080505371\n",
      "training on iteration:1871    \t/3400, val loss:8.544899940490723\n",
      "training on iteration:1872    \t/3400, val loss:5.110795497894287\n",
      "training on iteration:1873    \t/3400, val loss:6.161323547363281\n",
      "training on iteration:1874    \t/3400, val loss:4.973287105560303\n",
      "training on iteration:1875    \t/3400, val loss:7.397549152374268\n",
      "training on iteration:1876    \t/3400, val loss:2.8132083415985107\n",
      "training on iteration:1877    \t/3400, val loss:4.268515110015869\n",
      "training on iteration:1878    \t/3400, val loss:3.1844446659088135\n",
      "training on iteration:1879    \t/3400, val loss:6.663873195648193\n",
      "training on iteration:1880    \t/3400, val loss:3.142345666885376\n",
      "training on iteration:1881    \t/3400, val loss:3.0132765769958496\n",
      "training on iteration:1882    \t/3400, val loss:3.0137016773223877\n",
      "training on iteration:1883    \t/3400, val loss:4.2459845542907715\n",
      "training on iteration:1884    \t/3400, val loss:2.7089343070983887\n",
      "training on iteration:1885    \t/3400, val loss:2.8182835578918457\n",
      "training on iteration:1886    \t/3400, val loss:4.1215314865112305\n",
      "training on iteration:1887    \t/3400, val loss:3.4259910583496094\n",
      "training on iteration:1888    \t/3400, val loss:2.7620434761047363\n",
      "training on iteration:1889    \t/3400, val loss:2.9952220916748047\n",
      "training on iteration:1890    \t/3400, val loss:5.386351585388184\n",
      "training on iteration:1891    \t/3400, val loss:3.0805442333221436\n",
      "training on iteration:1892    \t/3400, val loss:3.394726514816284\n",
      "training on iteration:1893    \t/3400, val loss:3.1118245124816895\n",
      "training on iteration:1894    \t/3400, val loss:5.297596454620361\n",
      "training on iteration:1895    \t/3400, val loss:2.909782648086548\n",
      "training on iteration:1896    \t/3400, val loss:2.7873170375823975\n",
      "training on iteration:1897    \t/3400, val loss:4.017281532287598\n",
      "training on iteration:1898    \t/3400, val loss:4.346696376800537\n",
      "training on iteration:1899    \t/3400, val loss:2.873616933822632\n",
      "training on iteration:1900    \t/3400, val loss:2.8449103832244873\n",
      "training on iteration:1901    \t/3400, val loss:4.0098419189453125\n",
      "training on iteration:1902    \t/3400, val loss:2.8570468425750732\n",
      "training on iteration:1903    \t/3400, val loss:2.972470998764038\n",
      "training on iteration:1904    \t/3400, val loss:3.7995188236236572\n",
      "training on iteration:1905    \t/3400, val loss:2.8707313537597656\n",
      "training on iteration:1906    \t/3400, val loss:3.0279388427734375\n",
      "training on iteration:1907    \t/3400, val loss:4.326069355010986\n",
      "training on iteration:1908    \t/3400, val loss:3.0473577976226807\n",
      "training on iteration:1909    \t/3400, val loss:2.88208270072937\n",
      "training on iteration:1910    \t/3400, val loss:3.520815134048462\n",
      "training on iteration:1911    \t/3400, val loss:3.8689117431640625\n",
      "training on iteration:1912    \t/3400, val loss:3.3275279998779297\n",
      "training on iteration:1913    \t/3400, val loss:2.8428237438201904\n",
      "training on iteration:1914    \t/3400, val loss:3.7622857093811035\n",
      "training on iteration:1915    \t/3400, val loss:3.5019760131835938\n",
      "training on iteration:1916    \t/3400, val loss:3.138219118118286\n",
      "training on iteration:1917    \t/3400, val loss:3.411883592605591\n",
      "training on iteration:1918    \t/3400, val loss:3.505185842514038\n",
      "training on iteration:1919    \t/3400, val loss:2.8583905696868896\n",
      "training on iteration:1920    \t/3400, val loss:3.1253068447113037\n",
      "training on iteration:1921    \t/3400, val loss:3.524231433868408\n",
      "training on iteration:1922    \t/3400, val loss:2.7681124210357666\n",
      "training on iteration:1923    \t/3400, val loss:3.6863467693328857\n",
      "training on iteration:1924    \t/3400, val loss:3.258949041366577\n",
      "training on iteration:1925    \t/3400, val loss:3.1338677406311035\n",
      "training on iteration:1926    \t/3400, val loss:2.8672499656677246\n",
      "training on iteration:1927    \t/3400, val loss:3.3407416343688965\n",
      "training on iteration:1928    \t/3400, val loss:2.852437734603882\n",
      "training on iteration:1929    \t/3400, val loss:3.3229713439941406\n",
      "training on iteration:1930    \t/3400, val loss:3.077979564666748\n",
      "training on iteration:1931    \t/3400, val loss:3.0174880027770996\n",
      "training on iteration:1932    \t/3400, val loss:5.182491302490234\n",
      "training on iteration:1933    \t/3400, val loss:3.643828868865967\n",
      "training on iteration:1934    \t/3400, val loss:4.299348831176758\n",
      "training on iteration:1935    \t/3400, val loss:3.0006840229034424\n",
      "training on iteration:1936    \t/3400, val loss:5.761960029602051\n",
      "training on iteration:1937    \t/3400, val loss:3.102168321609497\n",
      "training on iteration:1938    \t/3400, val loss:3.746655225753784\n",
      "training on iteration:1939    \t/3400, val loss:4.558351516723633\n",
      "training on iteration:1940    \t/3400, val loss:4.500582695007324\n",
      "training on iteration:1941    \t/3400, val loss:3.2566707134246826\n",
      "training on iteration:1942    \t/3400, val loss:2.923673391342163\n",
      "training on iteration:1943    \t/3400, val loss:3.3703529834747314\n",
      "training on iteration:1944    \t/3400, val loss:3.1690495014190674\n",
      "training on iteration:1945    \t/3400, val loss:3.5720131397247314\n",
      "training on iteration:1946    \t/3400, val loss:3.69120454788208\n",
      "training on iteration:1947    \t/3400, val loss:2.9741899967193604\n",
      "training on iteration:1948    \t/3400, val loss:2.846277952194214\n",
      "training on iteration:1949    \t/3400, val loss:6.506279945373535\n",
      "training on iteration:1950    \t/3400, val loss:3.0686514377593994\n",
      "training on iteration:1951    \t/3400, val loss:3.4912266731262207\n",
      "training on iteration:1952    \t/3400, val loss:3.736989736557007\n",
      "training on iteration:1953    \t/3400, val loss:3.7919437885284424\n",
      "training on iteration:1954    \t/3400, val loss:3.3329408168792725\n",
      "training on iteration:1955    \t/3400, val loss:2.963136911392212\n",
      "training on iteration:1956    \t/3400, val loss:4.245803356170654\n",
      "training on iteration:1957    \t/3400, val loss:3.127443552017212\n",
      "training on iteration:1958    \t/3400, val loss:2.981114149093628\n",
      "training on iteration:1959    \t/3400, val loss:3.978210926055908\n",
      "training on iteration:1960    \t/3400, val loss:3.0068230628967285\n",
      "training on iteration:1961    \t/3400, val loss:3.386383295059204\n",
      "training on iteration:1962    \t/3400, val loss:4.650784015655518\n",
      "training on iteration:1963    \t/3400, val loss:4.777857780456543\n",
      "training on iteration:1964    \t/3400, val loss:4.601937770843506\n",
      "training on iteration:1965    \t/3400, val loss:3.688123941421509\n",
      "training on iteration:1966    \t/3400, val loss:7.592212200164795\n",
      "training on iteration:1967    \t/3400, val loss:6.181019306182861\n",
      "training on iteration:1968    \t/3400, val loss:4.23276424407959\n",
      "training on iteration:1969    \t/3400, val loss:4.710789203643799\n",
      "training on iteration:1970    \t/3400, val loss:5.705061912536621\n",
      "training on iteration:1971    \t/3400, val loss:7.587148189544678\n",
      "training on iteration:1972    \t/3400, val loss:4.840912818908691\n",
      "training on iteration:1973    \t/3400, val loss:4.037054538726807\n",
      "training on iteration:1974    \t/3400, val loss:6.160295009613037\n",
      "training on iteration:1975    \t/3400, val loss:8.790131568908691\n",
      "training on iteration:1976    \t/3400, val loss:6.494778633117676\n",
      "training on iteration:1977    \t/3400, val loss:4.184749603271484\n",
      "training on iteration:1978    \t/3400, val loss:4.1020708084106445\n",
      "training on iteration:1979    \t/3400, val loss:6.935620307922363\n",
      "training on iteration:1980    \t/3400, val loss:6.064787864685059\n",
      "training on iteration:1981    \t/3400, val loss:4.666606426239014\n",
      "training on iteration:1982    \t/3400, val loss:3.2008743286132812\n",
      "training on iteration:1983    \t/3400, val loss:9.007104873657227\n",
      "training on iteration:1984    \t/3400, val loss:10.303067207336426\n",
      "training on iteration:1985    \t/3400, val loss:4.610020160675049\n",
      "training on iteration:1986    \t/3400, val loss:8.362397193908691\n",
      "training on iteration:1987    \t/3400, val loss:4.8559088706970215\n",
      "training on iteration:1988    \t/3400, val loss:8.938048362731934\n",
      "training on iteration:1989    \t/3400, val loss:10.869002342224121\n",
      "training on iteration:1990    \t/3400, val loss:5.275162220001221\n",
      "training on iteration:1991    \t/3400, val loss:5.591243743896484\n",
      "training on iteration:1992    \t/3400, val loss:3.4409053325653076\n",
      "training on iteration:1993    \t/3400, val loss:7.572213649749756\n",
      "training on iteration:1994    \t/3400, val loss:9.237313270568848\n",
      "training on iteration:1995    \t/3400, val loss:4.815541744232178\n",
      "training on iteration:1996    \t/3400, val loss:5.795426368713379\n",
      "training on iteration:1997    \t/3400, val loss:4.179120063781738\n",
      "training on iteration:1998    \t/3400, val loss:7.428855895996094\n",
      "training on iteration:1999    \t/3400, val loss:6.809738636016846\n",
      "training on iteration:2000    \t/3400, val loss:3.1099183559417725\n",
      "training on iteration:2001    \t/3400, val loss:3.5609588623046875\n",
      "training on iteration:2002    \t/3400, val loss:4.67925500869751\n",
      "training on iteration:2003    \t/3400, val loss:5.24771785736084\n",
      "training on iteration:2004    \t/3400, val loss:3.030195474624634\n",
      "training on iteration:2005    \t/3400, val loss:3.085841655731201\n",
      "training on iteration:2006    \t/3400, val loss:4.41944694519043\n",
      "training on iteration:2007    \t/3400, val loss:3.609936475753784\n",
      "training on iteration:2008    \t/3400, val loss:2.615572214126587\n",
      "training on iteration:2009    \t/3400, val loss:3.1669528484344482\n",
      "training on iteration:2010    \t/3400, val loss:4.1359405517578125\n",
      "training on iteration:2011    \t/3400, val loss:2.657235622406006\n",
      "training on iteration:2012    \t/3400, val loss:2.816267490386963\n",
      "training on iteration:2013    \t/3400, val loss:5.118908882141113\n",
      "training on iteration:2014    \t/3400, val loss:4.4351420402526855\n",
      "training on iteration:2015    \t/3400, val loss:3.5450499057769775\n",
      "training on iteration:2016    \t/3400, val loss:2.909963369369507\n",
      "training on iteration:2017    \t/3400, val loss:8.145853042602539\n",
      "training on iteration:2018    \t/3400, val loss:7.586634635925293\n",
      "training on iteration:2019    \t/3400, val loss:3.286407470703125\n",
      "training on iteration:2020    \t/3400, val loss:4.802558422088623\n",
      "training on iteration:2021    \t/3400, val loss:4.284393787384033\n",
      "training on iteration:2022    \t/3400, val loss:5.591646671295166\n",
      "training on iteration:2023    \t/3400, val loss:3.7447407245635986\n",
      "training on iteration:2024    \t/3400, val loss:2.8248705863952637\n",
      "training on iteration:2025    \t/3400, val loss:5.658263206481934\n",
      "training on iteration:2026    \t/3400, val loss:5.843520164489746\n",
      "training on iteration:2027    \t/3400, val loss:2.6904046535491943\n",
      "training on iteration:2028    \t/3400, val loss:3.7321231365203857\n",
      "training on iteration:2029    \t/3400, val loss:3.7320497035980225\n",
      "training on iteration:2030    \t/3400, val loss:5.016687870025635\n",
      "training on iteration:2031    \t/3400, val loss:3.009880542755127\n",
      "training on iteration:2032    \t/3400, val loss:2.778684139251709\n",
      "training on iteration:2033    \t/3400, val loss:4.145125389099121\n",
      "training on iteration:2034    \t/3400, val loss:4.139974117279053\n",
      "training on iteration:2035    \t/3400, val loss:2.7141366004943848\n",
      "training on iteration:2036    \t/3400, val loss:2.97182297706604\n",
      "training on iteration:2037    \t/3400, val loss:3.310844898223877\n",
      "training on iteration:2038    \t/3400, val loss:2.861886739730835\n",
      "training on iteration:2039    \t/3400, val loss:3.590097188949585\n",
      "training on iteration:2040    \t/3400, val loss:3.005164623260498\n",
      "training on iteration:2041    \t/3400, val loss:2.9536092281341553\n",
      "training on iteration:2042    \t/3400, val loss:3.0283215045928955\n",
      "training on iteration:2043    \t/3400, val loss:3.802610158920288\n",
      "training on iteration:2044    \t/3400, val loss:3.4127650260925293\n",
      "training on iteration:2045    \t/3400, val loss:2.743687868118286\n",
      "training on iteration:2046    \t/3400, val loss:3.3050639629364014\n",
      "training on iteration:2047    \t/3400, val loss:3.1888558864593506\n",
      "training on iteration:2048    \t/3400, val loss:3.9049251079559326\n",
      "training on iteration:2049    \t/3400, val loss:3.3240246772766113\n",
      "training on iteration:2050    \t/3400, val loss:2.9180657863616943\n",
      "training on iteration:2051    \t/3400, val loss:5.053067684173584\n",
      "training on iteration:2052    \t/3400, val loss:4.638088226318359\n",
      "training on iteration:2053    \t/3400, val loss:3.7218663692474365\n",
      "training on iteration:2054    \t/3400, val loss:3.5385565757751465\n",
      "training on iteration:2055    \t/3400, val loss:4.971155643463135\n",
      "training on iteration:2056    \t/3400, val loss:3.2623469829559326\n",
      "training on iteration:2057    \t/3400, val loss:3.332646608352661\n",
      "training on iteration:2058    \t/3400, val loss:3.8713772296905518\n",
      "training on iteration:2059    \t/3400, val loss:4.601293087005615\n",
      "training on iteration:2060    \t/3400, val loss:2.819343090057373\n",
      "training on iteration:2061    \t/3400, val loss:2.765475034713745\n",
      "training on iteration:2062    \t/3400, val loss:3.2665812969207764\n",
      "training on iteration:2063    \t/3400, val loss:2.732045888900757\n",
      "training on iteration:2064    \t/3400, val loss:3.467188596725464\n",
      "training on iteration:2065    \t/3400, val loss:3.800537109375\n",
      "training on iteration:2066    \t/3400, val loss:2.793471097946167\n",
      "training on iteration:2067    \t/3400, val loss:2.686143159866333\n",
      "training on iteration:2068    \t/3400, val loss:6.456392765045166\n",
      "training on iteration:2069    \t/3400, val loss:3.6661295890808105\n",
      "training on iteration:2070    \t/3400, val loss:4.074149131774902\n",
      "training on iteration:2071    \t/3400, val loss:2.97696852684021\n",
      "training on iteration:2072    \t/3400, val loss:4.904797554016113\n",
      "training on iteration:2073    \t/3400, val loss:2.753448009490967\n",
      "training on iteration:2074    \t/3400, val loss:2.8420252799987793\n",
      "training on iteration:2075    \t/3400, val loss:4.536505222320557\n",
      "training on iteration:2076    \t/3400, val loss:3.692147970199585\n",
      "training on iteration:2077    \t/3400, val loss:3.123424530029297\n",
      "training on iteration:2078    \t/3400, val loss:3.079740047454834\n",
      "training on iteration:2079    \t/3400, val loss:4.016888618469238\n",
      "training on iteration:2080    \t/3400, val loss:2.9199259281158447\n",
      "training on iteration:2081    \t/3400, val loss:3.1262543201446533\n",
      "training on iteration:2082    \t/3400, val loss:4.33626651763916\n",
      "training on iteration:2083    \t/3400, val loss:3.06838059425354\n",
      "training on iteration:2084    \t/3400, val loss:2.682800531387329\n",
      "training on iteration:2085    \t/3400, val loss:6.453653812408447\n",
      "training on iteration:2086    \t/3400, val loss:4.9187517166137695\n",
      "training on iteration:2087    \t/3400, val loss:3.3420021533966064\n",
      "training on iteration:2088    \t/3400, val loss:2.93245530128479\n",
      "training on iteration:2089    \t/3400, val loss:5.774456977844238\n",
      "training on iteration:2090    \t/3400, val loss:3.545191526412964\n",
      "training on iteration:2091    \t/3400, val loss:4.392935752868652\n",
      "training on iteration:2092    \t/3400, val loss:3.2545547485351562\n",
      "training on iteration:2093    \t/3400, val loss:5.888808250427246\n",
      "training on iteration:2094    \t/3400, val loss:3.5599148273468018\n",
      "training on iteration:2095    \t/3400, val loss:3.833488941192627\n",
      "training on iteration:2096    \t/3400, val loss:2.678475856781006\n",
      "training on iteration:2097    \t/3400, val loss:4.4612717628479\n",
      "training on iteration:2098    \t/3400, val loss:3.0024235248565674\n",
      "training on iteration:2099    \t/3400, val loss:2.9809978008270264\n",
      "training on iteration:2100    \t/3400, val loss:3.6129486560821533\n",
      "training on iteration:2101    \t/3400, val loss:2.89892578125\n",
      "training on iteration:2102    \t/3400, val loss:3.472450017929077\n",
      "training on iteration:2103    \t/3400, val loss:2.7211477756500244\n",
      "training on iteration:2104    \t/3400, val loss:2.9487335681915283\n",
      "training on iteration:2105    \t/3400, val loss:2.841876983642578\n",
      "training on iteration:2106    \t/3400, val loss:3.022700548171997\n",
      "training on iteration:2107    \t/3400, val loss:2.950326442718506\n",
      "training on iteration:2108    \t/3400, val loss:2.685492992401123\n",
      "training on iteration:2109    \t/3400, val loss:3.445373773574829\n",
      "training on iteration:2110    \t/3400, val loss:3.522967576980591\n",
      "training on iteration:2111    \t/3400, val loss:2.771832227706909\n",
      "training on iteration:2112    \t/3400, val loss:3.185314416885376\n",
      "training on iteration:2113    \t/3400, val loss:2.7559876441955566\n",
      "training on iteration:2114    \t/3400, val loss:2.7003579139709473\n",
      "training on iteration:2115    \t/3400, val loss:4.420236110687256\n",
      "training on iteration:2116    \t/3400, val loss:2.7149345874786377\n",
      "training on iteration:2117    \t/3400, val loss:2.6461122035980225\n",
      "training on iteration:2118    \t/3400, val loss:3.2629892826080322\n",
      "training on iteration:2119    \t/3400, val loss:3.9311764240264893\n",
      "training on iteration:2120    \t/3400, val loss:2.858945608139038\n",
      "training on iteration:2121    \t/3400, val loss:2.7005226612091064\n",
      "training on iteration:2122    \t/3400, val loss:3.509328603744507\n",
      "training on iteration:2123    \t/3400, val loss:2.63081693649292\n",
      "training on iteration:2124    \t/3400, val loss:3.1169707775115967\n",
      "training on iteration:2125    \t/3400, val loss:2.808866500854492\n",
      "training on iteration:2126    \t/3400, val loss:2.7273471355438232\n",
      "training on iteration:2127    \t/3400, val loss:3.041015148162842\n",
      "training on iteration:2128    \t/3400, val loss:3.0142064094543457\n",
      "training on iteration:2129    \t/3400, val loss:2.748479127883911\n",
      "training on iteration:2130    \t/3400, val loss:2.7090258598327637\n",
      "training on iteration:2131    \t/3400, val loss:2.8341236114501953\n",
      "training on iteration:2132    \t/3400, val loss:2.950827121734619\n",
      "training on iteration:2133    \t/3400, val loss:3.380140542984009\n",
      "training on iteration:2134    \t/3400, val loss:2.7079827785491943\n",
      "training on iteration:2135    \t/3400, val loss:2.8981354236602783\n",
      "training on iteration:2136    \t/3400, val loss:4.767462253570557\n",
      "training on iteration:2137    \t/3400, val loss:2.8068878650665283\n",
      "training on iteration:2138    \t/3400, val loss:2.9340603351593018\n",
      "training on iteration:2139    \t/3400, val loss:3.197723150253296\n",
      "training on iteration:2140    \t/3400, val loss:2.879066228866577\n",
      "training on iteration:2141    \t/3400, val loss:2.6812760829925537\n",
      "training on iteration:2142    \t/3400, val loss:3.0175833702087402\n",
      "training on iteration:2143    \t/3400, val loss:2.819260835647583\n",
      "training on iteration:2144    \t/3400, val loss:3.0354812145233154\n",
      "training on iteration:2145    \t/3400, val loss:2.68583345413208\n",
      "training on iteration:2146    \t/3400, val loss:2.69557785987854\n",
      "training on iteration:2147    \t/3400, val loss:2.6780736446380615\n",
      "training on iteration:2148    \t/3400, val loss:2.6364238262176514\n",
      "training on iteration:2149    \t/3400, val loss:3.9243030548095703\n",
      "training on iteration:2150    \t/3400, val loss:2.7694454193115234\n",
      "training on iteration:2151    \t/3400, val loss:2.7593631744384766\n",
      "training on iteration:2152    \t/3400, val loss:2.9294488430023193\n",
      "training on iteration:2153    \t/3400, val loss:3.2685091495513916\n",
      "training on iteration:2154    \t/3400, val loss:2.680783748626709\n",
      "training on iteration:2155    \t/3400, val loss:3.2431514263153076\n",
      "training on iteration:2156    \t/3400, val loss:2.6601850986480713\n",
      "training on iteration:2157    \t/3400, val loss:2.649352550506592\n",
      "training on iteration:2158    \t/3400, val loss:3.0484509468078613\n",
      "training on iteration:2159    \t/3400, val loss:2.6497199535369873\n",
      "training on iteration:2160    \t/3400, val loss:3.3179991245269775\n",
      "training on iteration:2161    \t/3400, val loss:2.928955554962158\n",
      "training on iteration:2162    \t/3400, val loss:2.653010129928589\n",
      "training on iteration:2163    \t/3400, val loss:3.540519952774048\n",
      "training on iteration:2164    \t/3400, val loss:2.6146626472473145\n",
      "training on iteration:2165    \t/3400, val loss:2.5989232063293457\n",
      "training on iteration:2166    \t/3400, val loss:5.587651252746582\n",
      "training on iteration:2167    \t/3400, val loss:2.7230396270751953\n",
      "training on iteration:2168    \t/3400, val loss:3.4452874660491943\n",
      "training on iteration:2169    \t/3400, val loss:3.687861204147339\n",
      "training on iteration:2170    \t/3400, val loss:3.4470388889312744\n",
      "training on iteration:2171    \t/3400, val loss:3.1664416790008545\n",
      "training on iteration:2172    \t/3400, val loss:3.01757550239563\n",
      "training on iteration:2173    \t/3400, val loss:3.2569046020507812\n",
      "training on iteration:2174    \t/3400, val loss:2.7513155937194824\n",
      "training on iteration:2175    \t/3400, val loss:4.039677619934082\n",
      "training on iteration:2176    \t/3400, val loss:2.798091411590576\n",
      "training on iteration:2177    \t/3400, val loss:2.7911152839660645\n",
      "training on iteration:2178    \t/3400, val loss:3.7405178546905518\n",
      "training on iteration:2179    \t/3400, val loss:2.6501104831695557\n",
      "training on iteration:2180    \t/3400, val loss:2.8715250492095947\n",
      "training on iteration:2181    \t/3400, val loss:3.028012752532959\n",
      "training on iteration:2182    \t/3400, val loss:2.804725408554077\n",
      "training on iteration:2183    \t/3400, val loss:3.980592966079712\n",
      "training on iteration:2184    \t/3400, val loss:2.7400577068328857\n",
      "training on iteration:2185    \t/3400, val loss:2.679447889328003\n",
      "training on iteration:2186    \t/3400, val loss:3.458073377609253\n",
      "training on iteration:2187    \t/3400, val loss:2.867020845413208\n",
      "training on iteration:2188    \t/3400, val loss:2.589247941970825\n",
      "training on iteration:2189    \t/3400, val loss:3.27174973487854\n",
      "training on iteration:2190    \t/3400, val loss:2.569681406021118\n",
      "training on iteration:2191    \t/3400, val loss:2.740326404571533\n",
      "training on iteration:2192    \t/3400, val loss:3.1813771724700928\n",
      "training on iteration:2193    \t/3400, val loss:2.8052902221679688\n",
      "training on iteration:2194    \t/3400, val loss:3.2986972332000732\n",
      "training on iteration:2195    \t/3400, val loss:3.092578887939453\n",
      "training on iteration:2196    \t/3400, val loss:2.754357099533081\n",
      "training on iteration:2197    \t/3400, val loss:3.4474868774414062\n",
      "training on iteration:2198    \t/3400, val loss:2.5777411460876465\n",
      "training on iteration:2199    \t/3400, val loss:2.592855453491211\n",
      "training on iteration:2200    \t/3400, val loss:5.5444464683532715\n",
      "training on iteration:2201    \t/3400, val loss:2.84838604927063\n",
      "training on iteration:2202    \t/3400, val loss:3.9832184314727783\n",
      "training on iteration:2203    \t/3400, val loss:3.9134433269500732\n",
      "training on iteration:2204    \t/3400, val loss:3.942645311355591\n",
      "training on iteration:2205    \t/3400, val loss:3.719705820083618\n",
      "training on iteration:2206    \t/3400, val loss:2.9522056579589844\n",
      "training on iteration:2207    \t/3400, val loss:4.155701160430908\n",
      "training on iteration:2208    \t/3400, val loss:3.160971164703369\n",
      "training on iteration:2209    \t/3400, val loss:3.598958969116211\n",
      "training on iteration:2210    \t/3400, val loss:2.8115200996398926\n",
      "training on iteration:2211    \t/3400, val loss:2.698951244354248\n",
      "training on iteration:2212    \t/3400, val loss:3.5157439708709717\n",
      "training on iteration:2213    \t/3400, val loss:3.1053550243377686\n",
      "training on iteration:2214    \t/3400, val loss:2.9770894050598145\n",
      "training on iteration:2215    \t/3400, val loss:3.041374921798706\n",
      "training on iteration:2216    \t/3400, val loss:2.7393577098846436\n",
      "training on iteration:2217    \t/3400, val loss:2.651197910308838\n",
      "training on iteration:2218    \t/3400, val loss:4.282776832580566\n",
      "training on iteration:2219    \t/3400, val loss:2.622824192047119\n",
      "training on iteration:2220    \t/3400, val loss:2.9290387630462646\n",
      "training on iteration:2221    \t/3400, val loss:5.890294075012207\n",
      "training on iteration:2222    \t/3400, val loss:4.35040807723999\n",
      "training on iteration:2223    \t/3400, val loss:4.784384250640869\n",
      "training on iteration:2224    \t/3400, val loss:3.1974704265594482\n",
      "training on iteration:2225    \t/3400, val loss:6.577938079833984\n",
      "training on iteration:2226    \t/3400, val loss:3.203867197036743\n",
      "training on iteration:2227    \t/3400, val loss:5.202912330627441\n",
      "training on iteration:2228    \t/3400, val loss:3.3367297649383545\n",
      "training on iteration:2229    \t/3400, val loss:6.896461486816406\n",
      "training on iteration:2230    \t/3400, val loss:2.785846710205078\n",
      "training on iteration:2231    \t/3400, val loss:4.537551403045654\n",
      "training on iteration:2232    \t/3400, val loss:3.10119891166687\n",
      "training on iteration:2233    \t/3400, val loss:4.08728551864624\n",
      "training on iteration:2234    \t/3400, val loss:3.0079493522644043\n",
      "training on iteration:2235    \t/3400, val loss:3.1824352741241455\n",
      "training on iteration:2236    \t/3400, val loss:3.4219253063201904\n",
      "training on iteration:2237    \t/3400, val loss:3.4974191188812256\n",
      "training on iteration:2238    \t/3400, val loss:4.2480340003967285\n",
      "training on iteration:2239    \t/3400, val loss:3.6709930896759033\n",
      "training on iteration:2240    \t/3400, val loss:3.2330944538116455\n",
      "training on iteration:2241    \t/3400, val loss:3.0251388549804688\n",
      "training on iteration:2242    \t/3400, val loss:4.061228275299072\n",
      "training on iteration:2243    \t/3400, val loss:2.8314530849456787\n",
      "training on iteration:2244    \t/3400, val loss:3.1066062450408936\n",
      "training on iteration:2245    \t/3400, val loss:3.934345006942749\n",
      "training on iteration:2246    \t/3400, val loss:2.5807712078094482\n",
      "training on iteration:2247    \t/3400, val loss:3.0302464962005615\n",
      "training on iteration:2248    \t/3400, val loss:2.9376885890960693\n",
      "training on iteration:2249    \t/3400, val loss:2.6015727519989014\n",
      "training on iteration:2250    \t/3400, val loss:3.214839458465576\n",
      "training on iteration:2251    \t/3400, val loss:3.3080942630767822\n",
      "training on iteration:2252    \t/3400, val loss:2.7213666439056396\n",
      "training on iteration:2253    \t/3400, val loss:2.799337863922119\n",
      "training on iteration:2254    \t/3400, val loss:2.7352404594421387\n",
      "training on iteration:2255    \t/3400, val loss:3.9540045261383057\n",
      "training on iteration:2256    \t/3400, val loss:2.644442319869995\n",
      "training on iteration:2257    \t/3400, val loss:2.9517343044281006\n",
      "training on iteration:2258    \t/3400, val loss:2.7152085304260254\n",
      "training on iteration:2259    \t/3400, val loss:2.6812329292297363\n",
      "training on iteration:2260    \t/3400, val loss:2.958538055419922\n",
      "training on iteration:2261    \t/3400, val loss:2.6237378120422363\n",
      "training on iteration:2262    \t/3400, val loss:3.037184000015259\n",
      "training on iteration:2263    \t/3400, val loss:2.9778671264648438\n",
      "training on iteration:2264    \t/3400, val loss:2.583343982696533\n",
      "training on iteration:2265    \t/3400, val loss:3.4924826622009277\n",
      "training on iteration:2266    \t/3400, val loss:2.5572216510772705\n",
      "training on iteration:2267    \t/3400, val loss:2.582366704940796\n",
      "training on iteration:2268    \t/3400, val loss:5.893033504486084\n",
      "training on iteration:2269    \t/3400, val loss:2.8695480823516846\n",
      "training on iteration:2270    \t/3400, val loss:4.469722270965576\n",
      "training on iteration:2271    \t/3400, val loss:3.4492874145507812\n",
      "training on iteration:2272    \t/3400, val loss:4.4685187339782715\n",
      "training on iteration:2273    \t/3400, val loss:3.7008683681488037\n",
      "training on iteration:2274    \t/3400, val loss:2.6076743602752686\n",
      "training on iteration:2275    \t/3400, val loss:4.470885276794434\n",
      "training on iteration:2276    \t/3400, val loss:2.904264211654663\n",
      "training on iteration:2277    \t/3400, val loss:3.049678325653076\n",
      "training on iteration:2278    \t/3400, val loss:3.120321035385132\n",
      "training on iteration:2279    \t/3400, val loss:2.8847148418426514\n",
      "training on iteration:2280    \t/3400, val loss:3.460326671600342\n",
      "training on iteration:2281    \t/3400, val loss:3.1556589603424072\n",
      "training on iteration:2282    \t/3400, val loss:2.9767284393310547\n",
      "training on iteration:2283    \t/3400, val loss:3.001023769378662\n",
      "training on iteration:2284    \t/3400, val loss:2.854637384414673\n",
      "training on iteration:2285    \t/3400, val loss:2.6331915855407715\n",
      "training on iteration:2286    \t/3400, val loss:3.6878130435943604\n",
      "training on iteration:2287    \t/3400, val loss:2.6309616565704346\n",
      "training on iteration:2288    \t/3400, val loss:3.049927234649658\n",
      "training on iteration:2289    \t/3400, val loss:5.778416156768799\n",
      "training on iteration:2290    \t/3400, val loss:3.7290589809417725\n",
      "training on iteration:2291    \t/3400, val loss:4.783797264099121\n",
      "training on iteration:2292    \t/3400, val loss:2.9963126182556152\n",
      "training on iteration:2293    \t/3400, val loss:5.868326663970947\n",
      "training on iteration:2294    \t/3400, val loss:2.6352438926696777\n",
      "training on iteration:2295    \t/3400, val loss:2.598665952682495\n",
      "training on iteration:2296    \t/3400, val loss:5.11738920211792\n",
      "training on iteration:2297    \t/3400, val loss:2.9167442321777344\n",
      "training on iteration:2298    \t/3400, val loss:2.6209044456481934\n",
      "training on iteration:2299    \t/3400, val loss:4.5429229736328125\n",
      "training on iteration:2300    \t/3400, val loss:2.8444175720214844\n",
      "training on iteration:2301    \t/3400, val loss:4.176422595977783\n",
      "training on iteration:2302    \t/3400, val loss:4.668450832366943\n",
      "training on iteration:2303    \t/3400, val loss:5.413181781768799\n",
      "training on iteration:2304    \t/3400, val loss:3.8995563983917236\n",
      "training on iteration:2305    \t/3400, val loss:2.7919349670410156\n",
      "training on iteration:2306    \t/3400, val loss:7.782525062561035\n",
      "training on iteration:2307    \t/3400, val loss:5.771807670593262\n",
      "training on iteration:2308    \t/3400, val loss:6.4303202629089355\n",
      "training on iteration:2309    \t/3400, val loss:4.106153964996338\n",
      "training on iteration:2310    \t/3400, val loss:8.877561569213867\n",
      "training on iteration:2311    \t/3400, val loss:9.71160888671875\n",
      "training on iteration:2312    \t/3400, val loss:2.9689037799835205\n",
      "training on iteration:2313    \t/3400, val loss:8.497021675109863\n",
      "training on iteration:2314    \t/3400, val loss:3.2112762928009033\n",
      "training on iteration:2315    \t/3400, val loss:11.008039474487305\n",
      "training on iteration:2316    \t/3400, val loss:13.053667068481445\n",
      "training on iteration:2317    \t/3400, val loss:7.8045549392700195\n",
      "training on iteration:2318    \t/3400, val loss:8.097129821777344\n",
      "training on iteration:2319    \t/3400, val loss:8.418606758117676\n",
      "training on iteration:2320    \t/3400, val loss:5.798184394836426\n",
      "training on iteration:2321    \t/3400, val loss:8.833844184875488\n",
      "training on iteration:2322    \t/3400, val loss:3.9398858547210693\n",
      "training on iteration:2323    \t/3400, val loss:7.474692344665527\n",
      "training on iteration:2324    \t/3400, val loss:4.645483493804932\n",
      "training on iteration:2325    \t/3400, val loss:7.47409725189209\n",
      "training on iteration:2326    \t/3400, val loss:9.245255470275879\n",
      "training on iteration:2327    \t/3400, val loss:3.8309762477874756\n",
      "training on iteration:2328    \t/3400, val loss:7.641512870788574\n",
      "training on iteration:2329    \t/3400, val loss:4.974231243133545\n",
      "training on iteration:2330    \t/3400, val loss:6.893807888031006\n",
      "training on iteration:2331    \t/3400, val loss:9.650774955749512\n",
      "training on iteration:2332    \t/3400, val loss:5.403897762298584\n",
      "training on iteration:2333    \t/3400, val loss:5.661513328552246\n",
      "training on iteration:2334    \t/3400, val loss:6.2269768714904785\n",
      "training on iteration:2335    \t/3400, val loss:4.193544864654541\n",
      "training on iteration:2336    \t/3400, val loss:6.9500555992126465\n",
      "training on iteration:2337    \t/3400, val loss:3.1964058876037598\n",
      "training on iteration:2338    \t/3400, val loss:4.047539710998535\n",
      "training on iteration:2339    \t/3400, val loss:2.7088401317596436\n",
      "training on iteration:2340    \t/3400, val loss:7.598133563995361\n",
      "training on iteration:2341    \t/3400, val loss:8.839078903198242\n",
      "training on iteration:2342    \t/3400, val loss:3.4975249767303467\n",
      "training on iteration:2343    \t/3400, val loss:5.925108909606934\n",
      "training on iteration:2344    \t/3400, val loss:3.3859169483184814\n",
      "training on iteration:2345    \t/3400, val loss:7.4148712158203125\n",
      "training on iteration:2346    \t/3400, val loss:8.445136070251465\n",
      "training on iteration:2347    \t/3400, val loss:2.850536823272705\n",
      "training on iteration:2348    \t/3400, val loss:4.267492771148682\n",
      "training on iteration:2349    \t/3400, val loss:2.55958890914917\n",
      "training on iteration:2350    \t/3400, val loss:6.684916973114014\n",
      "training on iteration:2351    \t/3400, val loss:5.1247076988220215\n",
      "training on iteration:2352    \t/3400, val loss:3.851449966430664\n",
      "training on iteration:2353    \t/3400, val loss:2.896117925643921\n",
      "training on iteration:2354    \t/3400, val loss:6.972782135009766\n",
      "training on iteration:2355    \t/3400, val loss:6.467106819152832\n",
      "training on iteration:2356    \t/3400, val loss:3.0919954776763916\n",
      "training on iteration:2357    \t/3400, val loss:2.684326648712158\n",
      "training on iteration:2358    \t/3400, val loss:5.635821342468262\n",
      "training on iteration:2359    \t/3400, val loss:4.966043949127197\n",
      "training on iteration:2360    \t/3400, val loss:2.9038617610931396\n",
      "training on iteration:2361    \t/3400, val loss:2.8793609142303467\n",
      "training on iteration:2362    \t/3400, val loss:5.459930896759033\n",
      "training on iteration:2363    \t/3400, val loss:4.20558500289917\n",
      "training on iteration:2364    \t/3400, val loss:3.0535202026367188\n",
      "training on iteration:2365    \t/3400, val loss:2.6708502769470215\n",
      "training on iteration:2366    \t/3400, val loss:6.269786834716797\n",
      "training on iteration:2367    \t/3400, val loss:4.620577335357666\n",
      "training on iteration:2368    \t/3400, val loss:3.876201629638672\n",
      "training on iteration:2369    \t/3400, val loss:3.041426181793213\n",
      "training on iteration:2370    \t/3400, val loss:6.717502117156982\n",
      "training on iteration:2371    \t/3400, val loss:6.241043567657471\n",
      "training on iteration:2372    \t/3400, val loss:2.9569382667541504\n",
      "training on iteration:2373    \t/3400, val loss:3.3642830848693848\n",
      "training on iteration:2374    \t/3400, val loss:5.877974033355713\n",
      "training on iteration:2375    \t/3400, val loss:6.441847801208496\n",
      "training on iteration:2376    \t/3400, val loss:3.035693645477295\n",
      "training on iteration:2377    \t/3400, val loss:3.728893995285034\n",
      "training on iteration:2378    \t/3400, val loss:4.315432548522949\n",
      "training on iteration:2379    \t/3400, val loss:6.22115421295166\n",
      "training on iteration:2380    \t/3400, val loss:2.8795583248138428\n",
      "training on iteration:2381    \t/3400, val loss:3.394641876220703\n",
      "training on iteration:2382    \t/3400, val loss:3.2535855770111084\n",
      "training on iteration:2383    \t/3400, val loss:5.7034807205200195\n",
      "training on iteration:2384    \t/3400, val loss:2.9017679691314697\n",
      "training on iteration:2385    \t/3400, val loss:3.8550972938537598\n",
      "training on iteration:2386    \t/3400, val loss:2.710888624191284\n",
      "training on iteration:2387    \t/3400, val loss:6.299110412597656\n",
      "training on iteration:2388    \t/3400, val loss:3.9722354412078857\n",
      "training on iteration:2389    \t/3400, val loss:3.8536274433135986\n",
      "training on iteration:2390    \t/3400, val loss:2.8399441242218018\n",
      "training on iteration:2391    \t/3400, val loss:8.099217414855957\n",
      "training on iteration:2392    \t/3400, val loss:8.72956371307373\n",
      "training on iteration:2393    \t/3400, val loss:2.8408493995666504\n",
      "training on iteration:2394    \t/3400, val loss:6.325718879699707\n",
      "training on iteration:2395    \t/3400, val loss:2.881796360015869\n",
      "training on iteration:2396    \t/3400, val loss:8.460485458374023\n",
      "training on iteration:2397    \t/3400, val loss:9.18688678741455\n",
      "training on iteration:2398    \t/3400, val loss:3.2374937534332275\n",
      "training on iteration:2399    \t/3400, val loss:5.341336727142334\n",
      "training on iteration:2400    \t/3400, val loss:2.965747117996216\n",
      "training on iteration:2401    \t/3400, val loss:7.415493488311768\n",
      "training on iteration:2402    \t/3400, val loss:8.030035972595215\n",
      "training on iteration:2403    \t/3400, val loss:2.679142713546753\n",
      "training on iteration:2404    \t/3400, val loss:4.758457183837891\n",
      "training on iteration:2405    \t/3400, val loss:2.675261974334717\n",
      "training on iteration:2406    \t/3400, val loss:6.861155986785889\n",
      "training on iteration:2407    \t/3400, val loss:4.857868671417236\n",
      "training on iteration:2408    \t/3400, val loss:3.9397635459899902\n",
      "training on iteration:2409    \t/3400, val loss:3.213285207748413\n",
      "training on iteration:2410    \t/3400, val loss:5.26096248626709\n",
      "training on iteration:2411    \t/3400, val loss:5.93980073928833\n",
      "training on iteration:2412    \t/3400, val loss:2.607978105545044\n",
      "training on iteration:2413    \t/3400, val loss:3.186751127243042\n",
      "training on iteration:2414    \t/3400, val loss:3.615978717803955\n",
      "training on iteration:2415    \t/3400, val loss:4.65621280670166\n",
      "training on iteration:2416    \t/3400, val loss:2.6654205322265625\n",
      "training on iteration:2417    \t/3400, val loss:2.6175875663757324\n",
      "training on iteration:2418    \t/3400, val loss:3.337768316268921\n",
      "training on iteration:2419    \t/3400, val loss:3.07275128364563\n",
      "training on iteration:2420    \t/3400, val loss:2.682756185531616\n",
      "training on iteration:2421    \t/3400, val loss:3.329972982406616\n",
      "training on iteration:2422    \t/3400, val loss:4.1052751541137695\n",
      "training on iteration:2423    \t/3400, val loss:2.7666027545928955\n",
      "training on iteration:2424    \t/3400, val loss:2.7492852210998535\n",
      "training on iteration:2425    \t/3400, val loss:5.45150899887085\n",
      "training on iteration:2426    \t/3400, val loss:4.190349578857422\n",
      "training on iteration:2427    \t/3400, val loss:3.060533285140991\n",
      "training on iteration:2428    \t/3400, val loss:2.8588063716888428\n",
      "training on iteration:2429    \t/3400, val loss:4.9059906005859375\n",
      "training on iteration:2430    \t/3400, val loss:3.4714088439941406\n",
      "training on iteration:2431    \t/3400, val loss:3.4273569583892822\n",
      "training on iteration:2432    \t/3400, val loss:2.7100727558135986\n",
      "training on iteration:2433    \t/3400, val loss:6.407142162322998\n",
      "training on iteration:2434    \t/3400, val loss:4.552810192108154\n",
      "training on iteration:2435    \t/3400, val loss:3.8099403381347656\n",
      "training on iteration:2436    \t/3400, val loss:3.1941676139831543\n",
      "training on iteration:2437    \t/3400, val loss:5.517725467681885\n",
      "training on iteration:2438    \t/3400, val loss:4.818289756774902\n",
      "training on iteration:2439    \t/3400, val loss:3.356044054031372\n",
      "training on iteration:2440    \t/3400, val loss:2.9154200553894043\n",
      "training on iteration:2441    \t/3400, val loss:5.453055381774902\n",
      "training on iteration:2442    \t/3400, val loss:4.686982154846191\n",
      "training on iteration:2443    \t/3400, val loss:3.331110715866089\n",
      "training on iteration:2444    \t/3400, val loss:2.762601375579834\n",
      "training on iteration:2445    \t/3400, val loss:4.9033637046813965\n",
      "training on iteration:2446    \t/3400, val loss:3.5576422214508057\n",
      "training on iteration:2447    \t/3400, val loss:3.0829854011535645\n",
      "training on iteration:2448    \t/3400, val loss:2.726377248764038\n",
      "training on iteration:2449    \t/3400, val loss:5.083942890167236\n",
      "training on iteration:2450    \t/3400, val loss:3.1652779579162598\n",
      "training on iteration:2451    \t/3400, val loss:2.8929646015167236\n",
      "training on iteration:2452    \t/3400, val loss:2.831364870071411\n",
      "training on iteration:2453    \t/3400, val loss:3.655141830444336\n",
      "training on iteration:2454    \t/3400, val loss:2.6796016693115234\n",
      "training on iteration:2455    \t/3400, val loss:2.6675333976745605\n",
      "training on iteration:2456    \t/3400, val loss:4.088856220245361\n",
      "training on iteration:2457    \t/3400, val loss:3.0888659954071045\n",
      "training on iteration:2458    \t/3400, val loss:3.1321499347686768\n",
      "training on iteration:2459    \t/3400, val loss:3.8046085834503174\n",
      "training on iteration:2460    \t/3400, val loss:4.3825554847717285\n",
      "training on iteration:2461    \t/3400, val loss:2.8868489265441895\n",
      "training on iteration:2462    \t/3400, val loss:2.9368433952331543\n",
      "training on iteration:2463    \t/3400, val loss:3.847956418991089\n",
      "training on iteration:2464    \t/3400, val loss:3.8889377117156982\n",
      "training on iteration:2465    \t/3400, val loss:3.1763322353363037\n",
      "training on iteration:2466    \t/3400, val loss:2.7570993900299072\n",
      "training on iteration:2467    \t/3400, val loss:5.290816783905029\n",
      "training on iteration:2468    \t/3400, val loss:3.5226104259490967\n",
      "training on iteration:2469    \t/3400, val loss:3.4403090476989746\n",
      "training on iteration:2470    \t/3400, val loss:2.6396069526672363\n",
      "training on iteration:2471    \t/3400, val loss:4.604063987731934\n",
      "training on iteration:2472    \t/3400, val loss:3.0844292640686035\n",
      "training on iteration:2473    \t/3400, val loss:2.659435510635376\n",
      "training on iteration:2474    \t/3400, val loss:2.9877123832702637\n",
      "training on iteration:2475    \t/3400, val loss:2.9710805416107178\n",
      "training on iteration:2476    \t/3400, val loss:3.449049234390259\n",
      "training on iteration:2477    \t/3400, val loss:2.7095813751220703\n",
      "training on iteration:2478    \t/3400, val loss:2.808269500732422\n",
      "training on iteration:2479    \t/3400, val loss:2.8531367778778076\n",
      "training on iteration:2480    \t/3400, val loss:2.9448893070220947\n",
      "training on iteration:2481    \t/3400, val loss:3.2266600131988525\n",
      "training on iteration:2482    \t/3400, val loss:2.6801140308380127\n",
      "training on iteration:2483    \t/3400, val loss:2.827246904373169\n",
      "training on iteration:2484    \t/3400, val loss:3.3960564136505127\n",
      "training on iteration:2485    \t/3400, val loss:2.792161703109741\n",
      "training on iteration:2486    \t/3400, val loss:2.677948474884033\n",
      "training on iteration:2487    \t/3400, val loss:2.851970672607422\n",
      "training on iteration:2488    \t/3400, val loss:2.7019615173339844\n",
      "training on iteration:2489    \t/3400, val loss:3.3816111087799072\n",
      "training on iteration:2490    \t/3400, val loss:3.173154592514038\n",
      "training on iteration:2491    \t/3400, val loss:2.6554276943206787\n",
      "training on iteration:2492    \t/3400, val loss:2.742347002029419\n",
      "training on iteration:2493    \t/3400, val loss:4.857844829559326\n",
      "training on iteration:2494    \t/3400, val loss:2.6845755577087402\n",
      "training on iteration:2495    \t/3400, val loss:2.6858274936676025\n",
      "training on iteration:2496    \t/3400, val loss:3.3922665119171143\n",
      "training on iteration:2497    \t/3400, val loss:2.8924176692962646\n",
      "training on iteration:2498    \t/3400, val loss:2.637828826904297\n",
      "training on iteration:2499    \t/3400, val loss:3.066946268081665\n",
      "training on iteration:2500    \t/3400, val loss:2.775023937225342\n",
      "training on iteration:2501    \t/3400, val loss:2.7012391090393066\n",
      "training on iteration:2502    \t/3400, val loss:3.5565829277038574\n",
      "training on iteration:2503    \t/3400, val loss:2.643096685409546\n",
      "training on iteration:2504    \t/3400, val loss:2.7478444576263428\n",
      "training on iteration:2505    \t/3400, val loss:3.0547726154327393\n",
      "training on iteration:2506    \t/3400, val loss:3.361802577972412\n",
      "training on iteration:2507    \t/3400, val loss:2.7126448154449463\n",
      "training on iteration:2508    \t/3400, val loss:2.6224727630615234\n",
      "training on iteration:2509    \t/3400, val loss:3.2515037059783936\n",
      "training on iteration:2510    \t/3400, val loss:2.820619583129883\n",
      "training on iteration:2511    \t/3400, val loss:2.736692428588867\n",
      "training on iteration:2512    \t/3400, val loss:3.066560983657837\n",
      "training on iteration:2513    \t/3400, val loss:2.626269817352295\n",
      "training on iteration:2514    \t/3400, val loss:2.6948437690734863\n",
      "training on iteration:2515    \t/3400, val loss:3.350101947784424\n",
      "training on iteration:2516    \t/3400, val loss:2.767961263656616\n",
      "training on iteration:2517    \t/3400, val loss:2.9635369777679443\n",
      "training on iteration:2518    \t/3400, val loss:3.3555119037628174\n",
      "training on iteration:2519    \t/3400, val loss:2.764493227005005\n",
      "training on iteration:2520    \t/3400, val loss:2.898592710494995\n",
      "training on iteration:2521    \t/3400, val loss:2.82661771774292\n",
      "training on iteration:2522    \t/3400, val loss:2.8217451572418213\n",
      "training on iteration:2523    \t/3400, val loss:4.028049945831299\n",
      "training on iteration:2524    \t/3400, val loss:2.913806915283203\n",
      "training on iteration:2525    \t/3400, val loss:2.801574230194092\n",
      "training on iteration:2526    \t/3400, val loss:3.0816731452941895\n",
      "training on iteration:2527    \t/3400, val loss:3.7841107845306396\n",
      "training on iteration:2528    \t/3400, val loss:2.7346959114074707\n",
      "training on iteration:2529    \t/3400, val loss:2.574939012527466\n",
      "training on iteration:2530    \t/3400, val loss:3.5127904415130615\n",
      "training on iteration:2531    \t/3400, val loss:2.568941354751587\n",
      "training on iteration:2532    \t/3400, val loss:2.755207061767578\n",
      "training on iteration:2533    \t/3400, val loss:2.965571165084839\n",
      "training on iteration:2534    \t/3400, val loss:2.5612053871154785\n",
      "training on iteration:2535    \t/3400, val loss:3.125530242919922\n",
      "training on iteration:2536    \t/3400, val loss:2.9068353176116943\n",
      "training on iteration:2537    \t/3400, val loss:2.746795415878296\n",
      "training on iteration:2538    \t/3400, val loss:2.8084805011749268\n",
      "training on iteration:2539    \t/3400, val loss:2.6696741580963135\n",
      "training on iteration:2540    \t/3400, val loss:2.883545160293579\n",
      "training on iteration:2541    \t/3400, val loss:3.252833604812622\n",
      "training on iteration:2542    \t/3400, val loss:2.5745742321014404\n",
      "training on iteration:2543    \t/3400, val loss:2.668997287750244\n",
      "training on iteration:2544    \t/3400, val loss:4.62094783782959\n",
      "training on iteration:2545    \t/3400, val loss:2.637739419937134\n",
      "training on iteration:2546    \t/3400, val loss:2.5490310192108154\n",
      "training on iteration:2547    \t/3400, val loss:4.334384441375732\n",
      "training on iteration:2548    \t/3400, val loss:2.5769143104553223\n",
      "training on iteration:2549    \t/3400, val loss:2.8339569568634033\n",
      "training on iteration:2550    \t/3400, val loss:4.118145942687988\n",
      "training on iteration:2551    \t/3400, val loss:2.6964423656463623\n",
      "training on iteration:2552    \t/3400, val loss:2.6188387870788574\n",
      "training on iteration:2553    \t/3400, val loss:4.071604251861572\n",
      "training on iteration:2554    \t/3400, val loss:2.8083267211914062\n",
      "training on iteration:2555    \t/3400, val loss:4.0354485511779785\n",
      "training on iteration:2556    \t/3400, val loss:2.7520127296447754\n",
      "training on iteration:2557    \t/3400, val loss:5.481207370758057\n",
      "training on iteration:2558    \t/3400, val loss:2.7234179973602295\n",
      "training on iteration:2559    \t/3400, val loss:3.2497546672821045\n",
      "training on iteration:2560    \t/3400, val loss:3.478884220123291\n",
      "training on iteration:2561    \t/3400, val loss:3.9612796306610107\n",
      "training on iteration:2562    \t/3400, val loss:2.814786672592163\n",
      "training on iteration:2563    \t/3400, val loss:2.716027021408081\n",
      "training on iteration:2564    \t/3400, val loss:3.646839141845703\n",
      "training on iteration:2565    \t/3400, val loss:2.720489501953125\n",
      "training on iteration:2566    \t/3400, val loss:2.6358718872070312\n",
      "training on iteration:2567    \t/3400, val loss:3.403198719024658\n",
      "training on iteration:2568    \t/3400, val loss:2.552443027496338\n",
      "training on iteration:2569    \t/3400, val loss:2.7061803340911865\n",
      "training on iteration:2570    \t/3400, val loss:3.6680381298065186\n",
      "training on iteration:2571    \t/3400, val loss:2.5681843757629395\n",
      "training on iteration:2572    \t/3400, val loss:2.6144421100616455\n",
      "training on iteration:2573    \t/3400, val loss:3.1713404655456543\n",
      "training on iteration:2574    \t/3400, val loss:2.8325536251068115\n",
      "training on iteration:2575    \t/3400, val loss:2.901383876800537\n",
      "training on iteration:2576    \t/3400, val loss:2.5768144130706787\n",
      "training on iteration:2577    \t/3400, val loss:2.571289300918579\n",
      "training on iteration:2578    \t/3400, val loss:4.276389122009277\n",
      "training on iteration:2579    \t/3400, val loss:2.766292095184326\n",
      "training on iteration:2580    \t/3400, val loss:2.579737901687622\n",
      "training on iteration:2581    \t/3400, val loss:3.312246084213257\n",
      "training on iteration:2582    \t/3400, val loss:2.537554979324341\n",
      "training on iteration:2583    \t/3400, val loss:2.971649169921875\n",
      "training on iteration:2584    \t/3400, val loss:3.2771403789520264\n",
      "training on iteration:2585    \t/3400, val loss:2.591153860092163\n",
      "training on iteration:2586    \t/3400, val loss:2.734252691268921\n",
      "training on iteration:2587    \t/3400, val loss:3.504990339279175\n",
      "training on iteration:2588    \t/3400, val loss:2.9701597690582275\n",
      "training on iteration:2589    \t/3400, val loss:2.5392329692840576\n",
      "training on iteration:2590    \t/3400, val loss:3.204925298690796\n",
      "training on iteration:2591    \t/3400, val loss:2.851060152053833\n",
      "training on iteration:2592    \t/3400, val loss:3.318627119064331\n",
      "training on iteration:2593    \t/3400, val loss:2.6823267936706543\n",
      "training on iteration:2594    \t/3400, val loss:2.7832491397857666\n",
      "training on iteration:2595    \t/3400, val loss:4.1728949546813965\n",
      "training on iteration:2596    \t/3400, val loss:2.5633015632629395\n",
      "training on iteration:2597    \t/3400, val loss:2.711709499359131\n",
      "training on iteration:2598    \t/3400, val loss:3.0086028575897217\n",
      "training on iteration:2599    \t/3400, val loss:2.595951557159424\n",
      "training on iteration:2600    \t/3400, val loss:2.796992063522339\n",
      "training on iteration:2601    \t/3400, val loss:3.087848663330078\n",
      "training on iteration:2602    \t/3400, val loss:2.5498483180999756\n",
      "training on iteration:2603    \t/3400, val loss:2.816126585006714\n",
      "training on iteration:2604    \t/3400, val loss:2.9604790210723877\n",
      "training on iteration:2605    \t/3400, val loss:2.8390982151031494\n",
      "training on iteration:2606    \t/3400, val loss:2.7345364093780518\n",
      "training on iteration:2607    \t/3400, val loss:2.676260232925415\n",
      "training on iteration:2608    \t/3400, val loss:3.0816712379455566\n",
      "training on iteration:2609    \t/3400, val loss:3.308194875717163\n",
      "training on iteration:2610    \t/3400, val loss:2.5596792697906494\n",
      "training on iteration:2611    \t/3400, val loss:2.6393840312957764\n",
      "training on iteration:2612    \t/3400, val loss:4.585585594177246\n",
      "training on iteration:2613    \t/3400, val loss:2.550499200820923\n",
      "training on iteration:2614    \t/3400, val loss:2.6185081005096436\n",
      "training on iteration:2615    \t/3400, val loss:3.33164381980896\n",
      "training on iteration:2616    \t/3400, val loss:2.539829730987549\n",
      "training on iteration:2617    \t/3400, val loss:2.6642513275146484\n",
      "training on iteration:2618    \t/3400, val loss:3.298628330230713\n",
      "training on iteration:2619    \t/3400, val loss:2.527470350265503\n",
      "training on iteration:2620    \t/3400, val loss:3.127685070037842\n",
      "training on iteration:2621    \t/3400, val loss:2.713064432144165\n",
      "training on iteration:2622    \t/3400, val loss:2.7148802280426025\n",
      "training on iteration:2623    \t/3400, val loss:3.144141674041748\n",
      "training on iteration:2624    \t/3400, val loss:2.5139176845550537\n",
      "training on iteration:2625    \t/3400, val loss:3.1476573944091797\n",
      "training on iteration:2626    \t/3400, val loss:2.8443357944488525\n",
      "training on iteration:2627    \t/3400, val loss:2.602233409881592\n",
      "training on iteration:2628    \t/3400, val loss:2.838322401046753\n",
      "training on iteration:2629    \t/3400, val loss:3.5342953205108643\n",
      "training on iteration:2630    \t/3400, val loss:3.0085854530334473\n",
      "training on iteration:2631    \t/3400, val loss:2.9606876373291016\n",
      "training on iteration:2632    \t/3400, val loss:2.8904900550842285\n",
      "training on iteration:2633    \t/3400, val loss:2.5073554515838623\n",
      "training on iteration:2634    \t/3400, val loss:4.06782865524292\n",
      "training on iteration:2635    \t/3400, val loss:2.742837905883789\n",
      "training on iteration:2636    \t/3400, val loss:2.681872844696045\n",
      "training on iteration:2637    \t/3400, val loss:3.4956424236297607\n",
      "training on iteration:2638    \t/3400, val loss:2.4860405921936035\n",
      "training on iteration:2639    \t/3400, val loss:2.751527786254883\n",
      "training on iteration:2640    \t/3400, val loss:2.5474562644958496\n",
      "training on iteration:2641    \t/3400, val loss:2.517782688140869\n",
      "training on iteration:2642    \t/3400, val loss:3.849942445755005\n",
      "training on iteration:2643    \t/3400, val loss:2.5463600158691406\n",
      "training on iteration:2644    \t/3400, val loss:2.570828914642334\n",
      "training on iteration:2645    \t/3400, val loss:2.9618518352508545\n",
      "training on iteration:2646    \t/3400, val loss:2.981341600418091\n",
      "training on iteration:2647    \t/3400, val loss:2.599620819091797\n",
      "training on iteration:2648    \t/3400, val loss:3.215144634246826\n",
      "training on iteration:2649    \t/3400, val loss:2.555034637451172\n",
      "training on iteration:2650    \t/3400, val loss:2.527297258377075\n",
      "training on iteration:2651    \t/3400, val loss:4.2794976234436035\n",
      "training on iteration:2652    \t/3400, val loss:2.7341885566711426\n",
      "training on iteration:2653    \t/3400, val loss:2.6831068992614746\n",
      "training on iteration:2654    \t/3400, val loss:3.700495958328247\n",
      "training on iteration:2655    \t/3400, val loss:2.8883776664733887\n",
      "training on iteration:2656    \t/3400, val loss:2.7541089057922363\n",
      "training on iteration:2657    \t/3400, val loss:2.615238666534424\n",
      "training on iteration:2658    \t/3400, val loss:2.6334266662597656\n",
      "training on iteration:2659    \t/3400, val loss:4.655457019805908\n",
      "training on iteration:2660    \t/3400, val loss:2.5942020416259766\n",
      "training on iteration:2661    \t/3400, val loss:2.851954936981201\n",
      "training on iteration:2662    \t/3400, val loss:3.833966016769409\n",
      "training on iteration:2663    \t/3400, val loss:2.6494364738464355\n",
      "training on iteration:2664    \t/3400, val loss:2.4947054386138916\n",
      "training on iteration:2665    \t/3400, val loss:3.4893782138824463\n",
      "training on iteration:2666    \t/3400, val loss:2.4766898155212402\n",
      "training on iteration:2667    \t/3400, val loss:2.5428524017333984\n",
      "training on iteration:2668    \t/3400, val loss:4.417849063873291\n",
      "training on iteration:2669    \t/3400, val loss:2.8770012855529785\n",
      "training on iteration:2670    \t/3400, val loss:2.695112943649292\n",
      "training on iteration:2671    \t/3400, val loss:4.129366874694824\n",
      "training on iteration:2672    \t/3400, val loss:2.748633623123169\n",
      "training on iteration:2673    \t/3400, val loss:2.590055465698242\n",
      "training on iteration:2674    \t/3400, val loss:3.2308783531188965\n",
      "training on iteration:2675    \t/3400, val loss:2.752399206161499\n",
      "training on iteration:2676    \t/3400, val loss:4.156787872314453\n",
      "training on iteration:2677    \t/3400, val loss:2.5983023643493652\n",
      "training on iteration:2678    \t/3400, val loss:2.5772135257720947\n",
      "training on iteration:2679    \t/3400, val loss:3.5079967975616455\n",
      "training on iteration:2680    \t/3400, val loss:2.7535130977630615\n",
      "training on iteration:2681    \t/3400, val loss:2.4696292877197266\n",
      "training on iteration:2682    \t/3400, val loss:3.1916918754577637\n",
      "training on iteration:2683    \t/3400, val loss:2.4659221172332764\n",
      "training on iteration:2684    \t/3400, val loss:2.576237201690674\n",
      "training on iteration:2685    \t/3400, val loss:3.142481565475464\n",
      "training on iteration:2686    \t/3400, val loss:2.481142044067383\n",
      "training on iteration:2687    \t/3400, val loss:3.0809006690979004\n",
      "training on iteration:2688    \t/3400, val loss:2.6725575923919678\n",
      "training on iteration:2689    \t/3400, val loss:2.6654934883117676\n",
      "training on iteration:2690    \t/3400, val loss:2.957625150680542\n",
      "training on iteration:2691    \t/3400, val loss:2.605426549911499\n",
      "training on iteration:2692    \t/3400, val loss:2.787292003631592\n",
      "training on iteration:2693    \t/3400, val loss:3.182825803756714\n",
      "training on iteration:2694    \t/3400, val loss:2.509312868118286\n",
      "training on iteration:2695    \t/3400, val loss:2.66031813621521\n",
      "training on iteration:2696    \t/3400, val loss:2.482792615890503\n",
      "training on iteration:2697    \t/3400, val loss:4.372806072235107\n",
      "training on iteration:2698    \t/3400, val loss:2.8398945331573486\n",
      "training on iteration:2699    \t/3400, val loss:2.5545036792755127\n",
      "training on iteration:2700    \t/3400, val loss:3.2753703594207764\n",
      "training on iteration:2701    \t/3400, val loss:2.517134189605713\n",
      "training on iteration:2702    \t/3400, val loss:3.7321724891662598\n",
      "training on iteration:2703    \t/3400, val loss:2.5959317684173584\n",
      "training on iteration:2704    \t/3400, val loss:2.8604140281677246\n",
      "training on iteration:2705    \t/3400, val loss:2.9558136463165283\n",
      "training on iteration:2706    \t/3400, val loss:2.617387533187866\n",
      "training on iteration:2707    \t/3400, val loss:3.0742878913879395\n",
      "training on iteration:2708    \t/3400, val loss:2.5053954124450684\n",
      "training on iteration:2709    \t/3400, val loss:2.749035358428955\n",
      "training on iteration:2710    \t/3400, val loss:3.1056416034698486\n",
      "training on iteration:2711    \t/3400, val loss:2.505211114883423\n",
      "training on iteration:2712    \t/3400, val loss:2.718820095062256\n",
      "training on iteration:2713    \t/3400, val loss:2.53540301322937\n",
      "training on iteration:2714    \t/3400, val loss:4.785863399505615\n",
      "training on iteration:2715    \t/3400, val loss:2.5525777339935303\n",
      "training on iteration:2716    \t/3400, val loss:2.539824962615967\n",
      "training on iteration:2717    \t/3400, val loss:4.512969493865967\n",
      "training on iteration:2718    \t/3400, val loss:2.534541368484497\n",
      "training on iteration:2719    \t/3400, val loss:2.7852091789245605\n",
      "training on iteration:2720    \t/3400, val loss:4.124035358428955\n",
      "training on iteration:2721    \t/3400, val loss:2.4610421657562256\n",
      "training on iteration:2722    \t/3400, val loss:2.8442800045013428\n",
      "training on iteration:2723    \t/3400, val loss:3.3226563930511475\n",
      "training on iteration:2724    \t/3400, val loss:3.1120734214782715\n",
      "training on iteration:2725    \t/3400, val loss:2.6088900566101074\n",
      "training on iteration:2726    \t/3400, val loss:3.1475374698638916\n",
      "training on iteration:2727    \t/3400, val loss:2.4813663959503174\n",
      "training on iteration:2728    \t/3400, val loss:3.7339377403259277\n",
      "training on iteration:2729    \t/3400, val loss:2.593649387359619\n",
      "training on iteration:2730    \t/3400, val loss:2.615671157836914\n",
      "training on iteration:2731    \t/3400, val loss:5.552706241607666\n",
      "training on iteration:2732    \t/3400, val loss:3.221406936645508\n",
      "training on iteration:2733    \t/3400, val loss:4.337315082550049\n",
      "training on iteration:2734    \t/3400, val loss:3.5886552333831787\n",
      "training on iteration:2735    \t/3400, val loss:4.742493152618408\n",
      "training on iteration:2736    \t/3400, val loss:3.6214239597320557\n",
      "training on iteration:2737    \t/3400, val loss:2.939487934112549\n",
      "training on iteration:2738    \t/3400, val loss:4.630653381347656\n",
      "training on iteration:2739    \t/3400, val loss:3.4869630336761475\n",
      "training on iteration:2740    \t/3400, val loss:2.578298568725586\n",
      "training on iteration:2741    \t/3400, val loss:5.261488914489746\n",
      "training on iteration:2742    \t/3400, val loss:4.287293434143066\n",
      "training on iteration:2743    \t/3400, val loss:3.5301144123077393\n",
      "training on iteration:2744    \t/3400, val loss:4.1898064613342285\n",
      "training on iteration:2745    \t/3400, val loss:5.461491584777832\n",
      "training on iteration:2746    \t/3400, val loss:2.8228628635406494\n",
      "training on iteration:2747    \t/3400, val loss:2.7153470516204834\n",
      "training on iteration:2748    \t/3400, val loss:6.3567914962768555\n",
      "training on iteration:2749    \t/3400, val loss:4.266261577606201\n",
      "training on iteration:2750    \t/3400, val loss:5.426136493682861\n",
      "training on iteration:2751    \t/3400, val loss:2.892232656478882\n",
      "training on iteration:2752    \t/3400, val loss:7.514777183532715\n",
      "training on iteration:2753    \t/3400, val loss:4.88425874710083\n",
      "training on iteration:2754    \t/3400, val loss:7.758115291595459\n",
      "training on iteration:2755    \t/3400, val loss:4.4473876953125\n",
      "training on iteration:2756    \t/3400, val loss:9.336960792541504\n",
      "training on iteration:2757    \t/3400, val loss:11.804971694946289\n",
      "training on iteration:2758    \t/3400, val loss:6.270565986633301\n",
      "training on iteration:2759    \t/3400, val loss:9.234827041625977\n",
      "training on iteration:2760    \t/3400, val loss:8.476974487304688\n",
      "training on iteration:2761    \t/3400, val loss:6.1259942054748535\n",
      "training on iteration:2762    \t/3400, val loss:9.749883651733398\n",
      "training on iteration:2763    \t/3400, val loss:5.384315013885498\n",
      "training on iteration:2764    \t/3400, val loss:7.7141337394714355\n",
      "training on iteration:2765    \t/3400, val loss:6.3357062339782715\n",
      "training on iteration:2766    \t/3400, val loss:6.234220027923584\n",
      "training on iteration:2767    \t/3400, val loss:9.459763526916504\n",
      "training on iteration:2768    \t/3400, val loss:5.397114276885986\n",
      "training on iteration:2769    \t/3400, val loss:5.940832614898682\n",
      "training on iteration:2770    \t/3400, val loss:5.032737731933594\n",
      "training on iteration:2771    \t/3400, val loss:5.049408435821533\n",
      "training on iteration:2772    \t/3400, val loss:7.072051525115967\n",
      "training on iteration:2773    \t/3400, val loss:3.056260347366333\n",
      "training on iteration:2774    \t/3400, val loss:4.023923397064209\n",
      "training on iteration:2775    \t/3400, val loss:2.475234270095825\n",
      "training on iteration:2776    \t/3400, val loss:4.990108966827393\n",
      "training on iteration:2777    \t/3400, val loss:4.051078796386719\n",
      "training on iteration:2778    \t/3400, val loss:3.0374677181243896\n",
      "training on iteration:2779    \t/3400, val loss:2.380113363265991\n",
      "training on iteration:2780    \t/3400, val loss:5.299674987792969\n",
      "training on iteration:2781    \t/3400, val loss:3.4181835651397705\n",
      "training on iteration:2782    \t/3400, val loss:2.991272449493408\n",
      "training on iteration:2783    \t/3400, val loss:2.6042110919952393\n",
      "training on iteration:2784    \t/3400, val loss:5.172386169433594\n",
      "training on iteration:2785    \t/3400, val loss:3.114781141281128\n",
      "training on iteration:2786    \t/3400, val loss:3.267211437225342\n",
      "training on iteration:2787    \t/3400, val loss:2.9537580013275146\n",
      "training on iteration:2788    \t/3400, val loss:4.864582061767578\n",
      "training on iteration:2789    \t/3400, val loss:2.521562099456787\n",
      "training on iteration:2790    \t/3400, val loss:2.5256175994873047\n",
      "training on iteration:2791    \t/3400, val loss:4.4837870597839355\n",
      "training on iteration:2792    \t/3400, val loss:3.2216734886169434\n",
      "training on iteration:2793    \t/3400, val loss:3.911573648452759\n",
      "training on iteration:2794    \t/3400, val loss:2.670848846435547\n",
      "training on iteration:2795    \t/3400, val loss:7.081334114074707\n",
      "training on iteration:2796    \t/3400, val loss:4.53588342666626\n",
      "training on iteration:2797    \t/3400, val loss:4.063708305358887\n",
      "training on iteration:2798    \t/3400, val loss:2.863868474960327\n",
      "training on iteration:2799    \t/3400, val loss:8.632905006408691\n",
      "training on iteration:2800    \t/3400, val loss:9.293413162231445\n",
      "training on iteration:2801    \t/3400, val loss:3.0126569271087646\n",
      "training on iteration:2802    \t/3400, val loss:6.582761287689209\n",
      "training on iteration:2803    \t/3400, val loss:3.313703775405884\n",
      "training on iteration:2804    \t/3400, val loss:8.66364574432373\n",
      "training on iteration:2805    \t/3400, val loss:9.766966819763184\n",
      "training on iteration:2806    \t/3400, val loss:3.85908842086792\n",
      "training on iteration:2807    \t/3400, val loss:7.015510082244873\n",
      "training on iteration:2808    \t/3400, val loss:4.93292760848999\n",
      "training on iteration:2809    \t/3400, val loss:6.852663993835449\n",
      "training on iteration:2810    \t/3400, val loss:8.960784912109375\n",
      "training on iteration:2811    \t/3400, val loss:4.206833839416504\n",
      "training on iteration:2812    \t/3400, val loss:5.08272647857666\n",
      "training on iteration:2813    \t/3400, val loss:3.2668354511260986\n",
      "training on iteration:2814    \t/3400, val loss:6.227306365966797\n",
      "training on iteration:2815    \t/3400, val loss:6.7747602462768555\n",
      "training on iteration:2816    \t/3400, val loss:2.5886380672454834\n",
      "training on iteration:2817    \t/3400, val loss:3.574758291244507\n",
      "training on iteration:2818    \t/3400, val loss:2.6607747077941895\n",
      "training on iteration:2819    \t/3400, val loss:5.048031806945801\n",
      "training on iteration:2820    \t/3400, val loss:3.2640984058380127\n",
      "training on iteration:2821    \t/3400, val loss:3.3137500286102295\n",
      "training on iteration:2822    \t/3400, val loss:2.537790298461914\n",
      "training on iteration:2823    \t/3400, val loss:5.366793632507324\n",
      "training on iteration:2824    \t/3400, val loss:4.8166069984436035\n",
      "training on iteration:2825    \t/3400, val loss:2.610290288925171\n",
      "training on iteration:2826    \t/3400, val loss:2.710658311843872\n",
      "training on iteration:2827    \t/3400, val loss:3.6278185844421387\n",
      "training on iteration:2828    \t/3400, val loss:3.211347818374634\n",
      "training on iteration:2829    \t/3400, val loss:2.643808603286743\n",
      "training on iteration:2830    \t/3400, val loss:2.8553109169006348\n",
      "training on iteration:2831    \t/3400, val loss:4.032043933868408\n",
      "training on iteration:2832    \t/3400, val loss:2.657728910446167\n",
      "training on iteration:2833    \t/3400, val loss:2.9391419887542725\n",
      "training on iteration:2834    \t/3400, val loss:4.139188289642334\n",
      "training on iteration:2835    \t/3400, val loss:2.7580647468566895\n",
      "training on iteration:2836    \t/3400, val loss:2.7338812351226807\n",
      "training on iteration:2837    \t/3400, val loss:3.1789181232452393\n",
      "training on iteration:2838    \t/3400, val loss:4.4900617599487305\n",
      "training on iteration:2839    \t/3400, val loss:2.979985475540161\n",
      "training on iteration:2840    \t/3400, val loss:2.664440870285034\n",
      "training on iteration:2841    \t/3400, val loss:4.756595134735107\n",
      "training on iteration:2842    \t/3400, val loss:3.5617122650146484\n",
      "training on iteration:2843    \t/3400, val loss:2.886857271194458\n",
      "training on iteration:2844    \t/3400, val loss:2.6474785804748535\n",
      "training on iteration:2845    \t/3400, val loss:3.8704397678375244\n",
      "training on iteration:2846    \t/3400, val loss:2.770843029022217\n",
      "training on iteration:2847    \t/3400, val loss:2.6424496173858643\n",
      "training on iteration:2848    \t/3400, val loss:3.0331637859344482\n",
      "training on iteration:2849    \t/3400, val loss:2.6400139331817627\n",
      "training on iteration:2850    \t/3400, val loss:4.227841854095459\n",
      "training on iteration:2851    \t/3400, val loss:2.8671298027038574\n",
      "training on iteration:2852    \t/3400, val loss:2.6043269634246826\n",
      "training on iteration:2853    \t/3400, val loss:2.8316478729248047\n",
      "training on iteration:2854    \t/3400, val loss:2.921365261077881\n",
      "training on iteration:2855    \t/3400, val loss:2.8511273860931396\n",
      "training on iteration:2856    \t/3400, val loss:2.912609815597534\n",
      "training on iteration:2857    \t/3400, val loss:2.988973617553711\n",
      "training on iteration:2858    \t/3400, val loss:2.775552988052368\n",
      "training on iteration:2859    \t/3400, val loss:3.0125234127044678\n",
      "training on iteration:2860    \t/3400, val loss:2.6524360179901123\n",
      "training on iteration:2861    \t/3400, val loss:2.633863687515259\n",
      "training on iteration:2862    \t/3400, val loss:2.8862712383270264\n",
      "training on iteration:2863    \t/3400, val loss:3.0865304470062256\n",
      "training on iteration:2864    \t/3400, val loss:2.8356070518493652\n",
      "training on iteration:2865    \t/3400, val loss:2.576864719390869\n",
      "training on iteration:2866    \t/3400, val loss:2.7567899227142334\n",
      "training on iteration:2867    \t/3400, val loss:3.9879281520843506\n",
      "training on iteration:2868    \t/3400, val loss:2.8912746906280518\n",
      "training on iteration:2869    \t/3400, val loss:2.619793176651001\n",
      "training on iteration:2870    \t/3400, val loss:3.5787622928619385\n",
      "training on iteration:2871    \t/3400, val loss:2.6416401863098145\n",
      "training on iteration:2872    \t/3400, val loss:2.6715924739837646\n",
      "training on iteration:2873    \t/3400, val loss:3.252732515335083\n",
      "training on iteration:2874    \t/3400, val loss:2.8011817932128906\n",
      "training on iteration:2875    \t/3400, val loss:2.5992603302001953\n",
      "training on iteration:2876    \t/3400, val loss:3.301888942718506\n",
      "training on iteration:2877    \t/3400, val loss:2.710618734359741\n",
      "training on iteration:2878    \t/3400, val loss:2.639591932296753\n",
      "training on iteration:2879    \t/3400, val loss:3.0522539615631104\n",
      "training on iteration:2880    \t/3400, val loss:3.077744245529175\n",
      "training on iteration:2881    \t/3400, val loss:2.79522705078125\n",
      "training on iteration:2882    \t/3400, val loss:2.6191024780273438\n",
      "training on iteration:2883    \t/3400, val loss:2.7818219661712646\n",
      "training on iteration:2884    \t/3400, val loss:3.459953546524048\n",
      "training on iteration:2885    \t/3400, val loss:2.9397130012512207\n",
      "training on iteration:2886    \t/3400, val loss:2.805326223373413\n",
      "training on iteration:2887    \t/3400, val loss:3.0205271244049072\n",
      "training on iteration:2888    \t/3400, val loss:2.6168081760406494\n",
      "training on iteration:2889    \t/3400, val loss:3.4787468910217285\n",
      "training on iteration:2890    \t/3400, val loss:2.676591157913208\n",
      "training on iteration:2891    \t/3400, val loss:2.577847719192505\n",
      "training on iteration:2892    \t/3400, val loss:3.5955803394317627\n",
      "training on iteration:2893    \t/3400, val loss:2.5430755615234375\n",
      "training on iteration:2894    \t/3400, val loss:2.6153762340545654\n",
      "training on iteration:2895    \t/3400, val loss:2.7576346397399902\n",
      "training on iteration:2896    \t/3400, val loss:2.5184590816497803\n",
      "training on iteration:2897    \t/3400, val loss:3.487215757369995\n",
      "training on iteration:2898    \t/3400, val loss:2.640810251235962\n",
      "training on iteration:2899    \t/3400, val loss:2.6519615650177\n",
      "training on iteration:2900    \t/3400, val loss:2.8828866481781006\n",
      "training on iteration:2901    \t/3400, val loss:3.664047956466675\n",
      "training on iteration:2902    \t/3400, val loss:3.1103427410125732\n",
      "training on iteration:2903    \t/3400, val loss:2.665281295776367\n",
      "training on iteration:2904    \t/3400, val loss:3.4268171787261963\n",
      "training on iteration:2905    \t/3400, val loss:2.7344014644622803\n",
      "training on iteration:2906    \t/3400, val loss:3.169936418533325\n",
      "training on iteration:2907    \t/3400, val loss:2.5232436656951904\n",
      "training on iteration:2908    \t/3400, val loss:2.6475815773010254\n",
      "training on iteration:2909    \t/3400, val loss:3.0243732929229736\n",
      "training on iteration:2910    \t/3400, val loss:2.5003409385681152\n",
      "training on iteration:2911    \t/3400, val loss:2.588844060897827\n",
      "training on iteration:2912    \t/3400, val loss:2.486788511276245\n",
      "training on iteration:2913    \t/3400, val loss:2.612229824066162\n",
      "training on iteration:2914    \t/3400, val loss:3.2059876918792725\n",
      "training on iteration:2915    \t/3400, val loss:2.5746653079986572\n",
      "training on iteration:2916    \t/3400, val loss:2.6331865787506104\n",
      "training on iteration:2917    \t/3400, val loss:2.5381100177764893\n",
      "training on iteration:2918    \t/3400, val loss:3.719947099685669\n",
      "training on iteration:2919    \t/3400, val loss:2.669830322265625\n",
      "training on iteration:2920    \t/3400, val loss:2.5742685794830322\n",
      "training on iteration:2921    \t/3400, val loss:2.9806723594665527\n",
      "training on iteration:2922    \t/3400, val loss:2.5587916374206543\n",
      "training on iteration:2923    \t/3400, val loss:2.925767421722412\n",
      "training on iteration:2924    \t/3400, val loss:2.6663284301757812\n",
      "training on iteration:2925    \t/3400, val loss:2.5864641666412354\n",
      "training on iteration:2926    \t/3400, val loss:2.7011349201202393\n",
      "training on iteration:2927    \t/3400, val loss:2.827139139175415\n",
      "training on iteration:2928    \t/3400, val loss:2.497446060180664\n",
      "training on iteration:2929    \t/3400, val loss:2.6785242557525635\n",
      "training on iteration:2930    \t/3400, val loss:2.5000250339508057\n",
      "training on iteration:2931    \t/3400, val loss:3.6699044704437256\n",
      "training on iteration:2932    \t/3400, val loss:2.601454257965088\n",
      "training on iteration:2933    \t/3400, val loss:2.523878335952759\n",
      "training on iteration:2934    \t/3400, val loss:2.812718629837036\n",
      "training on iteration:2935    \t/3400, val loss:3.153635263442993\n",
      "training on iteration:2936    \t/3400, val loss:2.5214216709136963\n",
      "training on iteration:2937    \t/3400, val loss:2.874931573867798\n",
      "training on iteration:2938    \t/3400, val loss:2.7292771339416504\n",
      "training on iteration:2939    \t/3400, val loss:2.5983951091766357\n",
      "training on iteration:2940    \t/3400, val loss:3.4676144123077393\n",
      "training on iteration:2941    \t/3400, val loss:2.5569236278533936\n",
      "training on iteration:2942    \t/3400, val loss:2.8814971446990967\n",
      "training on iteration:2943    \t/3400, val loss:2.975071430206299\n",
      "training on iteration:2944    \t/3400, val loss:2.6372807025909424\n",
      "training on iteration:2945    \t/3400, val loss:3.0866873264312744\n",
      "training on iteration:2946    \t/3400, val loss:2.462655544281006\n",
      "training on iteration:2947    \t/3400, val loss:2.4640042781829834\n",
      "training on iteration:2948    \t/3400, val loss:4.840795040130615\n",
      "training on iteration:2949    \t/3400, val loss:2.4631078243255615\n",
      "training on iteration:2950    \t/3400, val loss:2.77964186668396\n",
      "training on iteration:2951    \t/3400, val loss:3.980215311050415\n",
      "training on iteration:2952    \t/3400, val loss:2.590641736984253\n",
      "training on iteration:2953    \t/3400, val loss:2.474309206008911\n",
      "training on iteration:2954    \t/3400, val loss:3.281642198562622\n",
      "training on iteration:2955    \t/3400, val loss:2.4677200317382812\n",
      "training on iteration:2956    \t/3400, val loss:2.5900862216949463\n",
      "training on iteration:2957    \t/3400, val loss:3.281466007232666\n",
      "training on iteration:2958    \t/3400, val loss:2.60626482963562\n",
      "training on iteration:2959    \t/3400, val loss:3.0768086910247803\n",
      "training on iteration:2960    \t/3400, val loss:2.96781587600708\n",
      "training on iteration:2961    \t/3400, val loss:2.8437037467956543\n",
      "training on iteration:2962    \t/3400, val loss:3.1007113456726074\n",
      "training on iteration:2963    \t/3400, val loss:2.488391876220703\n",
      "training on iteration:2964    \t/3400, val loss:2.5922884941101074\n",
      "training on iteration:2965    \t/3400, val loss:4.756608486175537\n",
      "training on iteration:2966    \t/3400, val loss:2.527369737625122\n",
      "training on iteration:2967    \t/3400, val loss:3.127375841140747\n",
      "training on iteration:2968    \t/3400, val loss:3.712026834487915\n",
      "training on iteration:2969    \t/3400, val loss:2.8049867153167725\n",
      "training on iteration:2970    \t/3400, val loss:2.6773335933685303\n",
      "training on iteration:2971    \t/3400, val loss:3.6122212409973145\n",
      "training on iteration:2972    \t/3400, val loss:2.534095525741577\n",
      "training on iteration:2973    \t/3400, val loss:2.642644166946411\n",
      "training on iteration:2974    \t/3400, val loss:4.620547294616699\n",
      "training on iteration:2975    \t/3400, val loss:2.4966557025909424\n",
      "training on iteration:2976    \t/3400, val loss:2.5010476112365723\n",
      "training on iteration:2977    \t/3400, val loss:4.482213497161865\n",
      "training on iteration:2978    \t/3400, val loss:2.7774367332458496\n",
      "training on iteration:2979    \t/3400, val loss:3.5341265201568604\n",
      "training on iteration:2980    \t/3400, val loss:2.8480727672576904\n",
      "training on iteration:2981    \t/3400, val loss:3.4189984798431396\n",
      "training on iteration:2982    \t/3400, val loss:2.785776138305664\n",
      "training on iteration:2983    \t/3400, val loss:3.09788179397583\n",
      "training on iteration:2984    \t/3400, val loss:2.9892756938934326\n",
      "training on iteration:2985    \t/3400, val loss:3.528780221939087\n",
      "training on iteration:2986    \t/3400, val loss:3.848412275314331\n",
      "training on iteration:2987    \t/3400, val loss:3.1805973052978516\n",
      "training on iteration:2988    \t/3400, val loss:3.3644886016845703\n",
      "training on iteration:2989    \t/3400, val loss:2.8921456336975098\n",
      "training on iteration:2990    \t/3400, val loss:3.7115910053253174\n",
      "training on iteration:2991    \t/3400, val loss:2.608938455581665\n",
      "training on iteration:2992    \t/3400, val loss:2.918626070022583\n",
      "training on iteration:2993    \t/3400, val loss:3.0319530963897705\n",
      "training on iteration:2994    \t/3400, val loss:2.4835658073425293\n",
      "training on iteration:2995    \t/3400, val loss:3.2418441772460938\n",
      "training on iteration:2996    \t/3400, val loss:2.484273672103882\n",
      "training on iteration:2997    \t/3400, val loss:2.603928327560425\n",
      "training on iteration:2998    \t/3400, val loss:3.7695257663726807\n",
      "training on iteration:2999    \t/3400, val loss:2.4654462337493896\n",
      "training on iteration:3000    \t/3400, val loss:2.702411413192749\n",
      "training on iteration:3001    \t/3400, val loss:2.9384121894836426\n",
      "training on iteration:3002    \t/3400, val loss:2.9837841987609863\n",
      "training on iteration:3003    \t/3400, val loss:4.570549964904785\n",
      "training on iteration:3004    \t/3400, val loss:3.013469934463501\n",
      "training on iteration:3005    \t/3400, val loss:3.300539255142212\n",
      "training on iteration:3006    \t/3400, val loss:3.186676263809204\n",
      "training on iteration:3007    \t/3400, val loss:3.2623417377471924\n",
      "training on iteration:3008    \t/3400, val loss:2.6006791591644287\n",
      "training on iteration:3009    \t/3400, val loss:3.2639641761779785\n",
      "training on iteration:3010    \t/3400, val loss:2.8360846042633057\n",
      "training on iteration:3011    \t/3400, val loss:2.4578471183776855\n",
      "training on iteration:3012    \t/3400, val loss:3.532921552658081\n",
      "training on iteration:3013    \t/3400, val loss:2.4509522914886475\n",
      "training on iteration:3014    \t/3400, val loss:2.4821581840515137\n",
      "training on iteration:3015    \t/3400, val loss:3.47554349899292\n",
      "training on iteration:3016    \t/3400, val loss:2.4499785900115967\n",
      "training on iteration:3017    \t/3400, val loss:2.9040186405181885\n",
      "training on iteration:3018    \t/3400, val loss:2.533545732498169\n",
      "training on iteration:3019    \t/3400, val loss:2.4886016845703125\n",
      "training on iteration:3020    \t/3400, val loss:5.132809162139893\n",
      "training on iteration:3021    \t/3400, val loss:2.6860127449035645\n",
      "training on iteration:3022    \t/3400, val loss:2.8904812335968018\n",
      "training on iteration:3023    \t/3400, val loss:3.598583936691284\n",
      "training on iteration:3024    \t/3400, val loss:2.729767084121704\n",
      "training on iteration:3025    \t/3400, val loss:2.8819046020507812\n",
      "training on iteration:3026    \t/3400, val loss:4.0759148597717285\n",
      "training on iteration:3027    \t/3400, val loss:2.681771993637085\n",
      "training on iteration:3028    \t/3400, val loss:2.507244110107422\n",
      "training on iteration:3029    \t/3400, val loss:4.1483588218688965\n",
      "training on iteration:3030    \t/3400, val loss:2.5273430347442627\n",
      "training on iteration:3031    \t/3400, val loss:3.553774356842041\n",
      "training on iteration:3032    \t/3400, val loss:3.5832996368408203\n",
      "training on iteration:3033    \t/3400, val loss:3.2107784748077393\n",
      "training on iteration:3034    \t/3400, val loss:2.7618725299835205\n",
      "training on iteration:3035    \t/3400, val loss:2.9981629848480225\n",
      "training on iteration:3036    \t/3400, val loss:2.520331859588623\n",
      "training on iteration:3037    \t/3400, val loss:3.2197041511535645\n",
      "training on iteration:3038    \t/3400, val loss:2.4262797832489014\n",
      "training on iteration:3039    \t/3400, val loss:2.7625155448913574\n",
      "training on iteration:3040    \t/3400, val loss:2.5531535148620605\n",
      "training on iteration:3041    \t/3400, val loss:2.5205461978912354\n",
      "training on iteration:3042    \t/3400, val loss:3.4833836555480957\n",
      "training on iteration:3043    \t/3400, val loss:2.7604808807373047\n",
      "training on iteration:3044    \t/3400, val loss:3.1386303901672363\n",
      "training on iteration:3045    \t/3400, val loss:3.208588123321533\n",
      "training on iteration:3046    \t/3400, val loss:3.052565097808838\n",
      "training on iteration:3047    \t/3400, val loss:2.7354254722595215\n",
      "training on iteration:3048    \t/3400, val loss:3.110675811767578\n",
      "training on iteration:3049    \t/3400, val loss:2.9631388187408447\n",
      "training on iteration:3050    \t/3400, val loss:4.149559020996094\n",
      "training on iteration:3051    \t/3400, val loss:2.705735445022583\n",
      "training on iteration:3052    \t/3400, val loss:2.943153142929077\n",
      "training on iteration:3053    \t/3400, val loss:3.224688768386841\n",
      "training on iteration:3054    \t/3400, val loss:3.1899352073669434\n",
      "training on iteration:3055    \t/3400, val loss:2.8483047485351562\n",
      "training on iteration:3056    \t/3400, val loss:3.131556272506714\n",
      "training on iteration:3057    \t/3400, val loss:2.7654054164886475\n",
      "training on iteration:3058    \t/3400, val loss:2.6122472286224365\n",
      "training on iteration:3059    \t/3400, val loss:4.338557243347168\n",
      "training on iteration:3060    \t/3400, val loss:2.503336191177368\n",
      "training on iteration:3061    \t/3400, val loss:2.4153125286102295\n",
      "training on iteration:3062    \t/3400, val loss:4.001688003540039\n",
      "training on iteration:3063    \t/3400, val loss:2.4670252799987793\n",
      "training on iteration:3064    \t/3400, val loss:2.6874637603759766\n",
      "training on iteration:3065    \t/3400, val loss:3.4001338481903076\n",
      "training on iteration:3066    \t/3400, val loss:2.4437217712402344\n",
      "training on iteration:3067    \t/3400, val loss:3.0168936252593994\n",
      "training on iteration:3068    \t/3400, val loss:3.021754026412964\n",
      "training on iteration:3069    \t/3400, val loss:2.761915922164917\n",
      "training on iteration:3070    \t/3400, val loss:2.5327956676483154\n",
      "training on iteration:3071    \t/3400, val loss:4.756601810455322\n",
      "training on iteration:3072    \t/3400, val loss:2.514556646347046\n",
      "training on iteration:3073    \t/3400, val loss:2.4349915981292725\n",
      "training on iteration:3074    \t/3400, val loss:4.304676532745361\n",
      "training on iteration:3075    \t/3400, val loss:2.5271472930908203\n",
      "training on iteration:3076    \t/3400, val loss:2.5128002166748047\n",
      "training on iteration:3077    \t/3400, val loss:3.9078614711761475\n",
      "training on iteration:3078    \t/3400, val loss:2.4177703857421875\n",
      "training on iteration:3079    \t/3400, val loss:2.683602809906006\n",
      "training on iteration:3080    \t/3400, val loss:3.315757989883423\n",
      "training on iteration:3081    \t/3400, val loss:2.6011643409729004\n",
      "training on iteration:3082    \t/3400, val loss:2.6791698932647705\n",
      "training on iteration:3083    \t/3400, val loss:2.6498587131500244\n",
      "training on iteration:3084    \t/3400, val loss:2.784599542617798\n",
      "training on iteration:3085    \t/3400, val loss:2.878275156021118\n",
      "training on iteration:3086    \t/3400, val loss:2.5899758338928223\n",
      "training on iteration:3087    \t/3400, val loss:2.728055238723755\n",
      "training on iteration:3088    \t/3400, val loss:4.258400917053223\n",
      "training on iteration:3089    \t/3400, val loss:3.1886069774627686\n",
      "training on iteration:3090    \t/3400, val loss:2.4469103813171387\n",
      "training on iteration:3091    \t/3400, val loss:3.9881234169006348\n",
      "training on iteration:3092    \t/3400, val loss:2.5514023303985596\n",
      "training on iteration:3093    \t/3400, val loss:2.648045063018799\n",
      "training on iteration:3094    \t/3400, val loss:3.8616998195648193\n",
      "training on iteration:3095    \t/3400, val loss:2.4413723945617676\n",
      "training on iteration:3096    \t/3400, val loss:2.8134634494781494\n",
      "training on iteration:3097    \t/3400, val loss:3.227898120880127\n",
      "training on iteration:3098    \t/3400, val loss:2.7836055755615234\n",
      "training on iteration:3099    \t/3400, val loss:2.621692180633545\n",
      "training on iteration:3100    \t/3400, val loss:2.76362943649292\n",
      "training on iteration:3101    \t/3400, val loss:2.4727401733398438\n",
      "training on iteration:3102    \t/3400, val loss:3.3700029850006104\n",
      "training on iteration:3103    \t/3400, val loss:2.5184783935546875\n",
      "training on iteration:3104    \t/3400, val loss:2.4645533561706543\n",
      "training on iteration:3105    \t/3400, val loss:5.246001720428467\n",
      "training on iteration:3106    \t/3400, val loss:2.453489065170288\n",
      "training on iteration:3107    \t/3400, val loss:2.7540905475616455\n",
      "training on iteration:3108    \t/3400, val loss:4.2334489822387695\n",
      "training on iteration:3109    \t/3400, val loss:2.4703164100646973\n",
      "training on iteration:3110    \t/3400, val loss:2.4554941654205322\n",
      "training on iteration:3111    \t/3400, val loss:4.734791278839111\n",
      "training on iteration:3112    \t/3400, val loss:2.583681583404541\n",
      "training on iteration:3113    \t/3400, val loss:2.888479709625244\n",
      "training on iteration:3114    \t/3400, val loss:4.450655937194824\n",
      "training on iteration:3115    \t/3400, val loss:2.7196173667907715\n",
      "training on iteration:3116    \t/3400, val loss:4.118523120880127\n",
      "training on iteration:3117    \t/3400, val loss:3.4062421321868896\n",
      "training on iteration:3118    \t/3400, val loss:3.637451171875\n",
      "training on iteration:3119    \t/3400, val loss:3.7650227546691895\n",
      "training on iteration:3120    \t/3400, val loss:2.678389072418213\n",
      "training on iteration:3121    \t/3400, val loss:3.7988507747650146\n",
      "training on iteration:3122    \t/3400, val loss:2.622241735458374\n",
      "training on iteration:3123    \t/3400, val loss:3.0809457302093506\n",
      "training on iteration:3124    \t/3400, val loss:2.8906266689300537\n",
      "training on iteration:3125    \t/3400, val loss:2.836747646331787\n",
      "training on iteration:3126    \t/3400, val loss:3.0741193294525146\n",
      "training on iteration:3127    \t/3400, val loss:3.0989768505096436\n",
      "training on iteration:3128    \t/3400, val loss:3.2449584007263184\n",
      "training on iteration:3129    \t/3400, val loss:3.3862719535827637\n",
      "training on iteration:3130    \t/3400, val loss:3.9228711128234863\n",
      "training on iteration:3131    \t/3400, val loss:3.3930165767669678\n",
      "training on iteration:3132    \t/3400, val loss:2.7076265811920166\n",
      "training on iteration:3133    \t/3400, val loss:3.3077938556671143\n",
      "training on iteration:3134    \t/3400, val loss:3.398540496826172\n",
      "training on iteration:3135    \t/3400, val loss:3.283989667892456\n",
      "training on iteration:3136    \t/3400, val loss:3.95434308052063\n",
      "training on iteration:3137    \t/3400, val loss:3.090825319290161\n",
      "training on iteration:3138    \t/3400, val loss:2.4261343479156494\n",
      "training on iteration:3139    \t/3400, val loss:6.584704875946045\n",
      "training on iteration:3140    \t/3400, val loss:4.167530059814453\n",
      "training on iteration:3141    \t/3400, val loss:3.6078755855560303\n",
      "training on iteration:3142    \t/3400, val loss:3.1626079082489014\n",
      "training on iteration:3143    \t/3400, val loss:4.274637699127197\n",
      "training on iteration:3144    \t/3400, val loss:2.7024242877960205\n",
      "training on iteration:3145    \t/3400, val loss:2.830472469329834\n",
      "training on iteration:3146    \t/3400, val loss:4.089186191558838\n",
      "training on iteration:3147    \t/3400, val loss:2.991692066192627\n",
      "training on iteration:3148    \t/3400, val loss:2.5401883125305176\n",
      "training on iteration:3149    \t/3400, val loss:4.051675319671631\n",
      "training on iteration:3150    \t/3400, val loss:2.9354300498962402\n",
      "training on iteration:3151    \t/3400, val loss:3.263139247894287\n",
      "training on iteration:3152    \t/3400, val loss:4.8004255294799805\n",
      "training on iteration:3153    \t/3400, val loss:6.055174350738525\n",
      "training on iteration:3154    \t/3400, val loss:2.6147823333740234\n",
      "training on iteration:3155    \t/3400, val loss:2.9915854930877686\n",
      "training on iteration:3156    \t/3400, val loss:6.656577110290527\n",
      "training on iteration:3157    \t/3400, val loss:4.8597822189331055\n",
      "training on iteration:3158    \t/3400, val loss:5.806678295135498\n",
      "training on iteration:3159    \t/3400, val loss:3.0093436241149902\n",
      "training on iteration:3160    \t/3400, val loss:8.531649589538574\n",
      "training on iteration:3161    \t/3400, val loss:8.740946769714355\n",
      "training on iteration:3162    \t/3400, val loss:2.3800814151763916\n",
      "training on iteration:3163    \t/3400, val loss:4.6815361976623535\n",
      "training on iteration:3164    \t/3400, val loss:3.2386813163757324\n",
      "training on iteration:3165    \t/3400, val loss:5.0218377113342285\n",
      "training on iteration:3166    \t/3400, val loss:2.6819417476654053\n",
      "training on iteration:3167    \t/3400, val loss:2.677629232406616\n",
      "training on iteration:3168    \t/3400, val loss:4.009400844573975\n",
      "training on iteration:3169    \t/3400, val loss:3.436177968978882\n",
      "training on iteration:3170    \t/3400, val loss:3.141507387161255\n",
      "training on iteration:3171    \t/3400, val loss:2.9322144985198975\n",
      "training on iteration:3172    \t/3400, val loss:3.6566669940948486\n",
      "training on iteration:3173    \t/3400, val loss:2.703867197036743\n",
      "training on iteration:3174    \t/3400, val loss:2.446693181991577\n",
      "training on iteration:3175    \t/3400, val loss:3.6451122760772705\n",
      "training on iteration:3176    \t/3400, val loss:2.664496660232544\n",
      "training on iteration:3177    \t/3400, val loss:2.505493640899658\n",
      "training on iteration:3178    \t/3400, val loss:4.220596790313721\n",
      "training on iteration:3179    \t/3400, val loss:2.6508896350860596\n",
      "training on iteration:3180    \t/3400, val loss:2.616250514984131\n",
      "training on iteration:3181    \t/3400, val loss:3.6059207916259766\n",
      "training on iteration:3182    \t/3400, val loss:2.651987075805664\n",
      "training on iteration:3183    \t/3400, val loss:2.4733240604400635\n",
      "training on iteration:3184    \t/3400, val loss:2.7929880619049072\n",
      "training on iteration:3185    \t/3400, val loss:2.5247292518615723\n",
      "training on iteration:3186    \t/3400, val loss:3.2672455310821533\n",
      "training on iteration:3187    \t/3400, val loss:2.5249998569488525\n",
      "training on iteration:3188    \t/3400, val loss:2.454385995864868\n",
      "training on iteration:3189    \t/3400, val loss:2.9933106899261475\n",
      "training on iteration:3190    \t/3400, val loss:3.5234246253967285\n",
      "training on iteration:3191    \t/3400, val loss:2.658740282058716\n",
      "training on iteration:3192    \t/3400, val loss:2.7635631561279297\n",
      "training on iteration:3193    \t/3400, val loss:2.8887579441070557\n",
      "training on iteration:3194    \t/3400, val loss:2.6016006469726562\n",
      "training on iteration:3195    \t/3400, val loss:4.408517837524414\n",
      "training on iteration:3196    \t/3400, val loss:2.850454568862915\n",
      "training on iteration:3197    \t/3400, val loss:2.65716552734375\n",
      "training on iteration:3198    \t/3400, val loss:4.250647068023682\n",
      "training on iteration:3199    \t/3400, val loss:4.032126426696777\n",
      "training on iteration:3200    \t/3400, val loss:3.449493408203125\n",
      "training on iteration:3201    \t/3400, val loss:3.0936439037323\n",
      "training on iteration:3202    \t/3400, val loss:4.845524787902832\n",
      "training on iteration:3203    \t/3400, val loss:2.7517292499542236\n",
      "training on iteration:3204    \t/3400, val loss:2.6548640727996826\n",
      "training on iteration:3205    \t/3400, val loss:3.704704523086548\n",
      "training on iteration:3206    \t/3400, val loss:2.738086700439453\n",
      "training on iteration:3207    \t/3400, val loss:2.9635605812072754\n",
      "training on iteration:3208    \t/3400, val loss:2.8292877674102783\n",
      "training on iteration:3209    \t/3400, val loss:2.4164187908172607\n",
      "training on iteration:3210    \t/3400, val loss:3.478261947631836\n",
      "training on iteration:3211    \t/3400, val loss:2.6632132530212402\n",
      "training on iteration:3212    \t/3400, val loss:2.697685956954956\n",
      "training on iteration:3213    \t/3400, val loss:2.7067484855651855\n",
      "training on iteration:3214    \t/3400, val loss:2.6427931785583496\n",
      "training on iteration:3215    \t/3400, val loss:2.990323305130005\n",
      "training on iteration:3216    \t/3400, val loss:2.6268374919891357\n",
      "training on iteration:3217    \t/3400, val loss:2.5917181968688965\n",
      "training on iteration:3218    \t/3400, val loss:2.434964895248413\n",
      "training on iteration:3219    \t/3400, val loss:2.8858702182769775\n",
      "training on iteration:3220    \t/3400, val loss:3.292346715927124\n",
      "training on iteration:3221    \t/3400, val loss:2.6280007362365723\n",
      "training on iteration:3222    \t/3400, val loss:2.6891915798187256\n",
      "training on iteration:3223    \t/3400, val loss:2.707348346710205\n",
      "training on iteration:3224    \t/3400, val loss:3.396544933319092\n",
      "training on iteration:3225    \t/3400, val loss:2.434457778930664\n",
      "training on iteration:3226    \t/3400, val loss:2.792985677719116\n",
      "training on iteration:3227    \t/3400, val loss:2.9099371433258057\n",
      "training on iteration:3228    \t/3400, val loss:2.77437424659729\n",
      "training on iteration:3229    \t/3400, val loss:3.1684250831604004\n",
      "training on iteration:3230    \t/3400, val loss:2.515582799911499\n",
      "training on iteration:3231    \t/3400, val loss:2.7335562705993652\n",
      "training on iteration:3232    \t/3400, val loss:2.7337756156921387\n",
      "training on iteration:3233    \t/3400, val loss:2.7063441276550293\n",
      "training on iteration:3234    \t/3400, val loss:2.762122392654419\n",
      "training on iteration:3235    \t/3400, val loss:2.578761577606201\n",
      "training on iteration:3236    \t/3400, val loss:2.613191604614258\n",
      "training on iteration:3237    \t/3400, val loss:3.167180299758911\n",
      "training on iteration:3238    \t/3400, val loss:2.4524762630462646\n",
      "training on iteration:3239    \t/3400, val loss:2.51291823387146\n",
      "training on iteration:3240    \t/3400, val loss:2.790907621383667\n",
      "training on iteration:3241    \t/3400, val loss:3.0344502925872803\n",
      "training on iteration:3242    \t/3400, val loss:2.5113511085510254\n",
      "training on iteration:3243    \t/3400, val loss:3.1855010986328125\n",
      "training on iteration:3244    \t/3400, val loss:2.7036612033843994\n",
      "training on iteration:3245    \t/3400, val loss:2.4649462699890137\n",
      "training on iteration:3246    \t/3400, val loss:3.4619216918945312\n",
      "training on iteration:3247    \t/3400, val loss:2.5542430877685547\n",
      "training on iteration:3248    \t/3400, val loss:3.106649398803711\n",
      "training on iteration:3249    \t/3400, val loss:2.6052379608154297\n",
      "training on iteration:3250    \t/3400, val loss:2.5867767333984375\n",
      "training on iteration:3251    \t/3400, val loss:2.9972751140594482\n",
      "training on iteration:3252    \t/3400, val loss:2.6650352478027344\n",
      "training on iteration:3253    \t/3400, val loss:3.088186502456665\n",
      "training on iteration:3254    \t/3400, val loss:2.824982166290283\n",
      "training on iteration:3255    \t/3400, val loss:2.819047689437866\n",
      "training on iteration:3256    \t/3400, val loss:2.59694504737854\n",
      "training on iteration:3257    \t/3400, val loss:2.6725142002105713\n",
      "training on iteration:3258    \t/3400, val loss:3.3677074909210205\n",
      "training on iteration:3259    \t/3400, val loss:2.860459566116333\n",
      "training on iteration:3260    \t/3400, val loss:3.126582384109497\n",
      "training on iteration:3261    \t/3400, val loss:2.736537218093872\n",
      "training on iteration:3262    \t/3400, val loss:2.5804898738861084\n",
      "training on iteration:3263    \t/3400, val loss:4.271475791931152\n",
      "training on iteration:3264    \t/3400, val loss:2.406560182571411\n",
      "training on iteration:3265    \t/3400, val loss:2.4654428958892822\n",
      "training on iteration:3266    \t/3400, val loss:3.595458745956421\n",
      "training on iteration:3267    \t/3400, val loss:2.460664749145508\n",
      "training on iteration:3268    \t/3400, val loss:2.629556179046631\n",
      "training on iteration:3269    \t/3400, val loss:2.4264869689941406\n",
      "training on iteration:3270    \t/3400, val loss:2.463701009750366\n",
      "training on iteration:3271    \t/3400, val loss:3.899545907974243\n",
      "training on iteration:3272    \t/3400, val loss:2.6388485431671143\n",
      "training on iteration:3273    \t/3400, val loss:2.4582321643829346\n",
      "training on iteration:3274    \t/3400, val loss:3.0139355659484863\n",
      "training on iteration:3275    \t/3400, val loss:2.4522624015808105\n",
      "training on iteration:3276    \t/3400, val loss:2.9535717964172363\n",
      "training on iteration:3277    \t/3400, val loss:2.498953104019165\n",
      "training on iteration:3278    \t/3400, val loss:2.5804429054260254\n",
      "training on iteration:3279    \t/3400, val loss:2.961050271987915\n",
      "training on iteration:3280    \t/3400, val loss:2.4611947536468506\n",
      "training on iteration:3281    \t/3400, val loss:2.6836278438568115\n",
      "training on iteration:3282    \t/3400, val loss:2.4666686058044434\n",
      "training on iteration:3283    \t/3400, val loss:2.5785441398620605\n",
      "training on iteration:3284    \t/3400, val loss:2.6110317707061768\n",
      "training on iteration:3285    \t/3400, val loss:2.4011058807373047\n",
      "training on iteration:3286    \t/3400, val loss:2.442730665206909\n",
      "training on iteration:3287    \t/3400, val loss:2.637444019317627\n",
      "training on iteration:3288    \t/3400, val loss:2.8401122093200684\n",
      "training on iteration:3289    \t/3400, val loss:2.504556894302368\n",
      "training on iteration:3290    \t/3400, val loss:2.4619503021240234\n",
      "training on iteration:3291    \t/3400, val loss:2.628946542739868\n",
      "training on iteration:3292    \t/3400, val loss:3.664961338043213\n",
      "training on iteration:3293    \t/3400, val loss:2.8109524250030518\n",
      "training on iteration:3294    \t/3400, val loss:2.8254172801971436\n",
      "training on iteration:3295    \t/3400, val loss:2.7354862689971924\n",
      "training on iteration:3296    \t/3400, val loss:2.588156223297119\n",
      "training on iteration:3297    \t/3400, val loss:5.04616641998291\n",
      "training on iteration:3298    \t/3400, val loss:2.7966971397399902\n",
      "training on iteration:3299    \t/3400, val loss:2.565211772918701\n",
      "training on iteration:3300    \t/3400, val loss:4.763702392578125\n",
      "training on iteration:3301    \t/3400, val loss:3.6751811504364014\n",
      "training on iteration:3302    \t/3400, val loss:4.540527820587158\n",
      "training on iteration:3303    \t/3400, val loss:3.1313436031341553\n",
      "training on iteration:3304    \t/3400, val loss:5.1231584548950195\n",
      "training on iteration:3305    \t/3400, val loss:2.5401885509490967\n",
      "training on iteration:3306    \t/3400, val loss:2.443349599838257\n",
      "training on iteration:3307    \t/3400, val loss:3.5188822746276855\n",
      "training on iteration:3308    \t/3400, val loss:3.331688404083252\n",
      "training on iteration:3309    \t/3400, val loss:3.414574384689331\n",
      "training on iteration:3310    \t/3400, val loss:2.9025022983551025\n",
      "training on iteration:3311    \t/3400, val loss:2.960026264190674\n",
      "training on iteration:3312    \t/3400, val loss:3.2813305854797363\n",
      "training on iteration:3313    \t/3400, val loss:2.510544538497925\n",
      "training on iteration:3314    \t/3400, val loss:2.436615467071533\n",
      "training on iteration:3315    \t/3400, val loss:3.3720695972442627\n",
      "training on iteration:3316    \t/3400, val loss:2.6043572425842285\n",
      "training on iteration:3317    \t/3400, val loss:2.9884350299835205\n",
      "training on iteration:3318    \t/3400, val loss:2.700068712234497\n",
      "training on iteration:3319    \t/3400, val loss:2.5177557468414307\n",
      "training on iteration:3320    \t/3400, val loss:2.953458786010742\n",
      "training on iteration:3321    \t/3400, val loss:2.416555166244507\n",
      "training on iteration:3322    \t/3400, val loss:3.6887097358703613\n",
      "training on iteration:3323    \t/3400, val loss:2.6567635536193848\n",
      "training on iteration:3324    \t/3400, val loss:2.946932077407837\n",
      "training on iteration:3325    \t/3400, val loss:2.4494223594665527\n",
      "training on iteration:3326    \t/3400, val loss:4.4109110832214355\n",
      "training on iteration:3327    \t/3400, val loss:2.4002363681793213\n",
      "training on iteration:3328    \t/3400, val loss:2.3851535320281982\n",
      "training on iteration:3329    \t/3400, val loss:3.642219305038452\n",
      "training on iteration:3330    \t/3400, val loss:2.7171216011047363\n",
      "training on iteration:3331    \t/3400, val loss:3.575535535812378\n",
      "training on iteration:3332    \t/3400, val loss:2.3684980869293213\n",
      "training on iteration:3333    \t/3400, val loss:2.4786264896392822\n",
      "training on iteration:3334    \t/3400, val loss:2.9873595237731934\n",
      "training on iteration:3335    \t/3400, val loss:2.364370584487915\n",
      "training on iteration:3336    \t/3400, val loss:3.016845941543579\n",
      "training on iteration:3337    \t/3400, val loss:2.450847864151001\n",
      "training on iteration:3338    \t/3400, val loss:2.63346266746521\n",
      "training on iteration:3339    \t/3400, val loss:3.1536478996276855\n",
      "training on iteration:3340    \t/3400, val loss:2.5031092166900635\n",
      "training on iteration:3341    \t/3400, val loss:2.647803783416748\n",
      "training on iteration:3342    \t/3400, val loss:2.4127767086029053\n",
      "training on iteration:3343    \t/3400, val loss:3.624544620513916\n",
      "training on iteration:3344    \t/3400, val loss:2.8002967834472656\n",
      "training on iteration:3345    \t/3400, val loss:3.076045274734497\n",
      "training on iteration:3346    \t/3400, val loss:2.573439359664917\n",
      "training on iteration:3347    \t/3400, val loss:2.590719223022461\n",
      "training on iteration:3348    \t/3400, val loss:5.380218029022217\n",
      "training on iteration:3349    \t/3400, val loss:2.4929161071777344\n",
      "training on iteration:3350    \t/3400, val loss:3.091744899749756\n",
      "training on iteration:3351    \t/3400, val loss:4.346306800842285\n",
      "training on iteration:3352    \t/3400, val loss:2.693438768386841\n",
      "training on iteration:3353    \t/3400, val loss:3.465662717819214\n",
      "training on iteration:3354    \t/3400, val loss:3.34281325340271\n",
      "training on iteration:3355    \t/3400, val loss:2.404296398162842\n",
      "training on iteration:3356    \t/3400, val loss:2.5253071784973145\n",
      "training on iteration:3357    \t/3400, val loss:3.5685770511627197\n",
      "training on iteration:3358    \t/3400, val loss:2.716935634613037\n",
      "training on iteration:3359    \t/3400, val loss:2.881484270095825\n",
      "training on iteration:3360    \t/3400, val loss:3.834880828857422\n",
      "training on iteration:3361    \t/3400, val loss:3.2330000400543213\n",
      "training on iteration:3362    \t/3400, val loss:2.786031484603882\n",
      "training on iteration:3363    \t/3400, val loss:3.1794376373291016\n",
      "training on iteration:3364    \t/3400, val loss:2.897141218185425\n",
      "training on iteration:3365    \t/3400, val loss:3.6708338260650635\n",
      "training on iteration:3366    \t/3400, val loss:2.423788070678711\n",
      "training on iteration:3367    \t/3400, val loss:2.373591184616089\n",
      "training on iteration:3368    \t/3400, val loss:3.5396170616149902\n",
      "training on iteration:3369    \t/3400, val loss:2.3822543621063232\n",
      "training on iteration:3370    \t/3400, val loss:2.5488693714141846\n",
      "training on iteration:3371    \t/3400, val loss:2.5511677265167236\n",
      "training on iteration:3372    \t/3400, val loss:2.373809337615967\n",
      "training on iteration:3373    \t/3400, val loss:4.388237953186035\n",
      "training on iteration:3374    \t/3400, val loss:2.5048635005950928\n",
      "training on iteration:3375    \t/3400, val loss:2.4280173778533936\n",
      "training on iteration:3376    \t/3400, val loss:3.8133749961853027\n",
      "training on iteration:3377    \t/3400, val loss:2.5134963989257812\n",
      "training on iteration:3378    \t/3400, val loss:2.5597097873687744\n",
      "training on iteration:3379    \t/3400, val loss:3.1918110847473145\n",
      "training on iteration:3380    \t/3400, val loss:2.567729949951172\n",
      "training on iteration:3381    \t/3400, val loss:2.8994803428649902\n",
      "training on iteration:3382    \t/3400, val loss:3.1455600261688232\n",
      "training on iteration:3383    \t/3400, val loss:2.5937068462371826\n",
      "training on iteration:3384    \t/3400, val loss:3.3393876552581787\n",
      "training on iteration:3385    \t/3400, val loss:2.8888299465179443\n",
      "training on iteration:3386    \t/3400, val loss:3.1847310066223145\n",
      "training on iteration:3387    \t/3400, val loss:3.050523519515991\n",
      "training on iteration:3388    \t/3400, val loss:2.962475299835205\n",
      "training on iteration:3389    \t/3400, val loss:3.106278896331787\n",
      "training on iteration:3390    \t/3400, val loss:4.407952308654785\n",
      "training on iteration:3391    \t/3400, val loss:2.637425661087036\n",
      "training on iteration:3392    \t/3400, val loss:3.2417728900909424\n",
      "training on iteration:3393    \t/3400, val loss:3.8639328479766846\n",
      "training on iteration:3394    \t/3400, val loss:2.7361743450164795\n",
      "training on iteration:3395    \t/3400, val loss:2.982091188430786\n",
      "training on iteration:3396    \t/3400, val loss:3.8605217933654785\n",
      "training on iteration:3397    \t/3400, val loss:2.5458967685699463\n",
      "training on iteration:3398    \t/3400, val loss:3.1170878410339355\n",
      "training on iteration:3399    \t/3400, val loss:4.933745861053467\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_weight, train_loss_hist, val_loss_hist = train(model, optimizer, loss_fn, train_data_loader, [X_val, y_val], epoch=200)\n",
    "modelSaveDirectory = './state_dict/basic'\n",
    "torch.save(model_weight, f\"{modelSaveDirectory}.pth\")\n",
    "show_loss_stats(train_loss_hist, val_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_loss_stats(train_loss_hist, val_loss_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
